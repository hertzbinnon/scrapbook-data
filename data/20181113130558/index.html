<!DOCTYPE html>
<html id="feedHandler" xmlns="http://www.w3.org/1999/xhtml">
<head xmlns="http://www.w3.org/1999/xhtml">
<meta content="application/xhtml+xml; charset=UTF-8" http-equiv="Content-Type" />

    <title>Planet GStreamer</title>
    <link rel="stylesheet" href="chrome://browser/skin/feeds/subscribe.css" type="text/css" media="all" />
    
  </head>
<body xmlns="http://www.w3.org/1999/xhtml" onunload="SubscribeHandler.uninit();">
    <div id="feedHeaderContainer">
      <div id="feedHeader" dir="ltr" class="feedBackground">
        <div id="feedIntroText">
          <p id="feedSubscriptionInfo1"></p>
          <p id="feedSubscriptionInfo2"></p>
        </div>
        <div id="feedSubscribeLine">
          <label id="subscribeUsingDescription">订阅此收取点，使用 
            <select id="handlersMenuList">
              <option id="liveBookmarksMenuItem" selected="true">实时书签</option>
              <option disabled="true">━━━━━━━</option>
            <option id="selectedAppMenuItem" handlerType="client" style="display: none;"></option><option id="defaultHandlerMenuItem" handlerType="client" style="display: none;"></option><option id="chooseApplicationMenuItem">选择应用程序…</option><option disabled="true">━━━━━━━</option><option id="liveBookmarksMenuItem" class="menuitem-iconic" handlerType="web" webhandlerurl="https://add.my.yahoo.com/rss?url=%s">我的 Yahoo</option></select>
          </label>
          <label id="checkboxText">
            <input id="alwaysUse" class="alwaysUse" type="checkbox" />总是用 实时书签 订阅收取点。</label>
          <button id="subscribeButton">立即订阅</button>
        </div>
      </div>
      <div id="feedHeaderContainerSpacer"></div>
    </div>

    

    <div id="feedBody">
      <div id="feedTitle">
        <a id="feedTitleLink">
          <img id="feedTitleImage" />
        </a>
        <div id="feedTitleContainer">
          <h1 id="feedTitleText" xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Planet GStreamer</h1>
          <h2 id="feedSubtitleText"></h2>
        </div>
      </div>
      <div id="feedContent"><div class="entry"><h3><a href="https://arunraghavan.net/2018/10/update-from-the-pipewire-hackfest/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Update from the PipeWire hackfest</span></a><div class="lastUpdated">2018年10月31日 23:49</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>As the third and final day of the <a href="https://wiki.gnome.org/Hackfests/PipeWire2018">PipeWire hackfest</a> draws to a close, I thought I’d summarise some of my thoughts on the goings-on and the future.</p>

<h3>Thanks</h3>

<p>Before I get into the details, I want to send out a <em>big thank you</em> to:</p>

<ul>
<li>Christian Schaller for all the hard work of organising the event and Wim Taymans for the work on PipeWire so far (and in the future)</li>
<li>The GNOME Foundation, for sponsoring the event as a whole</li>
<li>Qualcomm, who are funding my presence at the event</li>
<li>Collabora, for sponsoring dinner on Monday</li>
<li>Everybody who attended and participate for their time and thoughtful comments</li>
</ul>

<h3>Background</h3>

<p>For those of you who are not familiar with it, PipeWire (previously Pinos, previously PulseVideo) was Wim’s effort at providing secure, multi-program access to video devices (like webcams, or the desktop for screen capture). As he went down that rabbit hole, he wrote SPA, a lightweight general-purpose framework for representing a streaming graph, and this led to the idea of expanding the project to include support for low latency audio.</p>

<p>The Linux userspace audio story has, for the longest time, consisted of two top-level components: PulseAudio which handles consumer audio (power efficiency, wide range of arbitrary hardware), and JACK which deals with pro audio (low latency, high performance). Consolidating this into a good out-of-the-box experience for all use-cases has been a long-standing goal for myself and others in the community that I have spoken to.</p>

<h3>An Opportunity</h3>

<p>From a PulseAudio perspective, it has been hard to achieve the 1-to-few millisecond latency numbers that would be absolutely necessary for professional audio use-cases. A lot of work has gone into improving this situation, most recently with David Henningsson’s shared-ringbuffer channels that made client/server communication more efficient.</p>

<p>At the same time, as application sandboxing frameworks such as <a href="https://flatpak.org/">Flatpak</a> have added security requirements of us that were not accounted for when PulseAudio was written. Examples including choosing which devices an application has access to (or can even know of) or which applications can act as control entities (set routing etc., enable/disable devices). Some work has gone into this — Ahmed Darwish did some key work to get memfd support in PulseAudio, and Wim has prototyped an access-control mechanism module to enable a Flatpak portal for sound.</p>

<p>All this said, there are still fundamental limitations in architectural decisions in PulseAudio that would require significant plumbing to address. With Wim’s work on PipeWire and his extensive background with GStreamer and PulseAudio itself, I think we have an opportunity to revisit some of those decisions with the benefit of a decade’s worth of learning deploying PulseAudio in various domains starting from desktops/laptops to phones, cars, robots, home audio, telephony systems and a lot more.</p>

<h3>Key Ideas</h3>

<p>There are some core ideas of PipeWire that I am quite excited about.</p>

<p>The first of these is the graph. Like JACK, the entities that participate in the data flow are represented by PipeWire as nodes in a graph, and routing between nodes is very flexible — you can route applications to playback devices and capture devices to applications, but you can also route applications to other applications, and this is notionally the same thing.</p>

<p>The second idea is a bit more radical — PipeWire itself only “runs” the graph. The actual connections between nodes are created and managed by a “session manager”. This allows us to <em>completely separate</em> the data flow from policy, which means we could write completely separate policy for desktop use cases vs. specific embedded use cases. I’m particularly excited to see this be scriptable in a higher-level language, which is something <a href="http://www.hadess.net/2018/10/pipewire-hackfest-2018.html">Bastien has already started work on</a>!</p>

<p>A powerful idea in PulseAudio was rewinding — the ability to send out huge buffers to the device, but the flexibility to rewind that data when things changed (a new stream got added, or the stream moved, or the volume changed). While this is great for power saving, it is a significant amount of complexity in the code. In addition, with some filters in the data path, rewinding can break the algorithm by introducing non-linearity. PipeWire doesn’t support rewinds, and we will need to find a good way to manage latencies to account for low power use cases. One example is that we could have the session manager bump up the device latency when we know latency doesn’t matter (Android does this when the screen is off).</p>

<p>There are a bunch of other things that are in the process of being fleshed out, like being able to represent the hardware as a graph as well, to have a clearer idea of what is going on within a node. More updates as these things are more concrete.</p>

<h3>The Way Forward</h3>

<p>There is a <a href="https://blogs.gnome.org/uraeus/2018/10/30/pipewire-hackfest/">good summary by Christian</a> about our discussion about what is missing and how we can go about trying to make a smooth transition for PulseAudio users. There is, of course, a <em>lot</em> to do, and my ideal outcome is that we one day flip a switch and nobody knows that we have done so.</p>

<p>In practice, we’ll need to figure out how to make this transition seamless for most people, while folks with custom setup will need to be given a long runway and clear documentation to know what to do. It’s way to early to talk about this in more specifics, however.</p>

<h3>Configuration</h3>

<p>One key thing that PulseAudio does right (I know there are people who disagree!) is having a custom configuration that automagically works on a <em>lot</em> of Intel HDA-based systems. We’ve been wondering how to deal with this in PipeWire, and the path we think makes sense is to transition to ALSA UCM configuration. This is note as flexible as we need it to be, but I’d like to extend it for that purpose if possible. This would ideally also help consolidate the various methods of configuration being used by the various Linux userspaces.</p>

<p>To that end, I’ve started trying to get a UCM setup on my desktop that PulseAudio can use, and be functionally equivalent to what we do with our existing configuration. There are missing bits and bobs, and I’m currently focusing on the ones related to hardware volume control. I’ll write about this in the future as the effort expands out to other hardware.</p>

<h3>Onwards and upwards</h3>

<p>The transition to PipeWire is unlikely to be quick or completely-painless or free of contention. For those who are worried about the future, know that any switch is still a long way away. In the mean time, however, constructive feedback and comments are welcome.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://www.hadess.net/2018/10/pipewire-hackfest-2018.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Pipewire Hackfest 2018</span></a><div class="lastUpdated">2018年10月31日 19:44</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent">Good morning from Edinburgh, where the <a href="https://www.yell.com/biz/fatty-owls-edinburgh-8740065/">breakfast contains haggis</a>, and the charity shops have some <a href="https://twitter.com/Micro_Repairs">interesting finds</a>.<br /><br />My main goal in attending <a href="https://wiki.gnome.org/Hackfests/PipeWire2018">this hackfest</a> was to discuss Pipewire integration in the desktop, and how it will eventually replace PulseAudio as the audio daemon.<br /><br />The main problem GNOME has had over the years with PulseAudio relate mostly to how PulseAudio was a black box when it came to its routing policy. What happens when you plug in an HDMI cable into your laptop? Or turn on your Bluetooth headset? I've heard the stories of folks with highly mobile workstations having to constantly visit the Sound settings panel.<br /><br />PulseAudio has policy scattered in a number of places (do a "git grep routing" inside the sources to see that): some are in the device manager, then modules themselves can set priorities for their outputs and inputs. But there's nothing to take all the information in, and take a decision based on the hardware that's plugged in, and the applications currently in use.<br /><br />For Pipewire, the policy decisions would be split off from the main daemon. Pipewire, as it gains PulseAudio compatibility layers, will grow a default/example policy engine that will try to replicate PulseAudio's behaviour. At the very least, that will mean that Pipewire won't regress compared to PulseAudio, and might even be able to take better decisions in the short term.<br /><br />For GNOME, we still wanted to take control of that part of the experience, and make our own policy decisions. It's very possible that this engine will end up being featureful and generic enough that it will be used by more than just GNOME, or even become the default Pipewire one, but it's far too early to make that particular decision.<br /><br />In the meanwhile, we wanted the GNOME policies to not be written in C, difficult to experiment with for power users, and for edge use cases. We could have started writing a configuration language, but it would have been too specific, and there are plenty of embeddable languages around. It was also a good opportunity for me to finally write the helper library I've been meaning to write for years, based on my favourite embedded language, Lua.<br /><br />So I'm introducing <a href="https://gitlab.gnome.org/hadess/anatole">Anatole</a>. The goal of the project is to make it trivial to write chunks of programs in Lua, while the core of your project is written in C (we might even be able to embed it in Python or Javascript, once introspection support is added).<br /><br />It's still in the very early days, and unusable for anything as of yet, but progress should be pretty swift. The code is mostly based on Victor Toso's incredible <a href="http://www.hadess.net/2014/02/extend-gnome-videos-with-lua.html">"Lua factory" plugin in Grilo</a>. (I'm hoping that, once finished, I won't have to remember on which end of the stack I need to push stuff for Lua to do something with it ;)</div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/10/30/pipewire-hackfest/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">PipeWire Hackfest</span></a><div class="lastUpdated">2018年10月30日 22:15</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>So we kicked off the PipeWire hackfest in Edinburgh yesterday. We have 15 people attending including Arun Raghavan, Tanu Kaskinen and Colin Guthrie from PulseAudio, PipeWire creator Wim Taymans, Bastien Nocera and Jan Grulich representing GNOME and KDE, Mark Brown from the ALSA kernel team, Olivier Crête,George Kiagiadakis and Nicolas Dufresne was there to represent embedded usecases for PipeWire and finally Thierry Bultel representing automotive.</p>
<p>The event kicked off with Wim Taymans presenting on current state of PipeWire and outlining the remaining issues and current thoughts on how to resolve them. Most of the first day was spent on a roadtable discussion about what are and should be the goals of PipeWire and what potential tradeoffs there would be going forward. PipeWire is probably a bit closer to Jack than PulseAudio in design, so quite a bit of the discussion went on how that would affect the PulseAudio usecases and what is planned to ensure PipeWire works very well for consumer audio usecases. </p>
<p>Personally I ended up spending quite some time just testing and running various Jack apps to see what works already and what doesn’t. In terms of handling outputing audio with Jack apps I was positively surprised how many Jack apps I was able to make work (aka output audio) using PipeWire instead of Jack, but of course we still have some gaps to cover before PipeWire is ready as a drop-in Jack replacement, for instance the Jack session management protocol needs to be implemented first.</p>
<p>The second day we outlined the areas that need work before we are ready to replace PulseAudio and came up with the following list:</p>
<ul>
<li>Mixers – This is basically dealing with hardware mixers. Arun and Wim started looking at a design for this during the hackfest.</li>
<li>PulseAudio services – This is all the things in PulseAudio that is not very suitable for putting inside PipeWire. The idea is instead to put them in a separate daemon. This includes things like network streaming, ROAP, DBus apis and so on.</li>
<li>Policy/Session handling – We plan to move policy and session handling out of PulseAudio to make it easier for different usecases to set their own policies. PipeWire will still provide some default setup, but the idea here is to have a separate daemon(s) to provide this. Bastien Nocera started prototyping a setup where he could create policy and session handling using Lua scripting.</li>
<li>Filters</li>
<li>Bluetooth – Ensuring we have great bluetooth support with PipeWire. We would want to move Bluetooth handling to its own daemon, and not have it inside like in PulseAudio to allow for more flexibility with various embedded bluetooth stacks for instance. This could also mean looking at the Linux Bluetooth stack more widely as things are not ideal atm, especially from a security viewpoint.</li>
<li>Device reservation – We expect to replace Jack and PulseAudio in steps, starting with PulseAudio. So dealing well with hardware reservation is important to allow people to for instance keep running Jack alongside PipeWire until we are ready for full replacement.</li>
<li>Stream Monitoring – Important feature from Jack and PulseAudio that still needs implementing to allowing monitoring audio devices and streams.</li>
<li>Latency handling – Improving ways we can deal with hardware latency in for instance consumer devices such as TVs</li>
</ul>
<p>
It is still a bit hard to have a clear timeline for when we will be ready to drop in PipeWire support to replace PulseAudio and then Jack, but we feel the Wayland migration was a good example to follow where we held off doing the switch until we felt comfortable the move would be transparent to most users. There will of course always be corner cases and bugs, but we hope that in general people agree that the Wayland transition was done in a responsible manner and thus could be a good example to follow for us here.
</p>
<p>
We would like to offers big thanks to the GNOME Foundation for sponsoring travel for some of the community attendees and to Collabora for sponsoring dinner for all attendees the first night.</p>
<p>
If you want to take a look at PipeWire, Wim updated the wiki page <a href="https://github.com/PipeWire/pipewire/wiki/Building-and-Running"> with PipeWire build intructions to be up-to-date</a>. The hackfest attendees tested them out so we are sure they work, just be aware that you  want the ‘Work’ branch and not the Master branch, as that is the one where all the audio work is happening. The Master branch is the video focused branch we use in Fedora for desktop remoting support in browsers and VNC under Wayland.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/10/23/fedora-toolbox-ready-for-testing/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Fedora Toolbox ready for testing!</span></a><div class="lastUpdated">2018年10月24日 0:58</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>As many of you know we kicked of a ambitious goal to revamp the Linux desktop when we launched Fedora Workstation 4 years. We wanted to remove many of the barriers to adoption of Linux as a desktop and make it a better operating system for all, especially for developers.<br />
To that effect we have been pushing a long range of initiatives over the last 4 years ago, ranging from providing a better input stack through <a href="https://www.freedesktop.org/wiki/Software/libinput/">libinput</a>, a better display system through <a href="https://wayland.freedesktop.org/">Wayland</a>, a better audio and video subsystem through <a href="https://pipewire.org/">PipeWire</a>, a better way of doing application packaging and dependency handling through <a href="https://flatpak.org/">Flatpak</a>, a better application installation history through <a href="https://wiki.gnome.org/Apps/Software">GNOME Software</a>, actual firmware handling for Linux through <a href="https://fwupd.org/">Linux Vendor Firmware Service</a>, better manageability through <a href="https://fleet-commander.org/">Fleet Commander</a>, and <a href="https://silverblue.fedoraproject.org/">Project Silverblue</a> for reliable OS updates. We also had a lot of efforts done to improve general hardware handling, be that work on glvnd and friends for dealing with NVidia driver, the <a href="https://christian.kellner.me/2017/12/14/introducing-bolt-thunderbolt-3-security-levels-for-gnulinux/">Bolt project</a> for handling Thunderbolt devices better, HiDPI support in the desktop, better touch support in the desktop, improved laptop battery life, and ongoing work to improve state of fingerprint readers under Linux and to provide a flicker free boot experience.</p>
<p>One thing though that was clear to us was that as we where making all these changes to improve the ease of use and reliability of Linux as a desktop operating system we couldn’t make life worse for developers. Developers are the lifeblood of Fedora and Linux and thus we have had Debarshi Ray working on a project we call <a href="https://github.com/debarshiray/fedora-toolbox">Fedora Toolbox</a>. Fedora toolbox creates a seamless experience for developers when using an immutable OS like Silverblue, yet want to be able to install the wonderful world of software libraries and tools that makes Linux so powerful for developers. Fedora Toolbox is now ready for early adopters to start testing, so I recommend jumping over to <a href="https://debarshiray.wordpress.com/2018/10/22/fedora-toolbox-hacking-on-fedora-silverblue/">Debarshi’s blog to read up on Fedora Toolbox</a>.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://ramcq.net/2018/10/19/gnome-foundation-hackfest-2018/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GNOME Foundation Hackfest 2018</span></a><div class="lastUpdated">2018年10月19日 23:38</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>This week, the <a href="https://www.gnome.org/foundation/">GNOME Foundation</a> Board of Directors met at the <a href="http://www.collabora.com/">Collabora</a> office in Cambridge, UK, for the second annual Foundation Hackfest. We were also joined by the Executive Director, Neil McGovern, and Director of Operations, Rosanna Yuen. This event was started by last year’s board and is a great opportunity for the newly-elected board to set out goals for the coming year and get some uninterrupted hacking done on policies, documents, etc. While it’s fresh in our mind, we wanted to tell you about some of the things we have been working on this week and what the community can hope to see in the coming months.</p>
<h2>Wednesday: Goals</h2>
<p>On Wednesday we set out to define the overall goals of the Foundation, so we could focus our activities for the coming years, ensuring that we were working on the right priorities. Neil helped to facilitate the discussion using the <a href="http://independentsector.org/wp-content/uploads/2017/03/charting-impact-guide.pdf">Charting Impact</a> process. With that input, we went back to the purpose of the Foundation and mapped that to ten and five year goals, making sure that our current strategies and activities would be consistent with reaching those end points. This is turning out to be a very detailed and time-consuming process. We have made a great start, and hope to have something we can share for comments and input soon. The high level 10-year goals we identified boiled down to:</p>
<ul>
<li>Sustainable project and foundation</li>
<li>Wider awareness and mindshare – being a thought leader</li>
<li>Increased user base</li>
</ul>
<p>As we looked at the charter and bylaws, we identified a long-standing issue which we need to solve — there is currently no formal process to cover the “scope” of the Foundation in terms of which software we support with our resources. There is the release team, but that is only a subset of the software we support. We have some examples such as GIMP which “have always been here”, but at present there is no clear process to apply or be included in the Foundation. We need a clear list of projects that use resources such as CI, or have the right to use the GNOME trademark for the project. We have a couple of similar proposals from Allan Day and Carlos Soriano for how we could define and approve projects, and we are planning to work with them over the next couple of weeks to make one proposal for the board to review.</p>
<h2>Thursday: Budget forecast</h2>
<p>We started the second day with a review of the proposed forecast from Neil and Rosanna, because the Foundation’s financial year starts in October. We have policies in place to allow staff and committees to spend money against their budget without further approval being needed, which means that with no approved budget, it’s very hard for the Foundation to spend any money. The proposed budget was based off the previous year’s actual figures, with changes to reflect the increased staff headcount, increased spend on CI, increased staff travel costs, etc, and ensure after the year’s spending, we follow the reserves policy to keep enough cash to pay the foundation staff for a further year. We’re planning to go back and adjust a few things (internships, marketing, travel, etc) to make sure that we have the right resources for the goals we identified.</p>
<p>We had some “hacking time” in smaller groups to re-visit and clarify various policies, such as the conference and hackfest proposal/approval process, travel sponsorship process and look at ways to support internationalization (particularly to indigenous languages).</p>
<h2>Friday: Foundation Planning</h2>
<p>The Board started Friday with a board-only (no staff) meeting to make sure we were aligned on the goals that we were setting for the Executive Director during the coming year, informed by the Foundation goals we worked on earlier in the week. To avoid the “seven bosses” problem, there is one board member (myself) responsible for managing the ED’s priorities and performance. It’s important that I take advantage of the opportunity of the face to face meeting to check in with the Board about their feedback for the ED and things I should work together with Neil on over the coming months.</p>
<p>We also discussed a related topic, which is the length of the term that directors serve on the Foundation Board. With 7 staff members, the Foundation needs consistent goals and management from one year to the next, and the time demands on board members should be reduced from previous periods where the Foundation hasn’t had an Executive Director. We want to make sure that our “ten year goals” don’t change every year and undermine the strategies that we put in place and spend the Foundation resources on. We’re planning to change the Board election process so that each director has a two year term, so half of the board will be re-elected each year. This also prevents the situation where the majority of the Board is changed at the same election, losing continuity and institutional knowledge, and taking months for people to get back up to speed.</p>
<p>We finished the day with a formal board meeting to approve the budget, more hack time on various policies (and this blog!). Thanks to <a href="http://www.collabora.com/">Collabora</a> for use of their office space, food, and snacks – and thanks to my fellow Board members and the Foundation’s wonderful and growing staff team</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://ramcq.net/2018/10/15/flatpak-sandbox-security/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Flatpaks, sandboxes and security</span></a><div class="lastUpdated">2018年10月15日 21:40</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Last week the Flatpak community woke to the “news” that we are making the world a less secure place and we need to rethink what we’re doing. Personally, I’m not sure this is a fair assessment of the situation. The “tl;dr” summary is: Flatpak confers many benefits besides the sandboxing, and even looking just at the sandboxing, improving app security is a huge problem space and so is a work in progress across multiple upstream projects. Much of what has been achieved so far already delivers incremental improvements in security, and we’re making solid progress on the wider app distribution and portability problem space.</p>
<p>Sandboxing, like security in general, isn’t a binary thing – you can’t just say because you have a sandbox, you have 100% security. Like having two locks on your front door, two front doors, or locks on your windows too, sensible security is about <a href="https://en.wikipedia.org/wiki/Defense_in_depth_(computing)">defense in depth</a>. Each barrier that you implement precludes some invalid or possibly malicious behaviour. You hope that in total, all of these barriers would prevent anything bad, but you can never really guarantee this – it’s about multiplying together probabilities to get a smaller number. A computer which is switched off, in a locked faraday cage, with no connectivity, is perfectly secure – but it’s also perfectly useless because you cannot actually use it. Sandboxing is very much the same – whilst you could easily take systemd-nspawn, Docker or any other container technology of choice and 100% lock down a desktop app, you wouldn’t be able to interact with it at all.</p>
<p>Network services have incubated and driven most of the container usage on Linux up until now but they are fundamentally different to desktop applications. For services you can write a simple list of permissions like, “listen on this network port” and “save files over here” whereas desktop applications have a <i>much</i> larger number of touchpoints to the outside world which the user expects and requires for normal functionality. Just thinking off the top of my head you need to consider access to the filesystem, display server, input devices, notifications, IPC, accessibility, fonts, themes, configuration, audio playback and capture, video playback, screen sharing, GPU hardware, printing, app launching, removable media, and joysticks. Without making holes in the sandbox to allow access to these in to your app, it either wouldn’t work at all, or it wouldn’t work in the way that people have come to expect.</p>
<p>What Flatpak brings to this is understanding of the specific desktop app problem space – most of what I listed above is to a greater or lesser extent understood by Flatpak, or support is planned. The Flatpak sandbox is very configurable, allowing the application author to specify which of these resources they need access to. The Flatpak CLI asks the user about these during installation, and we provide the <tt>flatpak override</tt> command to allow the user to add or remove these sandbox escapes. Flatpak has introduced portals into the Linux desktop ecosystem, which we’re really pleased to be sharing with snap since earlier this year, to provide runtime access to resources outside the sandbox based on policy and user consent. For instance, document access, app launching, input methods and recursive sandboxing (“sandbox me harder”) have portals.</p>
<p>The starting security position on the desktop was quite terrible – anything in your session had basically complete access to everything belonging to your user, and many places to hide.</p>
<ul>
<li>Access to the X socket allows arbitrary input and output to any other app on your desktop, but without it, no app on an X desktop would work. Wayland fixes this, so Flatpak has a fallback setting to allow Wayland to be used if present, and the X socket to be shared if not.</li>
<li>Unrestricted access to the PulseAudio socket allows you to reconfigure audio routing, capture microphone input, etc. To ensure user consent we need a portal to control this, where by default you can play audio back but device access needs consent and <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/AccessControl/">work is under way</a> to create this portal.</li>
<li>Access to the webcam device node means an app can capture video whenever it wants – solving this <a href="https://pipewire.org/">required a whole new project</a>.</li>
<li>Sandboxing access to configuration in dconf <a href="https://blogs.gnome.org/mclasen/2018/10/08/flatpak-after-1-0/">is a priority for the project right now, after the 1.0 release</a>.</li>
</ul>
<p>Even with these caveats, Flatpak brings a bunch of default sandboxing – IPC filtering, a new filesystem, process and UID namespace, seccomp filtering, an immutable /usr and /app – and each of these is already a barrier to certain attacks.</p>
<p>Looking at the specific concerns raised:</p>
<ul>
<li>Hopefully from the above it’s clear that sandboxing desktop apps isn’t just a switch we can flick overnight, but what we already have is far better than having nothing at all. It’s not the intention of Flatpak to somehow mislead people that sandboxed means somehow impervious to all known security issues and can access nothing whatsoever, but we do want to encourage the use of the new technology so that we can work together on driving adoption and making improvements together. The idea is that over time, as the portals are filled out to cover the majority of the interfaces described, and supported in the major widget sets / frameworks, the criteria for earning a nice “sandboxed” badge or submitting your app to Flathub will become stricter. Many of the apps that access <tt>--filesystem=home</tt> are because they use old widget sets like Gtk2+ and frameworks like Electron that don’t support portals (yet!). Contributions to improve portal integration into other frameworks and desktops are very welcome and as mentioned above will also improve integration and security in other systems that use portals, such as snap.</li>
<li>As Alex has <a href="https://blogs.gnome.org/alexl/2018/10/11/moving-away-from-the-1-6-freedesktop-runtime/">already blogged</a>, the freedesktop.org 1.6 runtime was something we threw together because we needed something distro agnostic to actually be able to bootstrap the entire concept of Flatpak and runtimes. A confusing mishmash of Yocto with flatpak-builder, it’s thankfully nearing some form of retirement after a recent round of security fixes. The replacement <a href="https://gitlab.com/freedesktop-sdk/">freedesktop-sdk</a> project has just released its first stable 18.08 release, and rather than “one or two people in their spare time because something like this needs to exist”, is backed by a team from <a href="https://www.codethink.co.uk/">Codethink</a> and with support from the Flatpak, GNOME and KDE communities.</li>
<li>I’m not sure how fixing and disclosing a security problem in a relatively immature pre-1.0 program (in June 2017, Flathub had less than 50 apps) is considered an ongoing problem from a security perspective. The wording in the release notes?</li>
</ul>
<p>Zooming out a little bit, I think it’s worth also highlighting some of the other reasons why Flatpak exists at all – these are far bigger problems with the Linux desktop ecosystem than app security alone, and Flatpak brings a huge array of benefits to the table:</p>
<ul>
<li><b>Allowing apps to become agnostic of their underlying distribution</b>. The reason that runtimes exist at all is so that apps can specify the ABI and dependencies that they need, and you can run it on whatever distro you want. Flatpak has had this from day one, and it’s been hugely reliable because the sandboxed /usr means the app can rely on getting whatever they need. This is the foundation on which everything else is built.</li>
<li><b>Separating the release/update cadence of distributions from the apps</b>. The flip side of this, which I think is huge for more conservative platforms like Debian or enterprise distributions which don’t want to break their ABIs, hardware support or other guarantees, is that you can still get new apps into users hands. Wider than this, I think it allows us huge new freedoms to move in a direction of reinventing the distro – once you start to pull the gnarly complexity of apps and their dependencies into sandboxes, your constraints are hugely reduced and you can slim down or radically rethink the host system underneath. At <a href="https://www.endlessos.com/">Endless OS</a>, Flatpak literally changed the structure of our engineering team, and for the first time allowed us to develop and deliver our OS, SDK and apps in independent teams each with their own cadence.</li>
<li><b>Disintermediating app developers from their users</b>. <a href="https://www.flathub.org/">Flathub</a> now offers over 400 apps, and (at a rough count by Nick Richards over the summer) over half of them are directly maintained by or maintained in conjunction with the upstream developers. This is fantastic – we get the releases when they come out, the developers can choose the dependencies and configuration they need – and they get to deliver this same experience to everyone.</li>
<li><b>Decentralised</b>. Anyone can set up a Flatpak repo! We started our own at Flathub because there needs to be a center of gravity and a complete story to build out a user and developer base, but the idea is that anyone can use the same tools that we do, and publish whatever/wherever they want. GNOME uses GitLab CI to publish nightly Flatpak builds, KDE is setting up the same in their infrastructure, and Fedora is working on <a href="https://fedoraproject.org/wiki/Workstation/Flatpaks">completely different infrastructure</a> to build and deliver their packaged applications as Flatpaks.</li>
<li><b>Easy to build</b>. I’ve worked on Debian packages, RPMs, Yocto, etc and I can honestly say that flatpak-builder has done a very good job of making it really easy to put your app manifest together. Because the builds are sandboxed and each runtimes brings with it a consistent SDK environment, they are very reliably reproducible. It’s worth just calling this out because when you’re trying to attract developers to your platform or contributors to your app, hurdles like complex or fragile tools and build processes to learn and debug all add resistance and drag, and discourage contributions. GNOME Builder can take any flatpak’d app and build it for you automatically, ready to hack within minutes.</li>
<li><b>Different ways to distribute apps</b>. Using OSTree under the hood, Flatpak supports single-file app .bundles, pulling from OSTree repos and OCI registries, and at Endless we’ve been working on peer-to-peer distribution like USB sticks and LAN sharing.</li>
</ul>
<p>Nobody is trying to claim that Flatpak solves all of the problems at once, or that what we have is anywhere near perfect or completely secure, but I think what we have is pretty damn cool (I just wish we’d had it 10 years ago!). Even just in the security space, the overall effort we need is huge, but this is a journey that we are happy to be embarking together with the whole Linux desktop community. Thanks for reading, trying it out, and lending us a hand.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://wingolog.org/archives/2018/10/11/heap-object-representation-in-spidermonkey"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">heap object representation in spidermonkey</span></a><div class="lastUpdated">2018年10月11日 22:33</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div><p>I was having a look through SpiderMonkey's source code today and found something interesting about how it represents heap objects and wanted to share.</p><p>I was first looking to see how to implement <a href="https://tc39.github.io/proposal-bigint/">arbitrary-length integers ("bigints")</a> by storing the digits inline in the allocated object.  (I'll use the term "object" here, but from JS's perspective, bigints are rather <i>values</i>; they don't have identity.  But I digress.)  So you have a header indicating how many words it takes to store the digits, and the digits follow.  This is how <a href="https://trac.webkit.org/browser/webkit/trunk/Source/JavaScriptCore/runtime/JSBigInt.cpp#L1551">JavaScriptCore</a> and <a href="https://chromium.googlesource.com/v8/v8.git/+/master/src/objects/bigint.cc#1866">V8</a> implementations of bigints work.</p><p>Incidentally, JSC's implementation was taken from V8.  V8's was taken from Dart.  Dart's was taken from Go.  We might take SpiderMonkey's from Scheme48.  Good times, right??</p><p>When seeing if SpiderMonkey could use this same strategy, I couldn't find how to make a variable-sized GC-managed allocation.  It turns out that in SpiderMonkey you can't do that!  SM's memory management system wants to work in terms of fixed-sized "cells".  Even for objects that store properties inline in named slots, that's implemented in terms of standard cell sizes.  So if an object has 6 slots, it might be implemented as instances of cells that hold 8 slots.</p><p>Truly variable-sized allocations seem to be managed off-heap, via malloc or other allocators.  I am not quite sure how this works for GC-traced allocations like arrays, but let's assume that somehow it does.</p><p>Anyway, the point of this blog post.  I was looking to see which part of SpiderMonkey reserves space for type information.  For example, almost all objects in V8 start with a "map" word.  This is the object's "hidden class".  To know what kind of object you've got, you look at the map word.  That word points to information corresponding to a class of objects; it's not available to store information that might vary between objects of that same class.</p><p>Interestingly, SpiderMonkey doesn't have a map word!  Or at least, it doesn't have them on all allocations.  Concretely, BigInt values don't need to reserve space for a map word.  I can start storing data right from the beginning of the object.</p><p>But how can this work, you ask?  How does the engine know what the type of some arbitrary object is?</p><p>The answer has a few interesting wrinkles.  Firstly I should say that for objects that need hidden classes -- e.g. generic JavaScript objects -- there is indeed a map word.  SpiderMonkey calls it a "Shape" instead of a "map" or a "hidden class" or a "structure" (as in JSC), but it's there, for that subset of objects.</p><p>But not all heap objects need to have these words.  Strings, for example, are values rather than objects, and in SpiderMonkey they just have a small type code rather than a map word.  But you know it's a string rather than something else in two ways: one, for "newborn" objects (those in the nursery), the <a href="https://hg.mozilla.org/mozilla-central/file/tip/js/src/gc/Cell.h#l74">GC reserves a bit to indicate whether the object is a string or not</a>.  (Really: it's specific to strings.)</p><p>For objects promoted out to the heap ("tenured" objects), <a href="https://hg.mozilla.org/mozilla-central/file/tip/js/src/gc/Cell.h#l367">objects of similar kinds are allocated in the same memory region</a> (in kind-specific "arenas").  There are about a dozen <a href="https://hg.mozilla.org/mozilla-central/file/tip/js/public/TraceKind.h#l38">trace kinds</a>, corresponding to arena kinds.  To get the kind of object, you find its arena by rounding the object's address down to the arena size, then look at the arena to see what kind of objects it has.</p><p>There's another cell bit reserved to indicate that an object has been moved, and that the rest of the bits have been overwritten with a forwarding pointer.  These two reserved bits mostly don't conflict with any use a derived class might want to make from the first word of an object; if the derived class uses the first word for integer data, it's easy to just reserve the bits.  If the first word is a pointer, then it's probably always aligned to a 4- or 8-byte boundary, so the low bits are zero anyway.</p><p>The upshot is that while we won't be able to allocate digits inline to BigInt objects in SpiderMonkey in the general case, we won't have a per-object map word overhead; and we can optimize the common case of digits requiring only a word or two of storage to have the digit pointer point to inline storage.  GC is about compromise, and it seems this can be a good one.</p><p>Well, that's all I wanted to say.  Looking forward to getting BigInt turned on upstream in Firefox!</p></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-10-09T13:30:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer Conference 2018: Talks Abstracts and Speakers Biographies now available</span></a><div class="lastUpdated">2018年10月9日 21:30</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer Conference team is pleased to announce that talk abstracts and
speaker biographies are now available for this year's lineup of talks and
speakers, covering again an exciting range of topics!
    </p><p>
The <b>GStreamer Conference 2018</b> will take place on <b>25-26 October 2018 in
Edinburgh (Scotland)</b> just after the Embedded Linux Conference Europe (ELCE).
    </p><p>
Details about the conference and how to register can be found on the
<a href="https://gstreamer.freedesktop.org/conference/2018/">conference website</a>.
    </p><p>
    <b>This year's topics and speakers:</b>
    </p><ul>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#nvidia-deepstream-sdk">Introduction to DeepStreamSDK</a><br /><small><i>Tushar Khinvasara, Nvidia</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#video4linux-codecs">Discovering Video4Linux CODECs</a><br /><small><i>Nicolas Dufresne &amp; Ezequiel Garcia, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#cerbero">Taming the three-headed monster</a><br /><small><i>Nirbheek Chauhan, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#nnstreamer-neural-networks-as-filters">NNStreamer: Neural Networks as GStreamer Filters</a><br /><small><i>MyungJoo Ham, Samsung</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#gstreamer-and-rust">What's new with GStreamer &amp; Rust</a><br /><small><i>Sebastian Dröge, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#d3dx-video-game-streaming-on-windows">D3Dx Video Game Streaming on Windows</a><br /><small><i>Florian Nierhaus, Bebo</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#profiling-hawktracer">Profiling GStreamer applications with HawkTracer and tracing subsystem</a><br /><small><i>Marcin Kolny, Amazon</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#ci-for-embedded">GStreamer CI for embedded platforms</a><br /><small><i>Olivier Crête, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#webkitgtk-and-wpe">Multimedia support in WebKitGTK and WPE, current status and plans</a><br /><small><i>Philippe Normand, Igalia</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#servo-webaudio-rust">Using GStreamer for Servo's WebAudio implementation in Rust</a><br /><small><i>Manish Goregaokar, Mozilla</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#praxis-live-java-bindings">PraxisLIVE, PraxisCORE and the Java bindings for GStreamer</a><br /><small><i>Neil C Smith</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#state-of-the-union">GStreamer State of the Union</a><br /><small><i>Tim-Philipp Müller, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#cloud-based-live-video-handling">GStreamer for cloud-based live video handling</a><br /><small><i>Matthew Clark, BBC</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#streams-and-collections">Streams and collections: we're not done yet!</a><br /><small><i>Edward Hervey, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#non-interleaved-audio">Non-interleaved audio in GStreamer</a><br /><small><i>George Kiagiadakis, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#trust-but-verify">Trust but verify. Our road to robust multimedia and graphics stack verification (aka Multimedia testing on the budget for everyone)</a><br /><small><i>Michał Budzyński &amp; Marcin Kidzinski, Samsung</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#thread-sharing">When adding more threads adds more problems - Thread-sharing between elements in GStreamer</a><br /><small><i>Sebastian Dröge, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#gst-inference">GstInference: A GStreamer Deep Learning Framework</a><br /><small><i>Jose Manuel Jimenez, RidgeRun</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#deep-neural-networks">Bringing Deep Neural Networks to GStreamer</a><br /><small><i>Stian Selnes, Pexip</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#android-camera-source-2">Android camera source 2 - a continuation story</a><br /><small><i>Justin Kim, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#post-mortem-debugging">Post Mortem GStreamer Debugging with Gdb and Python</a><br /><small><i>Michael Olbrich, Pengutronix</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#asio">In which the protagonist explores why ASIO is still alive</a><br /><small><i>Nirbheek Chauhan, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#microsoft-teams-connector">Microsoft Teams Connector</a><br /><small><i>Håvard Graff, Pexip</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#closed-captions-in-gstreamer">Closed Captions in GStreamer</a><br /><small><i>Edward Hervey, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#video-editing">Video Editing with GStreamer, status update and plans for the future</a><br /><small><i>Thibault Saunier, Igalia</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#pipewire">PipeWire wants to be your audio server too</a><br /><small><i>Wim Taymans, RedHat</i></small></li>
    </ul>
  <p></p><p>
    <b>Lightning Talks:</b>
    </p><ul>
    <li>gst-mfx, gst-msdk and the Intel Media SDK: an update <i>(provisional title)</i> <small><br /><i>Haihao Xiang, Intel</i></small></li>
    <li>Improved flexibility and stability in GStreamer V4L2 support <small><br /><i>Nicolas Dufresne, Collabora</i></small></li>
    <li>GstQTOverlay <small><br /><i>Carlos Aguero, RidgeRun</i></small></li>
    <li>Documenting GStreamer <small><br /><i>Mathieu Duponchelle, Centricular</i></small></li>
    <li>GstCUDA <small><br /><i>Jose Jimenez-Chavarria, RidgeRun</i></small></li>
    <li>GstWebRTCBin in the real world <small><br /><i>Mathieu Duponchelle, Centricular</i></small></li>
    <li>Servo and GStreamer <small><br /><i>Víctor Jáquez, Igalia</i></small></li>
    <li>Interoperability between GStreamer and DirectShow <small><br /><i>Stéphane Cerveau, Fluendo</i></small></li>
    <li>Interoperability between GStreamer and FFMPEG <small><br /><i>Marek Olejnik, Fluendo</i></small></li>
    <li>Encrypted Media Extensions with GStreamer in WebKit <small><br /><i>Xabier Rodríguez Calvar, Igalia</i></small></li>
    <li>DataChannels in GstWebRTC <small><br /><i>Matthew Waters, Centricular</i></small></li>
    <li>Me TV – a journey from C and Xine to Rust and GStreamer, via D <small><br /><i>Russel Winder</i></small></li>
    <li>GStreamer pipeline on webOS OSE <small><br /><i>Jimmy Ohn (온용진), LG Electronics</i></small></li>
    <li>...and many more</li>
    <li>...</li>
    <li><i>Submit <u>your</u> lightning talk now!</i></li>
    </ul>
  <p></p><p>
Many thanks to our sponsors, <a href="https://www.collabora.com/">Collabora</a>,
<a href="https://www.pexip.com/">Pexip</a>, <a href="https://igalia.com/">Igalia</a>,
<a href="https://fluendo.com/">Fluendo</a>, <a href="https://www.facebook.com/">Facebook</a>,
<a href="https://centricular.com/">Centricular</a> and <a href="https://zeiss.com/">Zeiss</a>,
without whom the conference would not be possible in this form. And to
<a href="http://www.ubicast.eu/">Ubicast</a> who will be recording the talks again.
  </p><p>
Considering becoming a sponsor? Please check out our
<a href="https://gstreamer.freedesktop.org/conference/2018/gstreamer-conference-sponsor-brief-2018.pdf">sponsor brief</a>.
  </p><p>
We hope to see you all in Edinburgh in October! Don't forget to <a href="https://ti.to/gstreamer/gstreamer-conference-2018">register</a>!
  </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://k-d-w.org/blog/108/scikit-survival-060-released"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">scikit-survival 0.6.0 released</span></a><div class="lastUpdated">2018年10月8日 0:14</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><span class="field field-name-title field-formatter-string field-type-string field-label-hidden">scikit-survival 0.6.0 released</span>
<div class="clearfix text-formatted field field-node--body field-formatter-text-default field-name-body field-type-text-with-summary field-label-hidden has-single"><div class="field__items"><div class="field__item"><div class="tex2jax_process"><p>Today, I released <a href="https://github.com/sebp/scikit-survival">scikit-survival 0.6.0</a>. This release is long overdue and adds support for NumPy 1.14 and pandas up to 0.23. In addition, the new class <a href="https://scikit-survival.readthedocs.io/en/latest/generated/sksurv.util.Surv.html#sksurv.util.Surv">sksurv.util.Surv</a> makes it easier to construct a structured array from NumPy arrays, lists, or a pandas data frame. The examples below showcase how to create a structured array for the dependent variable.</p>
<p>First, we can construct a structered array from a list of boolean event indicators and a list of integers for the observed time:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">import</span> pandas
<span class="kw1">from</span> sksurv.<span class="me1">util</span> <span class="kw1">import</span> Surv
 
y <span class="sy0">=</span> Surv.<span class="me1">from_arrays</span><span class="br0">(</span><span class="br0">[</span><span class="kw2">True</span><span class="sy0">,</span> <span class="kw2">False</span><span class="sy0">,</span> <span class="kw2">False</span><span class="sy0">,</span> <span class="kw2">True</span><span class="sy0">,</span> <span class="kw2">True</span><span class="br0">]</span><span class="sy0">,</span> <span class="br0">[</span><span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">19</span><span class="sy0">,</span> <span class="nu0">11</span><span class="sy0">,</span> <span class="nu0">6</span><span class="sy0">,</span> <span class="nu0">9</span><span class="br0">]</span><span class="br0">)</span></pre></div></div>
<p>which equals</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">y <span class="sy0">=</span> numpy.<span class="kw3">array</span><span class="br0">(</span><span class="br0">[</span><span class="br0">(</span> <span class="kw2">True</span><span class="sy0">,</span>  <span class="nu0">1</span>.<span class="br0">)</span><span class="sy0">,</span> <span class="br0">(</span><span class="kw2">False</span><span class="sy0">,</span> <span class="nu0">19</span>.<span class="br0">)</span><span class="sy0">,</span> <span class="br0">(</span><span class="kw2">False</span><span class="sy0">,</span> <span class="nu0">11</span>.<span class="br0">)</span><span class="sy0">,</span> <span class="br0">(</span> <span class="kw2">True</span><span class="sy0">,</span>  <span class="nu0">6</span>.<span class="br0">)</span><span class="sy0">,</span>
                 <span class="br0">(</span> <span class="kw2">True</span><span class="sy0">,</span>  <span class="nu0">9</span>.<span class="br0">)</span><span class="br0">]</span><span class="sy0">,</span> dtype<span class="sy0">=</span><span class="br0">[</span><span class="br0">(</span><span class="st0">'event'</span><span class="sy0">,</span> <span class="st0">'?'</span><span class="br0">)</span><span class="sy0">,</span> <span class="br0">(</span><span class="st0">'time'</span><span class="sy0">,</span> <span class="st0">'</span><span class="br0">)</span><span class="br0">]</span><span class="br0">)</span></pre></div></div>
<p>Alternatively, we can use a 0/1 valued list for the event indicator:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">y <span class="sy0">=</span> Surv.<span class="me1">from_arrays</span><span class="br0">(</span><span class="br0">[</span><span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">0</span><span class="sy0">,</span> <span class="nu0">0</span><span class="sy0">,</span> <span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">1</span><span class="br0">]</span><span class="sy0">,</span> <span class="br0">[</span><span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">19</span><span class="sy0">,</span> <span class="nu0">11</span><span class="sy0">,</span> <span class="nu0">6</span><span class="sy0">,</span> <span class="nu0">9</span><span class="br0">]</span><span class="br0">)</span></pre></div></div>
<p>Finally, if event indicator and observed time are stored in a pandas data frame,<br />
we can just use <span class="geshifilter"><code class="text geshifilter-text">Surv.from_dataframe</code></span> and tell it what columns to use:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">data <span class="sy0">=</span> pandas.<span class="me1">DataFrame</span><span class="br0">(</span><span class="br0">{</span><span class="st0">"some_event"</span>: <span class="br0">[</span><span class="kw2">True</span><span class="sy0">,</span> <span class="kw2">False</span><span class="sy0">,</span> <span class="kw2">False</span><span class="sy0">,</span> <span class="kw2">True</span><span class="sy0">,</span> <span class="kw2">True</span><span class="br0">]</span><span class="sy0">,</span>
                         <span class="st0">"time_of_event"</span>: <span class="br0">[</span><span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">19</span><span class="sy0">,</span> <span class="nu0">11</span><span class="sy0">,</span> <span class="nu0">6</span><span class="sy0">,</span> <span class="nu0">9</span><span class="br0">]</span><span class="br0">}</span><span class="br0">)</span>
y <span class="sy0">=</span> Surv.<span class="me1">from_dataframe</span><span class="br0">(</span><span class="st0">"some_event"</span><span class="sy0">,</span> <span class="st0">"time_of_event"</span><span class="sy0">,</span> data<span class="br0">)</span></pre></div></div>
<p>The <em>some_event</em> column can also be 0/1 valued, of course.</p>
<h2>Download</h2>
<p>You can install the latest version via Anaconda (Linux, OSX and Windows):</p>
<p></p><div class="geshifilter"><div class="bash geshifilter-bash"><pre class="de1">conda <span class="kw2">install</span> <span class="re5">-c</span> sebp scikit-survival</pre></div></div>
<p>or via pip:</p>
<p></p><div class="geshifilter"><div class="bash geshifilter-bash"><pre class="de1">pip <span class="kw2">install</span> <span class="re5">-U</span> scikit-survival</pre></div></div>
</div></div></div>
</div>
<span class="field field-name-uid field-formatter-author field-type-entity-reference field-label-hidden"><span>sebp</span></span>
<span class="field field-name-created field-formatter-timestamp field-type-created field-label-hidden">Sun, 10/07/2018 - 18:14</span>
<a name="comments" id="comments"></a>
  <h1 class="begin-comments">Comments</h1></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/10/05/gstreamer-conference-2018/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer Conference 2018</span></a><div class="lastUpdated">2018年10月6日 1:08</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>For the 9th time this year there will be the <a href="https://gstreamer.freedesktop.org/conference/2018/">GStreamer Conference</a>. This year it will be in Edinburgh, UK right after the Embedded Linux Conference Europe, on the 25th of 26th of October. The GStreamer Conference is always a lot of fun with a wide variety of talks around Linux and multimedia, not all of them tied to GStreamer itself, for instance in the past we had a lot of talks about PulseAudio, V4L, OpenGL and Vulkan and new codecs.This year I am really looking forward to talks such as the DeepStream talk by NVidia, Bringing Deep Neural Networks to GStreamer by Pexip and D3Dx Video Game Streaming on Windows by Bebo, to mention a few.</p>
<p> For a variety of reasons I missed the last couple of conferences, but this year I will be back in attendance and I am really looking forward to it. In fact it will be the first GStreamer Conference I am attending that I am not the organizer for, so it will be nice to really be able to just enjoy the conference and the hallway track this time. </p>
<p>So if you haven’t booked yourself in already I strongly recommend going to the <a href="https://gstreamer.freedesktop.org/conference/2018/">GStreamer Conference website</a> and getting yourself signed up to attend. </p>
<p>See you all in Edinburgh!</p>
<p>Also looking forward to seeing everyone attending the <a href="https://blogs.gnome.org/uraeus/2018/09/24/getting-the-team-together-to-revolutionize-linux-audio/">PipeWire Hackfest</a> happening right after the GStreamer Conference.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-10-02T23:30:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.14.4 stable bug fix release</span></a><div class="lastUpdated">2018年10月3日 7:30</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce another bug fix
release in the stable 1.14 release series of your favourite cross-platform
multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from 1.14.x.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.14/#1.14.4">/releases/1.14/</a>
for the details.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Download tarballs directly here:
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.14.4.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.14.4.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.14.4.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.4.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.14.4.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.4.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.4.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.14.4.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.14.4.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.14.4.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.14.4.tar.xz">gstreamer-sharp</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.14.4.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.14.4.tar.xz">gst-omx</a>.
        </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/09/24/getting-the-team-together-to-revolutionize-linux-audio/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Getting the team together to revolutionize Linux audio</span></a><div class="lastUpdated">2018年9月25日 3:31</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>So anyone reading my blog posts would probably have picked up on my excitement for the <a href="http://www.pipewire.org/">PipeWire</a> project, the effort to unify the world of Linux audio, add an equivalent video bit and provide multimedia handling capabilities to containerized applications. The video part as I have mentioned before was the critical first step and that is starting to look really good with the screen sharing functionality in GNOME shell already using PipeWire and equivalent PipeWire support being added to KDE by Jan Grulich. We have internal patches for both Firefox and Chrome(ium) which we are polishing up to propose them upstream, but we will in the meantime offer them as downstream patches in Fedora as soon as they are ready for primetime. Once those patches are deployed you should have any browser based desktop sharing software, like Google Hangouts, working fully under Wayland (and X).</p>
<p>With the video part of PipeWire already in production we decided the time has come to try to accelerate the development of the audio bits. So PipeWire creator Wim Taymans, PulseAudio developer Arun Raghavan and myself decided to try to host a PipeWire hackfest this fall to bring together many of the core Linux audio developers to try to hash out a plan and a roadmap. So I am very happy to say that at the end of October we will have a gathering in Edinburgh to work on this and the critical people we where hoping to have there are coming. Filipe Coelho who is the current lead developer on Jack will be there alongside Arun Raghavan, Colin Guthrie and Tanu Kaskinen from PulseAudio, Bastien Nocera from the GNOME project and Jan Grulich from KDE will be there representing desktop integration and finally Nirbheek Chauhan, Nicolas Dufresne and George Kiagiadakis from the GStreamer project. I think we have about the right amount of people for this to be productive and at the same time have representation from everyone who needs to be there, so I am feeling very optimistic that we can come out of this event with both a plan for what we want to do and the right people involved to make it happen. The idea that we can have a shared infrastructure for consumer level audio and pro-audio under Linux really excites me and I do believe that if we do this right Linux will take a huge step forward as a natural home for pro-audio desktop users.</p>
<p>A big thanks you to the GNOME Foundation for sponsoring this event and allow us to bring all this people together! </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-09-19T17:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer Conference 2018: Schedule of Talks and Speakers available</span></a><div class="lastUpdated">2018年9月20日 1:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer Conference team is pleased to announce this year's lineup
of talks and speakers covering again an exciting range of topics!
    </p><p>
The <b>GStreamer Conference 2018</b> will take place on <b>25-26 October 2018 in
Edinburgh (Scotland)</b> just after the Embedded Linux Conference Europe (ELCE).
    </p><p>
Details about the conference and how to register can be found on the
<a href="https://gstreamer.freedesktop.org/conference/2018/">conference website</a>.
    </p><p>
    <b>This year's topics and speakers:</b>
    </p><ul>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#nvidia-deepstream-sdk">Introduction to DeepStreamSDK</a><br /><small><i>Tushar Khinvasara, Nvidia</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#video4linux-codecs">Discovering Video4Linux CODECs</a><br /><small><i>Nicolas Dufresne &amp; Ezequiel Garcia, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#cerbero">Taming the three-headed monster</a><br /><small><i>Nirbheek Chauhan, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#nnstreamer-neural-networks-as-filters">NNStreamer: Neural Networks as GStreamer Filters</a><br /><small><i>MyungJoo Ham, Samsung</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#gstreamer-and-rust">What's new with GStreamer &amp; Rust</a><br /><small><i>Sebastian Dröge, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#d3dx-video-game-streaming-on-windows">D3Dx Video Game Streaming on Windows</a><br /><small><i>Florian Nierhaus, Bebo</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#profiling-hawktracer">Profiling GStreamer applications with HawkTracer and tracing subsystem</a><br /><small><i>Marcin Kolny, Amazon</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#ci-for-embedded">GStreamer CI for embedded platforms</a><br /><small><i>Olivier Crête, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#webkitgtk-and-wpe">Multimedia support in WebKitGTK and WPE, current status and plans</a><br /><small><i>Philippe Normand, Igalia</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#servo-webaudio-rust">Using GStreamer for Servo's WebAudio implementation in Rust</a><br /><small><i>Manish Goregaokar, Mozilla</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#praxis-live-java-bindings">PraxisLIVE, PraxisCORE and the Java bindings for GStreamer</a><br /><small><i>Neil C Smith</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#state-of-the-union">GStreamer State of the Union</a><br /><small><i>Tim-Philipp Müller, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#cloud-based-live-video-handling">GStreamer for cloud-based live video handling</a><br /><small><i>Matthew Clark, BBC</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#streams-and-collections">Streams and collections: we're not done yet!</a><br /><small><i>Edward Hervey, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#non-interleaved-audio">Non-interleaved audio in GStreamer</a><br /><small><i>George Kiagiadakis, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#trust-but-verify">Trust but verify. Our road to robust multimedia and graphics stack verification (aka Multimedia testing on the budget for everyone)</a><br /><small><i>Michał Budzyński &amp; Marcin Kidzinski, Samsung</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#thread-sharing">When adding more threads adds more problems - Thread-sharing between elements in GStreamer</a><br /><small><i>Sebastian Dröge, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#gst-inference">GstInference: A GStreamer Deep Learning Framework</a><br /><small><i>Jose Manuel Jimenez, RidgeRun</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#deep-neural-networks">Bringing Deep Neural Networks to GStreamer</a><br /><small><i>Stian Selnes, Pexip</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#android-camera-source-2">Android camera source 2 - a continuation story</a><br /><small><i>Justin Kim, Collabora</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#post-mortem-debugging">Post Mortem GStreamer Debugging with Gdb and Python</a><br /><small><i>Michael Olbrich, Pengutronix</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#asio">In which the protagonist explores why ASIO is still alive</a><br /><small><i>Nirbheek Chauhan, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#microsoft-teams-connector">Microsoft Teams Connector</a><br /><small><i>Håvard Graff, Pexip</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#closed-captions-in-gstreamer">Closed Captions in GStreamer</a><br /><small><i>Edward Hervey, Centricular</i></small></li>
    <li><a href="https://gstreamer.freedesktop.org/conference/2018/talks-and-speakers.html#video-editing">Video Editing with GStreamer, status update and plans for the future</a><br /><small><i>Thibault Saunier, Igalia</i></small></li>
    </ul>
  <p></p><p>
    <b>Lightning Talks:</b>
    </p><ul>
    <li>gst-mfx, gst-msdk and the Intel Media SDK: an update <i>(provisional title)</i> <small><br /><i>Haihao Xiang, Intel</i></small></li>
    <li>Improved flexibility and stability in GStreamer V4L2 support <small><br /><i>Nicolas Dufresne, Collabora</i></small></li>
    <li>GstQTOverlay <small><br /><i>Carlos Aguero, RidgeRun</i></small></li>
    <li>Documenting GStreamer <small><br /><i>Mathieu Duponchelle, Centricular</i></small></li>
    <li>GstCUDA <small><br /><i>Jose Jimenez-Chavarria, RidgeRun</i></small></li>
    <li>GstWebRTCBin in the real world <small><br /><i>Mathieu Duponchelle, Centricular</i></small></li>
    <li>Servo and GStreamer <small><br /><i>Víctor Jáquez, Igalia</i></small></li>
    <li>Interoperability between GStreamer and DirectShow <small><br /><i>Stéphane Cerveau, Fluendo</i></small></li>
    <li>Interoperability between GStreamer and FFMPEG <small><br /><i>Marek Olejnik, Fluendo</i></small></li>
    <li>Encrypted Media Extensions with GStreamer in WebKit <small><br /><i>Xabier Rodríguez Calvar, Igalia</i></small></li>
    <li>DataChannels in GstWebRTC <small><br /><i>Matthew Waters, Centricular</i></small></li>
    <li>...and many more</li>
    <li>...</li>
    <li><i>Submit <u>your</u> lightning talk now!</i></li>
    </ul>
  <p></p><p>
Full talk abstracts and speaker biographies will be published shortly.
  </p><p>
Many thanks to our sponsors, <a href="https://www.collabora.com/">Collabora</a>,
<a href="https://igalia.com/">Igalia</a>, <a href="https://fluendo.com/">Fluendo</a>,
<a href="https://www.facebook.com/">Facebook</a>, <a href="https://centricular.com/">Centricular</a>
and <a href="https://zeiss.com/">Zeiss</a>, without whom the conference would not
be possible in this form. And to <a href="http://www.ubicast.eu/">Ubicast</a> who
will be recording the talks again.
  </p><p>
Considering becoming a sponsor? Please check out our
<a href="https://gstreamer.freedesktop.org/conference/2018/gstreamer-conference-sponsor-brief-2018.pdf">sponsor brief</a>.
  </p><p>
We hope to see you all in Edinburgh in October! Don't forget to <a href="https://ti.to/gstreamer/gstreamer-conference-2018">register</a>!
  </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-09-16T20:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.14.3 stable bug fix release</span></a><div class="lastUpdated">2018年9月17日 4:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce another bug fix
release in the stable 1.14 release series of your favourite cross-platform
multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from 1.14.x.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.14/#1.14.3">/releases/1.14/</a>
for the details.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Download tarballs directly here:
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.14.3.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.14.3.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.14.3.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.3.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.14.3.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.3.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.3.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.14.3.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.14.3.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.14.3.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.14.3.tar.xz">gstreamer-sharp</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.14.3.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.14.3.tar.xz">gst-omx</a>.
        </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://coaxion.net/blog/2018/09/gstreamer-rust-bindings-0-12-and-gstreamer-plugin-0-3-release/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer Rust bindings 0.12 and GStreamer Plugin 0.3 release</span></a><div class="lastUpdated">2018年9月10日 19:41</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>After almost 6 months, a new release of the <a href="https://github.com/sdroege/gstreamer-rs" rel="noopener" target="_top">GStreamer Rust bindings</a> and the <a href="https://github.com/sdroege/gst-plugin-rs" rel="noopener" target="_top">GStreamer plugin writing infrastructure</a> for Rust is out. As usual this was coinciding with the <a href="http://gtk-rs.org/blog/2018/09/09/new-release.html" rel="noopener" target="_top">release of all the gtk-rs crates</a> to make use of all the new features they contain.</p>
<p>Thanks to all the contributors of both gtk-rs and the GStreamer bindings for all the nice changes that happened over the last 6 months!</p>
<p>And as usual, if you find any bugs please report them and if you have any questions let me know.</p>
<h4>GStreamer Bindings</h4>
<p>For the full changelog check <a href="https://github.com/sdroege/gstreamer-rs/blob/0.12/gstreamer/CHANGELOG.md" rel="noopener" target="_top">here</a>.</p>
<p>Most changes this time were internally, especially because many user-facing changes (like <em>Debug</em> impls for various types) were already backported to the minor releases in the 0.11 release series.</p>
<h5>WebRTC</h5>
<p>The biggest change this time is probably the inclusion of bindings for the <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html" rel="noopener" target="_top">GStreamer WebRTC library</a>.</p>
<p>This allows using building all kinds of <a href="https://webrtc.org/" rel="noopener" target="_top">WebRTC</a> applications outside the browser (or providing a WebRTC implementation for a browser), and while not as full-featured as Google’s own implementation, this interoperates well with the various browsers and generally works much better on embedded devices.</p>
<p>A small example application in Rust is available <a href="https://github.com/centricular/gstwebrtc-demos/tree/master/sendrecv/gst-rust" rel="noopener" target="_top">here</a>.</p>
<h5>Serde</h5>
<p>Optionally, <a href="https://serde.rs/" rel="noopener" target="_top">serde</a> trait implementations for the <em>Serialize</em> and <em>Deserialize</em> trait can be enabled for various fundamental GStreamer types, including caps, buffers, events, messages and tag lists. This allows serializing them into any format that can be handled by serde (which are many!), and deserializing them back to normal Rust structs.</p>
<h5>Generic Tag API</h5>
<p>Previously only a strongly-typed tag API was exposed that made it impossible to use the wrong data type for a specific tag, e.g. code that tries to store a string for the track number or an integer for the title would simply not compile:</p>
<p></p><pre class="crayon-plain-tag">let mut tags = gst::TagList::new();
{
    let tags = tags.get_mut().unwrap();
    tags.add::&lt;Title&gt;(&amp;"some title", gst::TagMergeMode::Append);
    tags.add::&lt;TrackNumber&gt;(&amp;12, gst::TagMergeMode::Append);
}</pre><p> </p>
<p>While this is convenient, it made it rather complicated to work with tag lists if you only wanted to handle them in a generic way. For example by iterating over the tag list and simply checking what kind of tags are available. To solve that, a new generic API was added in addition. This works on <em>glib::Value</em>s, which can store any kind of type, and using the wrong type for a specific tag would simply cause an error at runtime instead of compile-time.</p>
<p></p><pre class="crayon-plain-tag">let mut tags = gst::TagList::new();
{
    let tags = tags.get_mut().unwrap();
    tags.add_generic(&amp;gst::tags::TAG_TITLE, &amp;"some title", gst::TagMergeMode::Append)
.expect("wrong type for title tag");
    tags.add_generic(&amp;gst::tags::TAG_TRACK_NUMBER, &amp;12, gst::TagMergeMode::Append)
.expect("wrong type for track number tag");
}</pre><p> </p>
<p>This also greatly simplified the serde serialization/deserialization for tag lists.</p>
<h4>GStreamer Plugins</h4>
<p>For the full changelog check <a href="https://github.com/sdroege/gst-plugin-rs/blob/0.3/gst-plugin/CHANGELOG.md" rel="noopener" target="_top">here</a>.</p>
<h5>gobject-subclass</h5>
<p>The main change this time is that all the generic <a href="https://developer.gnome.org/gobject/stable/" rel="noopener" target="_top">GObject</a> subclassing infrastructure was moved out of the <em>gst-plugin</em> crate and moved to its own <em><a href="https://github.com/gtk-rs/gobject-subclass" rel="noopener" target="_top">gobject-subclass</a></em> crate as part of the gtk-rs organization.</p>
<p>As part of this, some major refactoring has happened that allows subclassing more different types but also makes it simpler to add new types. There are also experimental crates for adding some subclassing support to <a href="https://github.com/gtk-rs/gobject-subclass/tree/master/gio-subclass" rel="noopener" target="_top">gio</a> and <a href="https://github.com/sdroege/gtk-subclass/" rel="noopener" target="_top">gtk</a>, and a <a href="https://github.com/gtk-rs/gir/pull/604" rel="noopener" target="_top">PR</a> for autogenerating part of the code via the gir code generator.</p>
<h5>More classes!</h5>
<p>The other big addition this time is that it’s now possible to subclass GStreamer <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstPad.html" rel="noopener" target="_top">Pads</a> and GhostPads, to implement the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstChildProxy.html" rel="noopener" target="_top">ChildProxy</a> interface and to subclass the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstAggregator.html" rel="noopener" target="_top">Aggregator</a> and AggregatorPad class.</p>
<p>This now allows to write custom mixer/muxer-style elements (or generally elements that have multiple sink pads) in Rust via the Aggregator base class, and to have custom pad types for elements to allow for setting custom properties on the pads (e.g. to control the opacity of a single video mixer input).</p>
<p>There is currently no example for such an element, but I’ll add a very simple video mixer to the repository some time in the next weeks and will also write a blog post about it for explaining all the steps.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/08/01/supporting-developers-on-patreon-and-similar/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Supporting developers on Patreon (and similar)</span></a><div class="lastUpdated">2018年8月1日 22:32</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>For some time now I been supporting two Linux developers on patreon. Namely <a href="https://www.patreon.com/icculus/overview">Ryan Gordon</a> of Linux game porting and SDL development fame and <a href="https://www.patreon.com/tanuk/overview">Tanu Kaskinen</a> who is a lead developer on PulseAudio these days. </p>
<p>One of the things I often think about is how we can enable more people to make a living from working on the Linux desktop and related technologies. If your reading my blog there is a good chance that you are enabling people to make a living on working on the Linux desktop by paying for RHEL Workstation subscriptions through your work. So a big thank you for that. The fact that Red Hat has paying customers for our desktop products is critical in terms of our ability to do so much of the maintenance and development work we do around the Linux Desktop and Linux graphics stack.</p>
<p>That said I do feel we need more venues than just employment by companies such as Red Hat and this is where I would love to see more people supporting their favourite projects and developers through for instance Patreon. Because unlike one of funding campaigns repeat crowdfunding like Patreon can give developers predictable income, which means they don’t have to worry about how to pay their rent or how to feed their kids.</p>
<p>So in terms of the two Patreons I support Ryan is probably the closest to being able to rely on it for his livelihood, but of course more Patreon supporters will enable Ryan to be even less reliant on payments from game makers. And Tanu’s patreon income at the moment is helping him to spend quite a bit of time on PulseAudio, but it is definitely not providing him with a living income. So if you are reading this I strongly recommend that you support  <a href="https://www.patreon.com/icculus/overview">Ryan Gordon</a> and <a href="https://www.patreon.com/tanuk/overview">Tanu Kaskinen</a> on Patreon. You don’t need to pledge a lot, I think in general it is in fact better to have many people pledging 10 dollars a Month than a few pledging hundreds, because the impact of one person coming or going is thus a lot less. And of course this is not just limited to Ryan and Tanu, search around and see if any projects or developers you personally care deeply about are using crowdfunding and support them, because if more of us did so then more people would be able to make a living of developing our favourite open source software.</p>
<p>Update: Seems I wasn’t the only one thinking about this, Flatpak <a href="https://twitter.com/FlatpakApps/status/1024608893933109248">announced today that application devs can put their crowdfunding information into their flatpaks and it will be advertised in GNOME Software</a>.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/tsaunier/2018/07/31/webkitgtk-and-wpe-gains-webrtc-support-back/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">WebKitGTK and WPE gain WebRTC support back!</span></a><div class="lastUpdated">2018年8月1日 2:06</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p><a href="https://www.w3.org/TR/webrtc/">WebRTC</a> is a w3c draft protocol that “enables rich, high-quality RTP applications to be developed for the browser, mobile platforms, and IoT devices, and allow them all to communicate via a common set of protocols”. The protocol is mainly used to provide video conferencing systems from within web browsers.</p>
<img src="appr.tc-screenshort-1.png" alt="https://appr.tc running in WebKitGTK" />https://appr.tc running in WebKitGTK
<h2>A brief history</h2>
<p>At the very beginning of the WebRTC protocol, before 2013, Google was still using WebKit in chrome and they started to implement support using <a href="https://webrtc.org/">LibWebRTC</a> but when they <a href="https://blog.chromium.org/2013/04/blink-rendering-engine-for-chromium.html">started the blink fork</a> the implementation stopped in WebKit.</p>
<p>Around 2015/2016 <a href="https://www.ericsson.com/research-blog/">Ericsson</a> and <a href="https://www.igalia.com/">Igalia</a> (later sponsored by <a href="https://www.metrological.com/">Metrological</a>) implemented <a href="https://blogs.igalia.com/xrcalvar/2016/10/03/webrtc-in-webkitwpe/">WebRTC support into WebKit</a>, but instead of using <a href="https://webrtc.org/">LibWebRTC</a> from google, <a href="https://www.openwebrtc.org/">OpenWebRTC</a> was used. This had the advantage of being implemented on top of the <a href="https://gstreamer.freedesktop.org/">GStreamer framework</a> which happens to be used for the Multimedia processing inside WebKitGTK and WebKitWPE. At that point in time, the standardization of the WebRTC protocol was still moving fast, mostly pushed by Google itself, and it was hard to be interoperable with the rest of the world. Despite of that, the WebKit/GTK/WPE WebRTC implementation started to be usable with website like <a href="https://appr.tc/">appr.tc</a> at the end of 2016.</p>
<p>Meanwhile, in late 2016, Apple decided to implement WebRTC support on top of google LibWebRTC in their ports of WebKit which led to <a href="https://webkit.org/blog/7726/announcing-webrtc-and-media-capture/">WebRTC support in WebKit announcement</a> in June 2017.</p>
<p>Later in 2017 the OpenWebRTC project lost momentum and as it was considered unmaintained, we, at <a href="https://www.igalia.com/">Igalia</a>, decided to use LibWebRTC for WebKitGTK and WebKitWPE too. At that point, the OpenWebRTC backend was <a href="https://bugs.webkit.org/show_bug.cgi?id=177868">completely removed</a>.</p>
<h2>GStreamer/LibWebRTC implementation</h2>
<p>Given that Apple had implemented a LibWebRTC based backend in WebKit, and because this library is being used by the two main web browsers (Chrome and Firefox), we decided to reuse Apple’s work to implement support in our ports based on LibWebRTC at the end of 2017. A that point, the two main APIs required to allow video conferencing with WebRTC needed to be implemented:</p>
<ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia">MediaDevices.GetUserMedia</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaStream">MediaStream</a>: Allows to retrieve Audio and Video streams from the user Cameras and Microphones (potentially more than that but those are the main use cases we cared about).</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection">RTCPeerConnection</a>: Represents a WebRTC connection between the local computer and a remote peer.</li>
</ul>
<p>As WeKit/GTK/WPE heavily relies on GStreamer for the multimedia processing, and given its flexibility, we made sure that our implementation of those APIs leverage the power of the framework and the existing integration of GStreamer in our WebKit ports.</p>
<p>Note that the whole implementation is reusing (after refactoring) big parts of the infrastructure introduced during the previously described history of WebRTC support in WebKit.</p>
<h3>GetUserMedia/MediaStream</h3>
<p>To implement that part of the API the following main components were developed:</p>
<ul>
<li>RealtimeMediaSourceCenterLibWebRTC: Main entry point for our GStreamer based LibWebRTC backend.</li>
<li>GStreamerCaptureDeviceManager: A class to list and manage local Video/Audio devices using the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gstreamer-GstDeviceMonitor.html">GstDeviceMonitor</a> API.</li>
<li>GStreamerCaptureDevice: Implementation of WebKit abstraction for capture devices, basically wrapping GstDevices.</li>
<li>GStreamerMediaStreamSource: A <a href="https://gstreamer.freedesktop.org/documentation/application-development/basics/elements.html#source-elements">GStreamer Source element</a> which wraps WebKit abstraction of MediaStreams to be used directly in a <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-plugins/html/gst-plugins-base-plugins-playbin3.html">playbin3</a> pipeline (through a custom <code>mediastream://</code> protocol). This implementation leverages latest <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gstreamer-GstStream.html">GstStream</a> APIs so it is already one foot into the future.</li>
</ul>
<p>The main commit can be found <a href="https://trac.webkit.org/changeset/232589">here</a></p>
<h3>RTCPeerConnection</h3>
<p>Enabling the PeerConnection API meant bridging previously implemented APIs and the LibWebRTC backend developed by Apple:</p>
<ul>
<li>RealtimeOutgoing/Video/Audio/SourceLibWebRTC: Passing local stream (basically from microphone or camera) to LibWebRTC to be sent to the peer.</li>
<li>RealtimeIncoming/Video/Audio/SourceLibWebRTC: Passing remote stream (from a remote peer) to the MediaStream object and in turn to the GStreamerMediaStreamSource element.</li>
</ul>
<p>On top of that and to leverage GStreamer Memory management and negotiation capabilities we implemented encoders and decoder for LibWebRTC (namely GStreamerVideoEncoder and GStreamerVideoDecoders). This brings us a huge number of Hardware accelerated encoders and decoders implementations, especially on embedded devices, which is a big advantage in particular for WPE which is tuned for those platforms.</p>
<p>The main commit can be found <a href="https://trac.webkit.org/changeset/234138">here</a></p>
<p><img src="webkitwpe_webrtc_media_dataflow.png" alt="WebKitWebRTC dataflow diagram" /></p>
<h2>Conclusion</h2>
<p>While we were able to make GStreamer and LibWebRTC work well together in that implementation, using the new <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html">GstWebRTC</a> component (that is now in upstream GStreamer) as a WebRTC backend would be cleaner. Many pieces of the current implementation could be reused and it would allow us to have a simpler infrastructure and avoid having several RTP stack in the WebKitGTK and WebKitWPE ports.</p>
<p>Most of the required APIs and features have been implemented, but a few are still under development (namely <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/enumerateDevices">MediaDevices.enumerateDevices</a>, <a href="https://bugs.webkit.org/show_bug.cgi?id=169811">canvas captureStream</a> and <a href="https://bugs.webkit.org/show_bug.cgi?id=186933">WebAudio and MediaStream bridging</a>) meaning that many Web applications using WebRTC already work, but some don’t yet, we are working on those!</p>
<p>A big thanks to my employer <a href="https://www.igalia.com/">Igalia</a> and <a href="https://www.metrological.com/">Metrological</a> for sponsoring our work on that!</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/07/26/some-thoughts-on-smart-home-technology/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Some thoughts on smart home technology</span></a><div class="lastUpdated">2018年7月27日 3:40</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>A couple of Months ago we visited IKEA and saw the IKEA Trådfri smart lighting system. Since it was relatively cheap we decided to buy their starter pack and enough bulbs to make the recessed lights in our living rooms smartlight. I got it up and running and had some fun switching the light hue and turning the lights on and off from my phone.<br />
A bit later I got a Google Assistant speaker at Google I/O and the system suddenly became somewhat more useful as I could control the lights by calling out to the google assistant. I was also able to connect our AC system thermostats to the google assistant so we could change the room temperature using voice commands.<br />
As a result of this I ended up reading up  on smart home technologies and found that my IKEA hub and bulbs was conforming to the ZigBee standard and that I should be able to buy further Zigbee compatible devices from other vendors to extend it. So I ordered a Zigbee compatible in-wall light switch from GE and I also ordered a Zigbee compatible in-ceiling switch from a company called Nue through Amazon. Once I got these home and tried to get them up and running I found that my understanding was flawed as there are two Zigbee standards, the older Zigbee HA and the newer Zigbee LightLink. The IKEA stuff is Zigbee LightLink while at least the GE switch was Zigbee HA and thus the IKEA hub could not control it. So I ended up ordering a Samsung SmartThings hub which supports Zigbee HA, Zigbee LL and the competing system called Z-Wave. At which point I got both my IKEA lights and my two devices working with it.<br />
In the meantime I had also gotten myself a Google Assistant compatible portable aircon from FrigidAire for my home office (no part of main house) and a Google Assistant compatible fire alarm  from Nest for the same office space.</p>
<p>So having lived in my new smarter home for a while now what are my conclusions? Well first of all that we still have some way to go before this is truly seamless and obvious. I consider myself a fairly technical person, but it still took quite a bit of googling for me to be able to get everything working. Secondly a lot of the smart home stuff feel a bit gimmicky in the end. For instance the Frigidaire portable air conditioner integration with the Google Assistant is more annoying than useful. It basically requires me to start by asking to talk to Frigidaire and then listen to a ton of crap before I can even start trying to do anything. As for the lights we do actually turn them on and off quite a bit using the voice commands (at least I try to until I realize my wife has disconnected the google assistant in order to use its cable to charge her phone :). I also realized that while installing and buying the in-wall switches are a bit more costly and complicated than just getting some smart bulbs it does work a lot better as I can then control the lights using both voice and switch. Because the smart bulbs can not be turned on using voice if you have a normal switch turned off (obviously, but not something I thought about before buying). So getting something like the IKEA bulbs is a nice and cheap way to try this stuff out I don’t see it as our long term solution here. The thermostat we haven’t controlled or queried once since I did the initial testing of its connection to Google assistant.</p>
<p>All in all I have to say that the smart home tech is cute, but it is far from being essential. I might end up putting in more wall switches for the light going forward, but apart from that, having smart home support in a device is not going to drive my purchasing decisions. Maybe as the tech matures and becomes more mainstream it will also become more useful, but as it stands it mostly solves first world problems (although of course there are real gains here for various accessibility situations). </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-07-20T15:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.14.2 stable bug fix release</span></a><div class="lastUpdated">2018年7月20日 23:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce the second bug fix
release in the stable 1.14 release series of your favourite cross-platform
multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from
1.14.x.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.14/#1.14.2">/releases/1.14/</a>
for the details.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Download tarballs directly here:
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.14.2.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.14.2.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.14.2.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.2.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.14.2.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.2.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.2.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.14.2.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.14.2.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.14.2.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.14.2.tar.xz">gstreamer-sharp</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.14.2.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.14.2.tar.xz">gst-omx</a>.
        </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/07/18/an-update-from-fedora-workstation-land/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">An update from Fedora Workstation land</span></a><div class="lastUpdated">2018年7月18日 23:23</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p><strong>Battery life</strong><br />
I was very happy to see that <a href="https://www.phoronix.com/scan.php?page=news_item&amp;px=Windows-Linux-Power-Dell-XPS">Fedora Workstation 28 in the Phoronix benchmark</a> got the best power consumption on a Dell XPS 13. Improving battery life has been a priority for us and <a href="https://hansdegoede.livejournal.com/">Hans de Goede</a> has been doing some incredible work so far. And best of all; more is to come :). So if you want great battery life with Linux on your laptop then be sure to be running Fedora on your laptop! On that note and to state the obvious, be aware that Fedora Workstation adoption rates are actually a major metric for us to decide where to put our efforts, so if we see good growth in Fedora due to people enjoying the improved battery life it enables us to keep investing in improving battery life, if we don’t see the growth we will need to conclude people don’t care that much and more our investment elsewhere.</p>
<p><strong>Desktop remoting under Wayland</strong><br />
The team is also making great strides with desktop remoting under Wayland. In Fedora Workstation 29 we will have the VNC based GNOME Shell integrated desktop sharing working under Wayland thanks to the work done by Jonas Ådahl. It relies on <a href="http://www.pipewire.org/">PipeWire</a> to share you Wayland session over VNC.<br />
On a similar note Jan Grulich, Tomas Popela and Eike Rathke has been working on enabling Wayland desktop sharing through Firefox and Chromium. They are reporting good progress and actually did a video call between Firefox and Chromium last week, sharing their desktops with each other. This is enables by providing a PipeWire backend for both Firefox and Chromium. They are now working on cleaning up their patches and prepare them for submission upstream. We are also looking at providing a patched Firefox in Fedora Workstation 28 supporting this.</p>
<p><strong>PipeWire</strong><br />
Wim Taymans talked about and demonstrated the latest improvements to PipeWire during <a href="http://www.guadec.org/">GUADEC</a> last week. He now got a libpulse.so drop in replacement that allows applications like Totem and Rhythmbox to play audio through PipeWire using the PulseAudio GStreamer plugin as Pipewire now provides a libpulse.so drop in replacement. Wim also keeps improving the Jack support in PipeWire by testing Jack applications one by one and fixing corner cases as he discovers them or they are reported by the Linux pro-audio community. We also ended up ordering Wim a Sony HT-Z9F soundbar for testing as we want to ensure PipeWire has great support for passthrough, be that SPDIF, HDMI or Bluetooth. The HT-Z9F also supports LDAC audio which is a new high quality audio format for Bluetooth and we want PipeWire to have full support for it.<br />
To accelerate Pipewire development and adoption for audio we also decied to try to organize a PipeWire and Linux Audio hackfest this fall, with the goal of mapping our remaining issues and to try to bring the wider linux audio community together. So I am very happy that Arun Raghavan of PulseAudio fame agreed to be one of the co-organizer of this hackfest. Anyone interested in attending the <a href="https://wiki.gnome.org/Hackfests/PipeWire2018">PipeWire 2018 hackfest</a> either add yourself to the attendee list or contact me (contact information can be found through the hackfest page) and I be happy to add you. The primary goal is to have developers from the PulseAudio and JACK communities attend alongside Wim Taymans and Bastien Nocera so we can make sure we got everything we need on the development roadmap and try to ensure we all pull in the same direction.</p>
<p><strong>GNOME Builder</strong><br />
<a href="https://blogs.gnome.org/chergert/">Christian Hergert</a> did an update during GUADEC this year on GNOME Builder. As usual a ton of interesting stuff happening including new support for developing towards embedded devices like the upcoming Purism phone. Christian in his talk mentioned how Builder is probably the worlds first ‘Container Native IDE’ where it both is being developed with being packaged as a Flatpak in mind, but also developed with the aim of creating Flatpaks as its primary output. So a lot of effort is being put into both making sure it works well being inside a container itself, but also got all the bells and whistles for creating containers from your code. Another worthwhile point to mention is that Builder is also one of the best IDEs for doing Rust development in general!</p>
<p><strong>Game mode in Fedora</strong><br />
<a href="http://www.feralinteractive.com/en/">Feral Interactive</a>, one of the leading Linux game companies, released a tool they call <a href="https://github.com/FeralInteractive/gamemode">gamemode</a> for Linux not long ago. Since we want gamers to be first class citizens in Fedora Workstation we ended up going back and forth internally a bit about what to do about it, basically discussing if there was another way to resolve the problem even more seamlessly than gamemode. In the end we concluded that while the ideal solution would be to have the default CPU governor be able to deal with games better, we also realized that the technical challenge games posed to the CPU governor, by having a very uneven workload, is hard to resolve automatically and not something we have the resources currently to take a deep dive into. So in the end we decided that just packaging gamemode was the most reasonable way forward. So the package is lined up for the next batch update in Fedora 28 so you should soon be able to install it and for Fedora Workstation 29 we are looking at including it as part of the default install. </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://andrescolubri.net/blog/2018/06/26/experiments_with_flutter_and_dart.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Experiments with Flutter and Dart</span></a><div class="lastUpdated">2018年6月27日 7:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Some time ago I heard about the <a href="https://flutter.io/">Flutter SDK</a> for cross-platform mobile development. Having written some apps as part of my research that needed to be available for both iOS and Android, and feeling sometimes frustrated by the fact that I could run Processing sketches on iPhones, I’ve been interested in this kind of cross-platform SDKs. In fact, I used <a href="https://kivy.org/">Kivy</a> in the past to create <a href="https://play.google.com/store/apps/details?id=org.broadinstitute.ebolacare">an app</a> for prognosis of Ebola patients. I liked that Kivy is based on Python and had a simple UI toolkit. However, it was not clear to me if Kivy would allow to create native UIs on iOS and Android, and implementing the Processing API as a Python library that could be used in Kivy seemed difficult at the time (although, with the new <a href="https://github.com/p5py/p5">P5 library</a> from Abhik Pal, this may be easier now). More recently, I revisited the prognosis app and created a new version which incorporates a more sophisticated visualization of prognosis predictions, as well as patient care and management recommendations. The Android version is <a href="https://play.google.com/store/apps/details?id=org.broadinstitute.ebolarisk">here</a>, and the iOS version <a href="https://itunes.apple.com/us/app/ebola-risk/id1376937550">here</a>. In this case, I ended up creating separate projects for each version, one with Android Studio and the Android SDK from Google, and the other with XCode and the iOS SDK from Apple (and consequently, the development time doubled).</p>

<p><em>(To be fair, you can use the <a href="https://itunes.apple.com/us/app/processing-icompiler/id648955851?mt=8">Processing iCompiler</a> from <a href="http://frogg.io/">Frogg</a> to write and run Processing sketches directly on your phone, but that’s a different use scenario)</em></p>

<p>In any case, I was also aware of a number of other existing SDK/frameworks for cross-platform mobile develoment: Xamarin, React Native, Cordova, PhoneGap… but never got enough time to explore at least a few of them in some detail. <a href="https://facebook.github.io/react-native/">React Native</a> seems very popular nowadays and it is JavaScript-based (there is even a <a href="https://www.npmjs.com/package/react-p5-wrapper">p5-wrapper</a> that one can use to embed <a href="https://p5js.org/">p5.js</a> sketches into mobile apps). Coming more from a Java background, and being quite involved in the <a href="http://android.processing.org/">Processing for Android</a> project, I was looking for something closer to these programming languages/environments.</p>

<p>Flutter is based on a relatively new language called <a href="https://www.dartlang.org/">Dart</a>, which according to what I found online, Dart is easy to learn for people with experience in Java. So, I decided to give it a try and see how hard would be to write Processing-like sketches with the Dart language together with the Flutter SDK.</p>

<p>I had to get used to Flutter’s widget and layout system first, but eventually got to understand the basics of it (the online documentation is <a href="https://flutter.io/tutorials/">pretty good</a>) and to access lower-level rendering functions in Flutter. The <a href="https://docs.flutter.io/flutter/rendering/CustomPainter-class.html">CustomPainter</a> class was key to achieve this, as it allows to paint on the canvas of a widget pretty much anything you want:</p>

<div class="language-dart highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">PPainter</span> <span class="kd">extends</span> <span class="n">ChangeNotifier</span> <span class="kd">implements</span> <span class="n">CustomPainter</span> <span class="o">{</span>
  <span class="nd">@override</span>
  <span class="kt">void</span> <span class="n">paint</span><span class="o">(</span><span class="n">Canvas</span> <span class="n">canvas</span><span class="o">,</span> <span class="n">Size</span> <span class="n">size</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">draw</span><span class="o">();</span> 
  <span class="o">}</span>

  <span class="kt">void</span> <span class="n">draw</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// draw anything you want...</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Next, I had to figure out how to animate the drawing, and handle touch events. Luckily, Flutter includes <a href="https://flutter.io/animations/">several built-in classes</a> to generate UI animations, and I could use them to trigger the drawing in the custom painter on a continuous loop. Finally, <a href="https://stackoverflow.com/questions/45578209/how-to-touch-paint-a-canvas">some online code</a> gave me enough hints to implement basic input handling:</p>

<div class="language-dart highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">PWidget</span> <span class="kd">extends</span> <span class="n">StatelessWidget</span> <span class="o">{</span>
  <span class="n">PPainter</span> <span class="n">painter</span><span class="o">;</span>
  
  <span class="nd">@override</span>
  <span class="n">Widget</span> <span class="n">build</span><span class="o">(</span><span class="n">BuildContext</span> <span class="n">context</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="k">new</span> <span class="n">Container</span><span class="o">(</span>
      <span class="nl">child:</span> <span class="k">new</span> <span class="n">ClipRect</span><span class="o">(</span>
          <span class="nl">child:</span> <span class="k">new</span> <span class="n">CustomPaint</span><span class="o">(</span>
            <span class="nl">painter:</span> <span class="n">painter</span><span class="o">,</span>
            <span class="nl">child:</span> <span class="k">new</span> <span class="n">GestureDetector</span><span class="o">(</span>
              <span class="nl">onTapDown:</span> <span class="o">(</span><span class="n">details</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">painter</span><span class="o">.</span><span class="na">onTapDown</span><span class="o">(</span><span class="n">context</span><span class="o">,</span> <span class="n">details</span><span class="o">);</span>
              <span class="o">},</span>
              <span class="nl">onTapUp:</span> <span class="o">(</span><span class="n">details</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">painter</span><span class="o">.</span><span class="na">onTapUp</span><span class="o">(</span><span class="n">context</span><span class="o">,</span> <span class="n">details</span><span class="o">);</span>
              <span class="o">},</span>
            <span class="o">),</span>
          <span class="o">)</span>
      <span class="o">),</span>
    <span class="o">);</span>
  <span class="o">}</span>              

</code></pre></div></div>

<p>With these basic elements in place, I was able to start implementing a few functions from the <a href="https://processing.org/reference/">Processing API</a> in order to create a simple proof-of-concept <a href="https://github.com/codeanticode/p5.dart">Dart package</a> that encapsulates these functions and can be imported into a Flutter app. In the end, a Dart sketch in Flutter looks surprisingly similar to a Java Processing sketch:</p>

<div class="language-dart highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">MySketch</span> <span class="kd">extends</span> <span class="n">PPainter</span> <span class="o">{</span>
  <span class="kd">var</span> <span class="n">strokes</span> <span class="o">=</span> <span class="k">new</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">PVector</span><span class="o">&gt;&gt;();</span>

  <span class="kt">void</span> <span class="n">setup</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">fullScreen</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="kt">void</span> <span class="n">draw</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">background</span><span class="o">(</span><span class="n">color</span><span class="o">(</span><span class="mi">255</span><span class="o">,</span> <span class="mi">255</span><span class="o">,</span> <span class="mi">255</span><span class="o">));</span>

    <span class="n">noFill</span><span class="o">();</span>
    <span class="n">strokeWeight</span><span class="o">(</span><span class="mi">10</span><span class="o">);</span>
    <span class="n">stroke</span><span class="o">(</span><span class="n">color</span><span class="o">(</span><span class="mi">10</span><span class="o">,</span> <span class="mi">40</span><span class="o">,</span> <span class="mi">200</span><span class="o">,</span> <span class="mi">60</span><span class="o">));</span>
    <span class="k">for</span> <span class="o">(</span><span class="kd">var</span> <span class="n">stroke</span> <span class="k">in</span> <span class="n">strokes</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">beginShape</span><span class="o">();</span>
      <span class="k">for</span> <span class="o">(</span><span class="kd">var</span> <span class="n">p</span> <span class="k">in</span> <span class="n">stroke</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">vertex</span><span class="o">(</span><span class="n">p</span><span class="o">.</span><span class="na">x</span><span class="o">,</span> <span class="n">p</span><span class="o">.</span><span class="na">y</span><span class="o">);</span>
      <span class="o">}</span>
      <span class="n">endShape</span><span class="o">();</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="kt">void</span> <span class="n">mousePressed</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">strokes</span><span class="o">.</span><span class="na">add</span><span class="o">([</span><span class="k">new</span> <span class="n">PVector</span><span class="o">(</span><span class="n">mouseX</span><span class="o">,</span> <span class="n">mouseY</span><span class="o">)]);</span>
  <span class="o">}</span>

  <span class="kt">void</span> <span class="n">mouseDragged</span><span class="o">()</span> <span class="o">{</span>
    <span class="kd">var</span> <span class="n">stroke</span> <span class="o">=</span> <span class="n">strokes</span><span class="o">.</span><span class="na">last</span><span class="o">;</span>
    <span class="n">stroke</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="k">new</span> <span class="n">PVector</span><span class="o">(</span><span class="n">mouseX</span><span class="o">,</span> <span class="n">mouseY</span><span class="o">));</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>This sketch can be embedded into a minimal UI layout in Flutter, if the purpose is just to show the sketch’s output with no other UI components. This is how the sketch above looks like when running on an iPhone and and Android, side to side:</p>

<p><img src="draw-p5.dart.jpg" alt="Running a simple Processing sketch on iOS and Android" /></p>

<p>So far, the experiment proved to be quite succesful! I published the <a href="https://pub.dartlang.org/packages/p5">p5 package</a> on the Dart Pub repository, so it can be easily imported into any Flutter app. Currently, it only includes a hanful of the Processing API functions, so it is not that useful, but an promising starting point nonetheless!</p>

<p>And in the end, I was able to write a Processing sketch and run it on an iPhone and an Android device from Android Studio, which was pretty satisfying:</p>

<p><img src="androidstudio-iphone.jpg" alt="Using Android Studio to debug on an iPhone" /></p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://www.hadess.net/2018/06/thomson-8-bit-computers-history.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Thomson 8-bit computers, a history</span></a><div class="lastUpdated">2018年6月23日 0:06</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent">In March 1986, my dad was in the market for a Thomson TO7/70. I have the circled classified ads in <a href="http://www.abandonware-magazines.org/affiche_mag.php?mag=106&amp;num=2548&amp;album=oui">“Téo” issue 1</a> to prove that :)<br /><br /><a href="https://4.bp.blogspot.com/-EpbKp7NQGrA/WyzfRZ4O5FI/AAAAAAAAA4g/rAQc6s1OjrgggE-dfvehMNBiZPbf3Xo9wCLcBGAs/s1600/F_to770-2.jpg"><img src="f_to770-2.jpg" width="640" height="290" border="0" /></a><br /><br /><div><i>TO7/70 with its chiclet keyboard and optical pen, courtesy of <a href="http://mo5.com/musee-machines-to770.html">MO5.com</a></i></div><br />The “<a href="https://fr.wikipedia.org/wiki/Plan_informatique_pour_tous">Plan Informatique pour Tous</a>” was in full swing, and Thomson were supplying schools with micro-computers. My dad, as a primary school teacher, needed to know how to operate those computers, and eventually teach them to kids.<br /><br />The first thing he showed us when he got the computer, on the living room TV, was a game called “Panic” or “Panique” where you controlled a missile, protecting a town from flying saucers that flew across the screen from either side, faster and faster as the game went on. I still haven't been able to locate this game again.<br /><br />A couple of years later, the TO7/70 was replaced by a TO9, with a floppy disk, and my dad used that computer to write an educational software about top-down additions, as part of a training program run by the teachers schools (“Écoles Normales” renamed to “IUFM“ in 1990).<br /><br />After months of nagging, and some spring cleaning, he found the listings of his <a href="https://github.com/hadess/inondation-d-additions">educational software, which I've liberated</a>, with his permission. I'm currently still working out how to generate floppy disks that are usable directly in emulators. But here's an early screenshot.<br /><br /><div class="separator"><a href="https://1.bp.blogspot.com/-xnZscl6PNbg/WyzixMXH6hI/AAAAAAAAA40/PKGugEAgWtcJO1DmF8bkYW9d-60VgCL8ACLcBGAs/s1600/teo.png"><img src="teo.png" width="400" height="262" border="0" /></a></div><br />Later on, my dad got an IBM PC compatible, an <a href="http://www.old-computers.com/museum/computer.asp?c=182&amp;st=1">Olivetti PC/1</a>, on which I'd play a clone of Asteroids for hours, but that's another story. The TO9 got passed down to me, and after spending a full summer doing planning for my hot-dog and chips van business (I was 10 or 11, and I had weird hobbies already), and entering every game from the “102 Programmes pour...” series of books, the TO9 got put to the side at Christmas, replaced by a Sega Master System, using that same handy SCART connector on the Thomson monitor.<br /><br /><div class="separator"></div>But how does this concern you. Well, I've worked with <a href="https://www.youtube.com/user/RetroManCave/videos">RetroManCave</a> on a <a href="https://youtu.be/HOhK9bgQo8g">Minitel episode</a> not too long ago, and he agreed to do a history of the Thomson micro-computers. I did a fair bit of the research and fact-checking, as well as some needed repairs to the (prototype!) hardware I managed to find for the occasion. The result is this first look at the history of Thomson.<br /><br /><div class="separator"></div><br /><br />Finally, if you fancy diving into the Thomson computers, there will be an episode coming shortly about the MO5E hardware, and some games worth running on it, on the same YouTube channel.<br /><br />I'm currently working on bringing the “<a href="https://sourceforge.net/projects/teoemulator/">Teo</a>” <a href="https://github.com/flathub/flathub/pull/443">TO8D emulator to Flathub</a>, for Linux users. When that's ready, <a href="http://dcmoto.free.fr/programmes/_html/index.html">grab some games from the DCMOTO archival site</a>, and have some fun!<br /><br />I'll also be posting some nitty gritty details about Thomson repairs on <a href="https://twitter.com/Micro_Repairs">my Micro Repairs Twitter feed</a> for the more technically enclined among you.</div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://www.hadess.net/2018/06/fingerprint-reader-support-second-coming.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Fingerprint reader support, the second coming</span></a><div class="lastUpdated">2018年6月13日 3:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent">Fingerprint readers are more and more common on Windows laptops, and hardware makers would really like to not have to make a separate SKU without the fingerprint reader just for Linux, if that fingerprint reader is unsupported there.<div><br /></div><div>The original makers of those fingerprint readers just need to send patches to the libfprint Bugzilla, I hear you say, and the problem's solved!</div><div><br /></div><div>But it turns out it's pretty difficult to write those new drivers, and those patches, without an insight on how the internals of libfprint work, and what all those internal, undocumented APIs mean.</div><div><br /></div><div>Most of the drivers already present in libfprint are the results of reverse engineering, which means that none of them is a best-of-breed example of a driver, with all the unknown values and magic numbers.</div><div><br /></div><div>Let's try to fix all this!</div><div><br /></div><div><b>Step 1: fail faster</b></div><div><br /></div><div>When you're writing a driver, the last thing you want is to have to wait for your compilation to fail. We ported libfprint to <a href="http://mesonbuild.com/">meson</a> and shaved off a significant amount of time from a successful compilation. We also reduced the number of places where new drivers need to be declared to be added to the compilation.</div><div><br /></div><div><b>Step 2: make it clearer</b></div><div><br /></div><div>While <a href="http://www.stack.nl/~dimitri/doxygen/">doxygen</a> is nice because it requires very little scaffolding to generate API documentation, the output is also not up to the level we expect. We ported the documentation to <a href="https://www.gtk.org/gtk-doc/">gtk-doc</a>, which has a more readable page layout, easy support for cross-references, and gives us more control over how introductory paragraphs are laid out. See the <a href="https://fprint.freedesktop.org/libfprint-stable/">before</a> and <a href="https://fprint.freedesktop.org/libfprint-dev/">after</a> for yourselves.</div><div><br /></div><div><b>Step 3: fail elsewhere</b></div><div><br /></div><div>You created your patch locally, tested it out, and it's ready to go! But you don't know about <a href="http://git.fishsoup.net/man/git-bz.html">git-bz</a>, and you ended up attaching a patch file which you uploaded. Except you uploaded the wrong patch. Or the patch with the right name but from the wrong directory. Or you know git-bz but used the wrong commit id and uploaded another unrelated patch. This is all a bit too much.</div><div><br /></div><div>We migrated our bugs and repository for both libfprint and fprintd to <a href="https://gitlab.freedesktop.org/libfprint/libfprint/">Freedesktop.org's GitLab</a>. <a href="https://gitlab.freedesktop.org/libfprint/libfprint/merge_requests">Merge Requests</a> are <a href="https://gitlab.freedesktop.org/libfprint/libfprint/pipelines">automatically built</a>, discussions are easier to follow!</div><div><br /></div><div><b>Step 4: show it to me</b></div><div><br /></div><div>Now that we have spiffy documentation, unified bug, patches and sources under one roof, we need to modernise our website. We used GitLab's CI/CD integration to generate our website <a href="https://gitlab.freedesktop.org/libfprint/fprint.freedesktop.org/">from sources</a>, including creating API documentation and <a href="https://fprint.freedesktop.org/supported-devices.html">listing supported devices from git master</a>, to reduce the need to search the sources for that information.</div><br /><b>Step 5: simplify</b><div><br /></div><div>This process has started, but isn't finished yet. We're slowly splitting up the internal API between "internal internal" (what the library uses to work internally) and "internal for drivers" which we eventually hope to document to make writing drivers easier. This is partially done, but will need a lot more work in the coming months.</div><div><br /></div><div><b>TL;DR</b>: We migrated libfprint to meson, gtk-doc, GitLab, added a CI, and are writing docs for driver authors, everything's <a href="https://fprint.freedesktop.org/">on the website</a>!</div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-06-12T12:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer Conference 2018 Announced</span></a><div class="lastUpdated">2018年6月12日 20:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer project is happy to announce that this year's GStreamer
Conference will take place on Thursday-Friday 25-26 October 2018 in
Edinburgh, Scotland.
</p><p>
You can find more details about the conference on the
<a href="https://gstreamer.freedesktop.org/conference/2018/">GStreamer
Conference 2018 web site</a>.
</p><p>
A call for papers will be sent out shortly. Registration will open at a
later time. We will announce those and any further updates on the
<a href="https://lists.freedesktop.org/mailman/listinfo/gstreamer-announce">gstreamer-announce mailing list</a>,
the <a href="https://gstreamer.freedesktop.org/news/">website</a>,
and <a href="https://twitter.com/GStreamer">on Twitter</a>.
</p><p>
Talk slots will be available in varying durations from 20 minutes up to
45 minutes. Whatever you're doing or planning to do with GStreamer,
we'd like to hear from you!
</p><p>
We also plan to have sessions with short lightning talks / demos /
showcase talks for those who just want to show what they've been
working on or do a mini-talk instead of a full-length talk. Lightning
talk slots will be allocated on a first-come-first-serve basis, so make
sure to reserve your slot if you plan on giving a lightning talk.
</p><p>
There will also be a social event again on Thursday evening.
</p><p>
There are also plans to have a hackfest the weekend right after the conference.
</p><p>
We hope to see you in Edinburgh!
</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/06/07/3rd-party-software-in-fedora-workstation/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">3rd Party Software in Fedora Workstation</span></a><div class="lastUpdated">2018年6月7日 21:22</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>So you have probably noticed by now that we started offering some 3rd party software in the latest Fedora Workstation namely Google Chrome, Steam, NVidia driver and PyCharm. This has come about due to a long discussion in the Fedora community on how we position Fedora Workstation and how we can improve our user experience. The principles we base of this policy you can read up on <a href="https://fedoraproject.org/wiki/Workstation/Third_party_software_policies?rd=Workstation/Third_party_software_proposal">in this policy document</a>. To sum it up though the idea is that while the Fedora operating system you install will continue as it has been for the last decade to be based on only free software (with an exception for firmware) you will be able to more easily find and install the plethora of applications out there through our software store application, GNOME Software. We also expect that as the world of Linux software moves towards containers in general and Flatpaks specifically we will have an increasing number of these 3rd party applications available in Fedora.</p>
<p>So the question I know some of you will have is, what do one actually have to do in order to get a 3rd party application listed in Fedora Workstation? Well wonder no longer as we put up a few documents now outlining the steps you will need to take. Compared to traditional linux packaging the major difference in the requirements around improved metadata for your application, so we are covering that aspect in special detail. These documents cover both RPMS and Flatpaks. </p>
<p>First of all you can get a <a href="https://fedoraproject.org/wiki/Workstation/Third_party_software_Workstation_Guidelines">general overview from our 3rd Party guidelines document</a>. This document also explains how you submit a request to the Fedora Workstation Working group for your application to be added.</p>
<p>Then if you want to dig into the details of what metadata you need to create for your application <a href="https://fedoraproject.org/wiki/Workstation/softwareappdata">there is the in-depth metadata tutorial here</a> and finally once you are ready to set up your repository there is <a href="https://fedoraproject.org/wiki/Workstation/hostingmetadata">a tutorial explaining how you ensure your repository is able to provide the metadata you created above</a>.</p>
<p>We expect to add more and more applications to Fedora Workstation over time here, and I would especially recommend that you look into Flatpaking your 3rd party application as it will decouple your application from the host operating system and thus decrease the workload on you maintaining your application for use in Fedora Workstation (and elsewhere).</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/05/29/adding-support-for-the-dell-canvas-and-totem/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Adding support for the Dell Canvas and Totem</span></a><div class="lastUpdated">2018年5月29日 21:28</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>I am very happy to see that Benjamin Tissoires <a href="http://lkml.iu.edu/hypermail/linux/kernel/1805.3/05261.html">work to enable the Dell Canvas and Totem</a> has started to land in the upstream kernel. This work is the result of a collaboration between ourselves at Red Hat and Dell to bring this exciting device to Linux users.</p>
<div id="attachment_2850" class="wp-caption aligncenter"><a href="https://blogs.gnome.org/uraeus/files/2018/05/peripheral-canvas-mod-2.jpg"><img src="peripheral-canvas-mod-2.jpg" alt="Dell Canvas 27" class="size-full wp-image-2850" width="399" height="262" /></a><p class="wp-caption-text">Dell Canvas</p></div>
<p>The Dell Canvas and totem is essentially a graphics tablet with a stylus and also a turnable knob that can be placed onto the graphics tablet. Dell feature some videos on their site showcasing the Dell Canvas being used in ares such as drawing, video editing and CAD.</p>
<p>So for Linux applications supporting graphic drawing tablets already the canvas should work once this lands, but where we hope to see applications developers step up is adding support in their application for the totem. I have been pondering how we could help make that happen as we would be happy to donate a Dell Canvas to help kickstart application support, I am just unsure about the best way to go ahead.<br />
I was considering offering one as a prize for the first application to add support for the totem, but that seems to be a chicken and egg problem by definition. If anyone got any suggestions for how to get one of these into the hands of the developer most interested and able to take advantage of it?</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://wingolog.org/archives/2018/05/21/correct-or-inotify-pick-one"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">correct or inotify: pick one</span></a><div class="lastUpdated">2018年5月21日 22:29</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div><p>Let's say you decide that you'd like to see what some other processes on your system are doing to a subtree of the file system.  You don't want to have to change how those processes work -- you just want to see what files those processes create and delete.</p><p>One approach would be to just scan the file-system tree periodically, enumerating its contents.  But when the file system tree is large and the change rate is low, that's not an optimal thing to do.</p><p>Fortunately, Linux provides an API to allow a process to receive notifications on file-system change events, called <tt>inotify</tt>.  So you open up the <a href="http://man7.org/linux/man-pages/man7/inotify.7.html"><tt>inotify(7)</tt></a> manual page, and are greeted with this:</p><blockquote><p> With careful programming, an application can use inotify to efficiently monitor and cache the state of a set of filesystem objects.  However, robust applications should allow for the fact that bugs in the monitoring logic or races of the kind described below may leave the cache inconsistent with the filesystem state.  It is probably wise to do some consistency checking, and rebuild the cache when inconsistencies are detected.  </p></blockquote><p>It's not exactly reassuring is it?  I mean, "you had one job" and all.</p><p>Reading down a bit farther, I thought that with some "careful programming", I could get by.  After a day of trying, I am now certain that it is impossible to build a correct recursive directory monitor with <tt>inotify</tt>, and I am not even sure that "good enough" solutions exist.</p><p><b>pitfall the first: buffer overflow</b></p><p>Fundamentally, <tt>inotify</tt> races the monitoring process with all other processes on the system.  Events are delivered to the monitoring process via a fixed-size buffer that can overflow, and the monitoring process provides no back-pressure on the system's rate of filesystem modifications.  With <tt>inotify</tt>, you have to be ready to lose events.</p><p>This I think is probably the easiest limitation to work around.  The kernel can let you know when the buffer overflows, and you can tweak the buffer size.  Still, it's a first indication that perfect is not possible.</p><p><b>pitfall the second: now you see it, now you don't</b></p><p>This one is the real kicker.  Say you get an event that says that a file "frenemies.txt" has been created in the directory "/contacts/".  You go to open the file -- but is it still there?  By the time you get around to looking for it, it could have been deleted, or renamed, or maybe even created again or replaced!  This is a <a href="https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use">TOCTTOU</a> race, built-in to the <tt>inotify</tt> API.  It is literally impossible to use <tt>inotify</tt> without this class of error.</p><p>The canonical solution to this kind of issue in the kernel is to use file descriptors instead.  Instead of or possibly in addition to getting a name with the file change event, you get a descriptor to a (possibly-unlinked) open file, which you would then be responsible for closing.  But that's not what <tt>inotify</tt> does.  Oh well!</p><p><b>pitfall the third: race conditions between inotify instances</b></p><p>When you <tt>inotify</tt> a directory, you get change notifications for just that directory.  If you want to get change notifications for subdirectories, you need to open more <tt>inotify</tt> instances and <tt>poll</tt> on them all.  However now you have <i>N<sup>2</sup></i> problems: as <tt>poll</tt> and the like return an unordered set of readable file descriptors, each with their own ordering, you no longer have access to a linear order in which changes occurred.</p><p>It is <i>impossible</i> to build a recursive directory watcher that definitively says "ok, first <tt>/contacts/frenemies.txt</tt> was created, then <tt>/contacts</tt> was renamed to <tt>/peeps</tt>, ..."  because you have no ordering between the different watches.  You don't know that there was <i>ever</i> even a time that <tt>/contacts/frenemies.txt</tt> was an accessible file name; it could have been only ever openable as <tt>/peeps/frenemies.txt</tt>.</p><p>Of course, this is the most basic ordering problem.  If you are building a monitoring tool that actually wants to open files -- good luck bubster!  It literally cannot be correct.  (It might work well enough, of course.)</p><p><b>reflections</b></p><p>As far as I am aware, <tt>inotify</tt> came out to address the needs of desktop search tools like the belated <a href="https://en.wikipedia.org/wiki/Beagle_(software)">Beagle</a> (11/10 good pupper just trying to get his pup on).  Especially in the days of spinning metal, grovelling over the whole hard-drive was a real non-starter, especially if the search database should to be up-to-date.</p><p>But after looking into <tt>inotify</tt>, I start to see why someone at Google said that desktop search was in some ways harder than web search -- I mean we all struggle to find files on our own machines, even now, 15 years after the whole dnotify/inotify thing started.  Part of it is that the given the choice between supporting reliable, fool-proof file system indexes on the one hand, and overclocking the IOPS benchmarks on the other, the kernel gave us <tt>inotify</tt>.  I understand it, but <tt>inotify</tt> still sucks.</p><p>I dunno about you all but whenever I've had to document such an egregious uncorrectable failure mode as any of the ones in the <tt>inotify</tt> manual, I have rewritten the software instead.  In that spirit, I hope that some day we shall send <tt>inotify</tt> to the pet cemetery, to rest in peace beside Beagle.</p></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-05-17T17:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.14.1 stable bug fix release</span></a><div class="lastUpdated">2018年5月18日 1:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce the first bug fix
release in the stable 1.14 release series of your favourite cross-platform
multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from
1.14.x.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.14/#1.14.1">/releases/1.14/</a>
for the details.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Download tarballs directly here:
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.14.1.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.14.1.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.14.1.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.1.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.14.1.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.1.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.1.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.14.1.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.14.1.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.14.1.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.14.1.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.14.1.tar.xz">gst-omx</a>.
        </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://wingolog.org/archives/2018/05/16/lightweight-concurrency-in-lua"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">lightweight concurrency in lua</span></a><div class="lastUpdated">2018年5月16日 23:17</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div><p>Hello, all!  Today I'd like to share some work I have done recently as part of the <a href="https://github.com/snabbco/snabb">Snabb user-space networking toolkit</a>.  Snabb is mainly about high-performance packet processing, but it also needs to communicate with management-oriented parts of network infrastructure.  These communication needs are performed by a dedicated <a href="https://github.com/snabbco/snabb/blob/master/src/lib/ptree/README.md">manager process</a>, but that process has many things to do, and can't afford to make blocking operations.</p><p>Snabb is written in Lua, which doesn't have built-in facilities for concurrency.  What we'd like is to have <a href="https://wingolog.org/archives/2017/06/27/growing-fibers">fibers</a>.  Fortunately, Lua's coroutines are powerful enough to implement fibers.  Let's do that!</p><p><b>fibers in lua</b></p><p>First we need a scheduling facility.  Here's the smallest possible scheduler: simply a queue of tasks and a function to run those tasks.</p><pre>local task_queue = {}

function schedule_task(thunk)
   table.insert(task_queue, thunk)
end

function run_tasks()
   local queue = task_queue
   task_queue = {}
   for _,thunk in ipairs(queue) do thunk() end
end
</pre><p>For our purposes, a task is just a function that will be called with no arguments.</p><p>Now let's build fibers.  This is easier than you might think!</p><pre>local current_fiber = false

function spawn_fiber(fn)
   local fiber = coroutine.create(fn)
   schedule_task(function () resume_fiber(fiber) end)
end

function resume_fiber(fiber, ...)
   current_fiber = fiber
   local ok, err = coroutine.resume(fiber, ...)
   current_fiber = nil
   if not ok then
      print('Error while running fiber: '..tostring(err))
   end
end

function suspend_current_fiber(block, ...)
   -- The block function should arrange to reschedule
   -- the fiber when it becomes runnable.
   block(current_fiber, ...)
   return coroutine.yield()
end
</pre><p>Here, a fiber is simply a coroutine underneath.  Suspending a fiber suspends the coroutine.  Resuming a fiber runs the coroutine.  If you're unfamiliar with coroutines, or coroutines in Lua, maybe have a look at <a href="http://lua-users.org/wiki/CoroutinesTutorial">the lua-users wiki page on the topic</a>.</p><p>The difference between a fibers facility and just coroutines is that with fibers, you have a scheduler as well.  Very much like Scheme's <a href="https://www.gnu.org/software/guile/manual/html_node/Prompt-Primitives.html"><tt>call-with-prompt</tt></a>, coroutines are one of those powerful language building blocks that should rarely be used directly; concurrent programming needs more structure than what Lua offers.</p><p>If you're following along, it's probably worth it here to think how you would implement <tt>yield</tt> based on these functions.  A <tt>yield</tt> implementation should yield control to the scheduler, and resume the fiber on the next scheduler turn.  The answer is <a href="https://gitlab.com/snippets/1715966#L34">here</a>.</p><p><b>communication</b></p><p>Once you have fibers and a scheduler, you have concurrency, which means that if you're not careful, you have a mess.  Here I think the Go language got the essence of the idea exactly right: <a href="https://blog.golang.org/share-memory-by-communicating"> Do not communicate by sharing memory; instead, share memory by communicating.</a></p><p>Even though Lua doesn't support multiple machine threads running concurrently, concurrency between fibers can still be fraught with bugs.  Tony Hoare's <a href="http://usingcsp.com/">Communicating Sequential Processes</a> showed that we can avoid a class of these bugs by treating communication as a first-class concept.</p><p>Happily, the Concurrent ML project showed that it's possible to build these first-class communication facilities as a library, provided the language you are working in has threads of some kind, and fibers are enough.  Last year I built a <a href="https://wingolog.org/archives/2017/06/29/a-new-concurrent-ml">Concurrent ML library for Guile Scheme</a>, and when in <a href="https://github.com/snabbco/snabb">Snabb</a> we had a similar need, I ported that code over to Lua.  As it's a new take on the problem in a different language, I think I've been able to simplify things even more.</p><p>So let's take a crack at implementing Concurrent ML in Lua.  In CML, the fundamental primitive for communication is the <i>operation</i>.  An operation represents the potential for communication.  For example, if you have a channel, it would have methods to return "get operations" and "put operations" on that channel.  Actually receiving or sending a message on a channel occurs by <i>performing</i> those operations.  One operation can be performed many times, or not at all.</p><p>Compared to a system like Go, for example, there are two main advantages of CML.  The first is that CML allows non-deterministic choice between a number of potential operations in a generic way.  For example, you can construct a operation that, when performed, will either get on one channel or wait for a condition variable to be signalled, whichever comes first.  In Go, you can only <tt>select</tt> between operations on channels.</p><p>The other interesting part of CML is that operations are built from a uniform protocol, and so users can implement new kinds of operations.  Compare again to Go where all you have are channels, and nothing else.</p><p>The CML operation protocol consists three related functions:  <tt>try</tt> which attempts to directly complete an operation in a non-blocking way; <tt>block</tt>, which is called after a fiber has suspended, and which arranges to resume the fiber when the operation completes; and <tt>wrap</tt>, which is called on the result of a successfully performed operation.</p><p>In Lua, we can call this an <i>implementation</i> of an operation, and create it like this:</p><pre>function new_op_impl(try, block, wrap)
   return { try=try, block=block, wrap=wrap }
end
</pre><p>Now let's go ahead and write the guts of CML: the operation implementation.  We'll represent an operation as a Lua object with two methods.  The <tt>perform</tt> method will attempt to perform the operation, and return the resulting value.  If the operation can complete immediately, the call to <tt>perform</tt> will return directly.  Otherwise, <tt>perform</tt> will suspend the current fiber and arrange to continue only when the operation completes.</p><p>The <tt>wrap</tt> method "decorates" an operation, returning a new operation that, if and when it completes, will "wrap" the result of the completed operation with a function, by applying the function to the result.  It's useful to distinguish the sub-operations of a non-deterministic choice from each other.</p><p>Here our <tt>new_op</tt> function will take an array of operation implementations and return an operation that, when performed, will synchronize on the first available operation.  As you can see, it already has the equivalent of Go's <tt>select</tt> built in.</p><pre>function new_op(impls)
   local op = { impls=impls }
   
   function op.perform()
      for _,impl in ipairs(impls) do
         local success, val = impl.try()
         if success then return impl.wrap(val) end
      end
      local function block(fiber)
         local suspension = new_suspension(fiber)
         for _,impl in ipairs(impls) do
            impl.block(suspension, impl.wrap)
         end
      end
      local wrap, val = suspend_current_fiber(block)
      return wrap(val)
   end

   function op.wrap(f)
      local wrapped = {}
      for _, impl in ipairs(impls) do
         local function wrap(val)
            return f(impl.wrap(val))
         end
         local impl = new_op_impl(impl.try, impl.block, wrap)
         table.insert(wrapped, impl)
      end
      return new_op(wrapped)
   end

   return op
end
</pre><p>There's only one thing missing there, which is <tt>new_suspension</tt>.  When you go to suspend a fiber because none of the operations that it's trying to do can complete directly (i.e. all of the <tt>try</tt> functions of its impls returned false), at that point the corresponding <tt>block</tt> functions will publish the fact that the fiber is waiting.  However the fiber only waits until the first operation is ready; subsequent operations becoming ready should be ignored.  The suspension is the object that manages this state.</p><pre>function new_suspension(fiber)
   local waiting = true
   local suspension = {}
   function suspension.waiting() return waiting end
   function suspension.complete(wrap, val)
      assert(waiting)
      waiting = false
      local function resume()
         resume_fiber(fiber, wrap, val)
      end
      schedule_task(resume)
   end
   return suspension
end
</pre><p>As you can see, the suspension's <tt>complete</tt> method is also the bit that actually arranges to resume a suspended fiber.</p><p>Finally, just to round out the implementation, here's a function implementing non-deterministic choice from among a number of sub-operations:</p><pre>function choice(...)
   local impls = {}
   for _, op in ipairs({...}) do
      for _, impl in ipairs(op.impls) do
         table.insert(impls, impl)
      end
   end
   return new_op(impls)
end
</pre><p><b>on cml</b></p><p>OK, I'm sure this seems a bit abstract at this point.  Let's implement something concrete in terms of these primitives:  channels.</p><p>Channels expose two similar but different kinds of operations: put operations, which try to send a value, and get operations, which try to receive a value.  If there's a sender already waiting to send when we go to perform a <tt>get_op</tt>, the operation continues directly, and we resume the sender; otherwise the receiver publishes its suspension to a queue.  The <tt>put_op</tt> case is similar.</p><p>Finally we add some synchronous <tt>put</tt> and <tt>get</tt> convenience methods, in terms of their corresponding CML operations.</p><pre>function new_channel()
   local ch = {}
   -- Queues of suspended fibers waiting to get or put values
   -- via this channel.
   local getq, putq = {}, {}

   local function default_wrap(val) return val end
   local function is_empty(q) return #q == 0 end
   local function peek_front(q) return q[1] end
   local function pop_front(q) return table.remove(q, 1) end
   local function push_back(q, x) q[#q+1] = x end

   -- Since a suspension could complete in multiple ways
   -- because of non-deterministic choice, it could be that
   -- suspensions on a channel's putq or getq are already
   -- completed.  This helper removes already-completed
   -- suspensions.
   local function remove_stale_entries(q)
      local i = 1
      while i &lt;= #q do
         if q[i].suspension.waiting() then
            i = i + 1
         else
            table.remove(q, i)
         end
      end
   end

   -- Make an operation that if and when it completes will
   -- rendezvous with a receiver fiber to send VAL over the
   -- channel.  Result of performing operation is nil.
   function ch.put_op(val)
      local function try()
         remove_stale_entries(getq)
         if is_empty(getq) then
            return false, nil
         else
            local remote = pop_front(getq)
            remote.suspension.complete(remote.wrap, val)
            return true, nil
         end
      end
      local function block(suspension, wrap)
         remove_stale_entries(putq)
         push_back(putq, {suspension=suspension, wrap=wrap, val=val})
      end
      return new_op({new_op_impl(try, block, default_wrap)})
   end

   -- Make an operation that if and when it completes will
   -- rendezvous with a sender fiber to receive one value from
   -- the channel.  Result is the value received.
   function ch.get_op()
      local function try()
         remove_stale_entries(putq)
         if is_empty(putq) then
            return false, nil
         else
            local remote = pop_front(putq)
            remote.suspension.complete(remote.wrap)
            return true, remote.val
         end
      end
      local function block(suspension, wrap)
         remove_stale_entries(getq)
         push_back(getq, {suspension=suspension, wrap=wrap})
      end
      return new_op({new_op_impl(try, block, default_wrap)})
   end

   function ch.put(val) return ch.put_op(val).perform() end
   function ch.get()    return ch.get_op().perform()    end

   return ch
end
</pre><p><b>a wee example</b></p><p>You might be wondering what it's like to program with channels in Lua, so here's a little example that shows a prime sieve based on channels.  It's not a great example of concurrency in that it's not an inherently concurrent problem, but it's cute to show computations in terms of infinite streams.</p><pre>function prime_sieve(count)
   local function sieve(p, rx)
      local tx = new_channel()
      spawn_fiber(function ()
         while true do
            local n = rx.get()
            if n % p ~= 0 then tx.put(n) end
         end
      end)
      return tx
   end

   local function integers_from(n)
      local tx = new_channel()
      spawn_fiber(function ()
         while true do
            tx.put(n)
            n = n + 1
         end
      end)
      return tx
   end

   local function primes()
      local tx = new_channel()
      spawn_fiber(function ()
         local rx = integers_from(2)
         while true do
            local p = rx.get()
            tx.put(p)
            rx = sieve(p, rx)
         end
      end)
      return tx
   end

   local done = false
   spawn_fiber(function()
      local rx = primes()
      for i=1,count do print(rx.get()) end
      done = true
   end)

   while not done do run_tasks() end
end
</pre><p>Here you also see an example of running the scheduler in the last line.</p><p><b>where next?</b></p><p>Let's put this into perspective: in a couple hundred lines of code, we've gone from minimal Lua to a language with lightweight multitasking, extensible CML-based operations, and CSP-style channels; truly a delight.</p><p>There are a number of possible ways to extend this code.  One of them is to implement true multithreading, if the language you are working in supports that.  In that case there are some small protocol modifications to take into account; see <a href="https://wingolog.org/archives/2017/06/29/a-new-concurrent-ml">the notes on the Guile CML implementation</a> and especially the <a href="http://manticore.cs.uchicago.edu/">Manticore Parallel CML</a> project.</p><p>The implementation above is pleasantly small, but it could be faster with the choice of more specialized data structures.  I think interested readers probably see a number of opportunities there.</p><p>In a library, you might want to avoid the global <tt>task_queue</tt> and implement nested or multiple independent schedulers, and of course in a parallel situation you'll want core-local schedulers as well.</p><p>The implementation above has no notion of time.  What we did in the <a href="https://github.com/Igalia/snabb/tree/lwaftr/src/lib/fibers">Snabb implementation of fibers</a> was to implement a <a href="https://github.com/Igalia/snabb/blob/lwaftr/src/lib/fibers/timer.lua">timer wheel</a>, inspired by <a href="https://www.snellman.net/blog/archive/2016-07-27-ratas-hierarchical-timer-wheel/">Juho Snellman's Ratas</a>, and then add that timer wheel as a task source to Snabb's scheduler.  In Snabb, every time the equivalent of <tt>run_tasks()</tt> is called, a scheduler asks its sources to schedule additional tasks.  The timer wheel implementation schedules expired timers.  It's straightforward to build <a href="https://github.com/Igalia/snabb/blob/lwaftr/src/lib/fibers/sleep.lua">CML timeout operations</a> in terms of timers.</p><p>Additionally, your system probably has other external sources of communication, such as sockets.  The trick to integrating sockets into fibers is to suspend the current fiber whenever an operation on a file descriptor would block, and arrange to resume it when the operation can proceed.  <a href="https://github.com/Igalia/snabb/blob/lwaftr/src/lib/fibers/file.lua">Here's the implementation in Snabb</a>.</p><p>The only difficult bit with getting nice nonblocking socket support is that you need to be able to suspend the calling thread when you see the <tt>EWOULDBLOCK</tt> condition, and for coroutines that is often only possible if you implemented the buffered I/O yourself.  In Snabb that's what we did: we implemented <a href="https://github.com/Igalia/snabb/blob/lwaftr/src/lib/fibers/file.lua">a compatible replacement for Lua's built-in streams, in Lua</a>.  That lets us handle <tt>EWOULDBLOCK</tt> conditions in a flexible manner.  Integrating <tt>epoll</tt> as a task source also lets us sleep when there are no runnable tasks.</p><p>Likewise in the Snabb context, we are also working on a TCP implementation.  In that case you want to structure TCP endpoints as fibers, and arrange to suspend and resume them as appropriate, while also allowing timeouts.  I think the scheduler and CML patterns are going to allow us to do that without much trouble.  (Of course, the TCP implementation will give us lots of trouble!)</p><p>Additionally your system might want to communicate with fibers from other threads.  It's entirely possible to implement CML on top of pthreads, and it's entirely possible as well to support communication between pthreads and fibers.  If this is interesting to you, see Guile's implementation.</p><p>When I talked about fibers in <a href="https://wingolog.org/archives/2017/06/27/growing-fibers">an earlier article</a>, I built them in terms of delimited continuations.  Delimited continuations are fun and more expressive than coroutines, but it turns out that for fibers, all you need is the expressive power of coroutines -- multi-shot continuations aren't useful.  Also I think the presentation might be more straightforward.  So if all your language has is coroutines, that's still good enough.</p><p>There are many more kinds of standard CML operations; implementing those is also another next step.  In particular, I have found semaphores and condition variables to be quite useful.  Also, standard CML supports "guards", invoked when an operation is performed, and "nacks", invoked when an operation is definitively <i>not</i> performed because a <tt>choice</tt> selected some other operation.  These can be layered on top; see the Parallel CML paper for notes on "primitive CML".</p><p>Also, the <tt>choice</tt> operator above is left-biased: it will prefer earlier impls over later ones.  You might want to not always start with the first impl in the list.</p><p>The scheduler shown above is the simplest thing I could come up with.  You may want to experiment with other scheduling algorithms, e.g. <a href="https://dl.acm.org/citation.cfm?doid=3190508.3190539">capability-based scheduling</a>, or <a href="https://www.cs.utah.edu/plt/publications/pldi04-ff.pdf">kill-safe abstractions</a>.  Do it!</p><p>Or, it could be you already have a scheduler, like some kind of main loop that's already there.  Cool, you can use it directly -- all that fibers needs is some way to schedule functions to run.</p><p><b>godspeed</b></p><p>In summary, I think Concurrent ML should be better-known.  Its simplicity and expressivity make it a valuable part of any concurrent system.  Already in Snabb it helped us solve some <a href="https://github.com/Igalia/snabb/issues/668">longstanding gnarly issues</a> by making the right solutions expressible.</p><p>As Adam Solove says, <a href="https://medium.com/@asolove/concurrent-ml-has-a-branding-problem-ce0286eab598">Concurrent ML is great, but it has a branding problem</a>.  Its ideas haven't penetrated the industrial concurrent programming world to the extent that they should.  This article is another attempt to try to get the word out.  Thanks to Adam for the observation that CML is really a protocol; I'm sure the concepts could be made even more clear, but at least this is a step forward.</p><p>All the code in this article is up on a <a href="https://gitlab.com/snippets/1715966">gitlab snippet</a> along with instructions for running the example program from the command line.  Give it a go, and happy hacking with CML!</p></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://andrescolubri.net/blog/2018/05/07/lassa_fever_in_nigeria_lessons_learnt.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Lassa fever in Nigeria: lessons learnt</span></a><div class="lastUpdated">2018年5月8日 1:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Back in March of this year we reached an important milestone in the collaboration between the Sabeti Lab and the Irrua Specialist Teaching Hospital (ISTH) in <a href="https://goo.gl/maps/n5D8dM2tCZ62" target="_top">Edo State, Nigeria</a>: we published <a href="https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(18)30121-X/fulltext" target="_top">a joint paper</a> on the journal The Lancet Infectious Diseases describing the largest and most detailed retrospective cohort study of Lassa fever patients in Nigeria, and identifying the clinical and laboratory predictors of outcome observed at ISTH for this deadly disease. This is a culmination of several years of work, starting in 2013.</p>

<p><img src="lassa_ward.jpg" alt="Lassa virus" />
<em>At the entrance of the Lassa ward at ISTH with Christopher Iruolagbe, one of the clinicians.</em></p>

<p>First of all, I will give a brief introduction to <a href="https://www.cdc.gov/vhf/lassa/index.html" target="_top">Lassa fever</a>. This is a member of a family of diseases called Viral Hemorrhagic Fevers (VHFs), which includes Ebola, dengue, and yellow fever among many others. These illnesses are caused by different viruses, but all have fever and hemorrhage as clinical manifestations. The <a href="https://www.cdc.gov/vhf/ebola/outbreaks/2014-west-africa/index.html" target="_top">2014-2016 Ebola outbreak</a> caused widespread concern due to its high mortality and fear for a worldwide pandemic, although its spread was largely contained to the African nations of Liberia, Sierra Leone, and Guinea. The outbreak left a large impact in the region, with almost 30,000 total cases and over 15,000 deaths. In contrast, Lassa fever is an endemic disease, with cases occurring throughout the year in West Africa, and presenting a wide range of clinical severity: most people don’t get sick enough to go to the hospital, but a small percentage become acutely ill and need medical attention. It is estimated that 300,000 people get infected with the Lassa virus every year, but less than 5% of those end up going to the hospital. The overall mortality among hospitalized cases is around 20%, lower than Ebola, but it can be much higher than that for elderly patients and pregnant women. Another difference with Ebola is the natural host of the virus: in the case of Lassa fever, it is the <a href="https://www.inaturalist.org/taxa/45326-Mastomys-natalensis" target="_top"><em>mastomys</em> rat</a>, which enters into people’s homes looking for food and spreads the virus through feces and urine, while for Ebola it is likely to be <a href="https://news.nationalgeographic.com/news/2005/11/1130_051130_ebolabathost.html" target="_top">fruit bats</a> instead. Even though the pathogens causing these diseases were discovered only in the last 50 years (Lassa fever in 1969, Ebola in 1976), <a href="https://linkinghub.elsevier.com/retrieve/pii/S0092-8674(15)00897-1" target="_top">genetic studies</a> from our lab indicate that Lassa fever is an old disease, with the virus spreading out of Nigeria at least 400 years ago. A similar picture appears for Ebola, leading to the question of whether we are in the presence of <a href="http://science.sciencemag.org/content/338/6108/750.long" target="_top">emerging diseases or diagnoses</a>.</p>

<p><img src="lasv.jpg" alt="Lassa virus" />
<em>TEM micrograph of Lassa virus virions.</em></p>

<p>Back in 2013, I just began developing the visualization tool <a href="https://fathom.info/mirador/" target="_top">Mirador</a> at <a href="https://fathom.info/" target="_top">Fathom Information Design</a>, and was looking for “real world” datasets to apply Mirador to. Around that time, Dr Peter Okokhere, the head of the Lassa fever ward at ISTH, was visiting the Sabeti Lab, and had compiled the records of all the patients treated in the ward since 2011 into an anonymized Excel spreadsheet. Mirador was designed precisely to handle that kind of tabular data, so it seemed to us that it should be straightforward to use Mirador to carry out exploratory analysis of Dr Okokhere’s dataset. In particular, we were interested in finding correlations between the different demographic, clinical, and laboratory variables collected for all patients and their outcome (death or survival). A subsequent step was to apply Machine Learning in order to train <a href="https://www.bmj.com/content/338/bmj.b375" target="_top">prognostic models</a> that could eventually be helpful for clinicians, for example to triage patients upon admission to the ward depending on their death risk (so that time and material resources would be prioritized for high-risk patients). The models’ predictions could also identify the clinical features most strongly the risk, and hence inform medical judgment.</p>

<p><img src="mirador-isth.png" alt="Lassa virus" />
<em>Mirador displaying ISTH data.</em></p>

<p>However, the work on the ISTH clinical data had to be put on hold as the lab shifted its focus and resources to the Ebola outbreak during the next two years. Some of the initial modeling approaches we were developing for the ISTH dataset were applied to similar datasets from Ebola patients, and this work led to some publications (<a href="http://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0004549" target="_top">here</a> and <a href="http://biorxiv.org/cgi/content/short/294587v3" target="_top">here</a>), where we investigated the possibility of deploying this prognostic models to the clinic in the form of medical apps for patient triage, care, and management. By late 2015, the Ebola outbreak was beginning to subside, and we were able to come back to our research on Lassa. In the meantime, Dr Okokhere had incorporated patients treated in 2014 and 2015, which enlarged the dataset to nearly 300 patients. At that time, I started to realize that I was originally too enthusiastic about the <a href="https://www.bmj.com/content/338/bmj.b606" target="_top">applicability of prognostic models to inform clinical decisions</a>. As I learned while after delving deeper into the topic, such models need to be trained on much larger cohorts and then validated on independently-obtained datasets, so that they can be generalized to new patients. Widely used prognostic scores such as <a href="https://www.mdcalc.com/apache-ii-score" target="_top">APACHE II</a> or the <a href="https://www.mdcalc.com/framingham-coronary-heart-disease-risk-score" target="_top">Framingham Risk Score</a> took thousands of patients and many years to be developed. As unique and detailed as the ISTH dataset is, it cannot support the creation of a Lassa fever prognostic score yet. Thanks to Dr Okokhere’s efforts to digitalize the paper medical records of the patients treated in the ward, we were able to have some data, but unfortunately there are no similar datasets available for analysis and validation. <a href="https://academic.oup.com/jid/article-abstract/155/3/445/851520" target="_top">Only one other study</a>, published in 1987, included more than 300 confirmed cases of Lassa fever and detailed demographic, clinical, and laboratory data from patients in Sierra Leone between 1977 and 1979.</p>

<p>Regardless of the current limitations, the ISTH dataset allowed us to provide a significant update on the clinical knowledge of Lassa fever in Nigeria, and to find the most important manifestations of the disease among the patients treated at ISTH.</p>

<p><img src="isth_cfr_map.jpg" alt="CFR map" />
<em>Map of cases treated at ISTH between 2011 and 2015, clustered by geographical location and shaded by case fatality rate.</em></p>

<p>The predictive models also proved to be very useful, not as prognostic scores, but to test the independence between various biomarkers that characterize the pathophysiology of the disease. This, in addition other complementary results and previous experience from the clinicians at ISTH, led us to make hypothesis of medical relevance (e.g.: Lassa virus may damage the kidney cells in some of the patients), which we need to explore further.</p>

<p><img src="isth_pred_model.jpg" alt="Model performance" />
<em>Predictive performance of the logistic regression model trained on the ISTH data. The horizontal axis shows the mortality risk threshold used to predict death vs survival, and the right vertical axis the corresponding sensitivity and specificity of the model. The bars indicate the actual number of patients and mortality within each risk bin.</em></p>

<p>However, one of the requirements to move forward with this research and also improve patient care is to build better on-site capacity for data collection and management. This not only impacts the handling of the patients’ medical records, but also the laboratory samples that are used for diagnosis, clinical decision, and research. For this reason, we have been designing mobile apps for collecting clinical and laboratory data, using <a href="https://www.dimagi.com/commcare/" target="_top">Dimagi’s CommCare platform</a>. This platform has enabled us to translate paper-based protocols for data collection into digital forms that can work with limited internet connectivity, store the records in a HIPPA-compliant database, and generate real-time reports. These apps are currently undergoing the final stages of debug and testing, and will soon be deployed at ISTH’s Lassa ward and research lab for daily use.</p>

<p><img src="clinical_app.jpg" alt="CommCare clinical app" />
<em>CommCare clinical app for patient management.</em></p>

<p>As part of the testing process, I had the chance to visit ISTH recently, together with my lab colleague, Dr Kayla Barnes. We worked with the clinicians and lab staff, going through all the data entry modules in the apps and making sure that they function as expected and fit the workflow patient and sample management protocols. Being my first time in Nigeria and in Africa in general, this was an exceptional experience for me, and felt incredibly welcomed by all people we met during the visit. We even got tailor-made traditional Nigerian dresses we wore the last day of our stay at ISTH, as you can see in some of the pictures below!</p>




<div id="lassa_fever_lessons">
  <img src="1_isth_entrance.jpg" title="" alt="ISTH's main entrance" />
  <img src="2_lassa_ward.jpg" title="" alt="Lassa ward building" />
  <img src="3_ward_staff.jpg" title="" alt="Staff preparing a Ribavirin shot for a patient" />
  <img src="4_working.jpg" title="" alt="Working at the Lassa research lab" />
  <img src="5_lab_app.jpg" title="" alt="Discussing the data collection protocols" />
  <img src="6_lab_app.jpg" title="" alt="Testing the CommCare lab app" />
  <img src="7_group.jpg" title="" alt="Wearing traditional Nigeran dresses. Right to left: Eghosa Uyige, Kayla Barnes, Ikponmwosa Odia, John Aiyepada, and myself" />
  </div>
<p><em>Some pictures from our trip to ISTH in March 2018.</em></p>

<p>As much as we enjoyed a very successful visit and the hospitality of our Nigerian hosts, Nigeria and ISTH in particular were still recovering of the <a href="https://www.nature.com/articles/d41586-018-03171-y" target="_top">largest recorded Lassa fever outbreak in history</a>: more than 200 confirmed cases were received and tested at ISTH between January and February of this year, which is more than all the cases from 2016 and 2017 combined. This outbreak was covered by several <a href="https://www.bbc.com/news/world-africa-43211086" target="_top">news</a> <a href="https://www.npr.org/sections/goatsandsoda/2018/03/19/587603462/nigeria-faces-mystiifying-spike-in-deadly-lassa-fever" target="_top">sources</a>, and it put a lot of strain on the Lassa ward personnel as they run out of beds and had to resort to temporary spaces to accommodate all patients. Fortunately, by the time we arrived the situation was gradually coming back to normal, as <a href="http://www.ncdc.gov.ng/diseases/sitreps/?cat=5&amp;name=An%20update%20of%20Lassa%20fever%20outbreak%20in%20Nigeria" target="_top">reported by the Nigerian Center for Disease Control</a>. However, this recent event is a reminder of the threat posed by Lassa fever and other emerging infectious diseases, and how we need efficient detection and containment mechanisms to stop outbreaks and avoid loss of human life.</p>

<p>Research continues at ISTH and we hope to find insights about the recent outbreak from the <a href="http://virological.org/t/new-lassa-virus-genomes-from-nigeria-2015-2016/191" target="_top">viral sequences</a> generated in Nigeria, thanks to the capacity building efforts from the <a href="http://acegid.org/" target="_top">African Center of Excellence for Genomics of Infectious Diseases</a>, which our lab is part of together with several African and international partners. Among further next steps, we plan to carry out analyses of the combined the genomic and clinical data that could shed light on the genetic origin for the variability in the clinical manifestation of Lassa fever, and to develop new privacy-preserving Machine Learning algorithms that would allow us to respond to an outbreak faster by sharing data and models in real-time while protecting patient’s privacy.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/04/24/warming-up-for-fedora-workstation-28/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Warming up for Fedora Workstation 28</span></a><div class="lastUpdated">2018年4月25日 1:15</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Been some time now since my last update on what is happening in Fedora Workstation and with current plans to release Fedora Workstation 28 in early May I thought this could be a good time to write something. As usual this is just a small subset of what the team has been doing and I always end up feeling a bit bad for not talking about the avalanche of general fixes and improvements the team adds to each release.</p>
<p><strong>Thunderbolt</strong><br />
Christian Kellner has done a tremendous job keeping everyone informed of his work making sure we have proper Thunderbolt support in Fedora Workstation 28. One important aspect for us of this improved Thunderbolt support is that a lot of docking stations coming out will be requiring it and thus without this work being done you would not be able to use a wide range of docking stations. For a lot of screenshots and more details about how the thunderbolt support is done I recommend reading this article in <a href="https://christian.kellner.me/2018/04/23/the-state-of-thunderbolt-3-in-fedora-28/">Christians Blog</a>.</p>
<p><strong>3rd party applications</strong><br />
It has taken us quite some time to get there as getting this feature right both included a lot of internal discussion about policies around it and implementation detail. But starting from Fedora Workstation 28 you will be able to find more 3rd party software listed in GNOME Software if you enable it. The way it will work is that you as part of the initial setup will be asked if you want to have 3rd party software show up in GNOME Software. If you are upgrading you will be asked inside GNOME Software if you want to enable 3rd party software. You can also disable 3rd party software after enabling it from the GNOME Software settings as seen below:</p>
<p></p><div id="attachment_2840" class="wp-caption aligncenter"><a href="https://blogs.gnome.org/uraeus/files/2018/04/gnome-software-3rd-party.png"><img src="gnome-software-3rd-party-278x300.png" alt="GNOME Software settings" class="size-medium wp-image-2840" width="278" height="300" /></a><p class="wp-caption-text">GNOME Software settings</p></div> In Fedora Workstation 27 we did have PyCharm available, but we have now added the NVidia driver and Steam to the list for Fedora Workstation 28.<p></p>
<p>We have also been working with Google to try to get Chrome included here and we are almost there as they merged for instance the needed Appstream metadata some time ago, but the last steps requires some tweaking of how Google generates their package repository (basically adding the appstream metadata to their yum repository) and we don’t have a clear timeline for when that will happen, but as soon as it does the Chrome will also appear in GNOME Software if you have 3rd party software enabled.</p>
<p>As we speak all 3rd party packages are RPMs, but we expect that going forward we will be adding applications packaged as Flatpaks too.</p>
<p>Finally if you want to propose 3rd party applications for inclusion you can find some <a href="https://fedoraproject.org/wiki/Workstation/Third_party_software_Workstation_Guidelines">instructions for how to do it here</a>.</p>
<p><strong>Virtualbox guest</strong><br />
Another major feature that got some attention that we worked on for this release was Hans de Goedes work to ensure Fedora Workstation could run as a virtualbox guest out of the box. We know there are many people who have their first experience with linux running it under Virtualbox on Windows or MacOSX and we wanted to make their first experience as good as possible. Hans worked with the virtualbox team to clean up their kernel drivers and agree on a stable ABI so that they could be merged into the kernel and maintained there from now on. </p>
<p><strong>Firmware updates</strong><br />
The Spectre/Meltdown situation did hammer home to a lot of people the need to have firmware updates easily available and easy to update. We created the Linux Vendor Firmware service for Fedora Workstation users with that in mind and it was great to see the service paying off for many Linux users, not only on Fedora, but also on other distributions who started using the service we provided. I would like to call out to Dell who was a critical partner for the Linux Vendor Firmware effort from day 1 and thus their users got the most benefit from it when Spectre and Meltdown hit. Spectre and Meltdown also helped get a lot of other vendors off the fence or to accelerate their efforts to support LVFS and Richard Hughes and Peter Jones have been working closely with a lot of new vendors during this cycle to get support for their hardware and devices into LVFS. In fact Peter even flew down to the offices one of the biggest laptop vendors recently to help them resolve the last issues before their hardware will start showing up in the firmware service. Thanks to the work of Richard Hughes and Peter Jones you will both see a wider range of brands supported in the Linux Vendor Firmware Service in Fedora Workstation 28, but also a wider range of device classes.</p>
<p><strong>Server side GL Vendor Neutral Dispatch</strong><br />
This is a bit of a technical detail, but Adam Jackson and Lyude Paul has been working hard this cycle on getting what we call Server side GLVND ready for Fedora Workstation 28. Currently we are looking at enabling it either as a zero-day update or short afterwards. so what is Server Side GLVND you say? Well it is basically the last missing piece we need to enable the use of the NVidia binary driver through XWayland. Currently the NVidia driver works with Wayland native OpenGL applications, but if you are trying to run an OpenGL application requiring X we need this to support it. And to be clear once we ship this in Fedora Workstation 28 it will also require a driver update from NVidia to use it, so us shipping it is just step 1 here. We do also expect there to be some need for further tuning once we got all the pieces released to get top notch performance. Of course over time we hope and expect all applications to become Wayland native, but this is a crucial transition technology for many of our users. Of course if you are using Intel or AMD graphics with the Mesa drivers things already work great and this change will not affect you in any way.</p>
<p><strong>Flatpak</strong><br />
<a href="http://www.flatpak.org/">Flatpak</a>s basically already work, but we have kept focus this time around on to fleshing out the story in terms of the so called Portals. Portals are essentially how applications are meant to be able to interact with things outside of the container on your desktop. Jan Grulich has put in a lot of great effort making sure we get portal support for Qt and KDE applications, most recently by adding support for the screen capture portal on top of Pipewire. You can ready more about that <a href="http://www.jgrulich.cz/2018/03/06/screen-sharing-in-plasma-wayland-session/">on Jan Grulichs blog</a>. He is now focusing on getting the printing portal working with Qt.</p>
<p>Wim Taymans has also kept going full steam ahead of <a href="http://www.pipewire.org/">PipeWire</a>, which is critical for us to enable applications dealing with cameras and similar on your system to be containerized. More details on that in my <a href="https://blogs.gnome.org/uraeus/2018/01/26/an-update-on-pipewire-the-multimedia-revolution-an-update/">previous blog entry talking specifically about Pipewire</a>.</p>
<p>It is also worth noting that we are working with Canonical engineers to ensure Portals also works with Snappy as we want to ensure that developers have a single set of APIs to target in order to allow their applications to be sandboxed on Linux. Alexander Larsson has already reviewed quite a bit of code from the Snappy developers to that effect.</p>
<p><strong>Performance work</strong><br />
Our engineers have spent significant time looking at various performance and memory improvements since the last release. The main credit for the recently talked about ‘memory leak’ goes to Georges Basile Stavracas Neto from Endless, but many from our engineering team helped with diagnosing that and also fixed many other smaller issues along the way. More details about the ‘memory leak’ fix in <a href="https://feaneron.com/2018/04/20/the-infamous-gnome-shell-memory-leak/">Georges blog</a>.</p>
<p>We are not done here though and Alberto Ruiz is organizing a big performance focused hackfest in Cambridge, England in May. We hope to bring together many of our core engineers to work with other members of the community to look at possible improvements. The Raspberry Pi will be the main target, but of course most improvements we do to make GNOME Shell run better on a Raspberry Pi also means improvements for normal x86 systems too.</p>
<p><strong>Laptop Battery life</strong><br />
In our efforts to make Linux even better on laptops Hans De Goede spent a lot of time figuring out things we could do to make Fedora Workstation 28 have better battery life. How valuable these changes are will of course depend on your exact hardware, but I expect more or less everyone to have a bit better battery life on Fedora Workstation 28 and for some it could be a lot better battery life. You can read a bit more about these changes in <a href="https://hansdegoede.livejournal.com/18653.html">Hans de Goedes blog</a>.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://coaxion.net/blog/2018/04/glib-gio-async-operations-and-rust-futures-async-await/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GLib/GIO async operations and Rust futures + async/await</span></a><div class="lastUpdated">2018年4月23日 16:46</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Unfortunately I was not able to attend the <a href="https://wiki.gnome.org/Hackfests/Rust2018" rel="noopener" target="_top">Rust+GNOME hackfest</a> in Madrid last week, but I could at least spend some of my work time at <a href="https://centricular.com/" rel="noopener" target="_top">Centricular</a> on implementing one of the things I wanted to work on during the hackfest. The other one, more closely related to the <a href="https://blog.guillaume-gomez.fr/articles/2018-04-21+Rust%2BGNOME+Hackfest+in+Madrid" rel="noopener" target="_top">gnome-class work</a>, will be the topic of a future blog post once I actually have something to show.</p>
<p>So back to the topic. With the latest GIT version of the <a href="https://www.rust-lang.org/en-US/" rel="noopener" target="_top">Rust</a> bindings for <a href="http://gtk-rs.org/" rel="noopener" target="_top">GLib, GTK, etc</a> it is now possible to make use of the Rust futures infrastructure for GIO async operations and various other functions. This should make writing of <a href="https://www.gnome.org/" rel="noopener" target="_top">GNOME</a>, and in general GLib-using, applications in Rust quite a bit more convenient.</p>
<p>For the impatient, the summary is that you can use Rust futures with GLib and GIO now, that it works both on the stable and nightly version of the compiler, and with the nightly version of the compiler it is also possible to use async/await. An example with the latter can be found <a href="https://github.com/gtk-rs/examples/blob/a684d90e4233d6eeab8aff5e65013cd9208b409f/src/bin/gio_futures_await.rs" rel="noopener" target="_top">here</a>, and an example just using futures without async/await <a href="https://github.com/gtk-rs/examples/blob/a684d90e4233d6eeab8aff5e65013cd9208b409f/src/bin/gio_futures.rs" rel="noopener" target="_top">here</a>.</p>
<h3 id="toc">Table of Contents</h3>
<ol>
<li><a href="https://coaxion.net/blog/feed/#futures">Futures</a>
<ol>
<li><a href="https://coaxion.net/blog/feed/#futures-rust">Futures in Rust</a></li>
<li><a href="https://coaxion.net/blog/feed/#futures-async-await">Async/Await</a></li>
<li><a href="https://coaxion.net/blog/feed/#futures-tokio">Tokio</a></li>
</ol>
</li>
<li><a href="https://coaxion.net/blog/feed/#futures-glib">Futures &amp; GLib/GIO</a>
<ol>
<li><a href="https://coaxion.net/blog/feed/#glib-callbacks">Callbacks</a></li>
<li><a href="https://coaxion.net/blog/feed/#glib-futures">GLib Futures</a></li>
<li><a href="https://coaxion.net/blog/feed/#glib-gio">GIO Asynchronous Operations</a></li>
<li><a href="https://coaxion.net/blog/feed/#glib-async-await">Async/Await</a></li>
</ol>
</li>
<li><a href="https://coaxion.net/blog/feed/#future">The Future</a></li>
</ol>
<h3 id="futures">Futures</h3>
<p>First of all, what are futures and how do they work in Rust. In a few words, a <a href="https://en.wikipedia.org/wiki/Futures_and_promises" rel="noopener" target="_top">future</a> (also called promise elsewhere) is a value that represents the result of an asynchronous operation, e.g. establishing a TCP connection. The operation itself (usually) runs in the background, and only once the operation is finished (or fails), the future resolves to the result of that operation. There are all kinds of ways to combine futures, e.g. to execute some other (potentially async) code with the result once the first operation has finished.</p>
<p>It’s a concept that is also widely used in various other programming languages (e.g. C#, JavaScript, Python, …) for asynchronous programming and can probably be considered a proven concept at this point.</p>
<h4 id="futures-rust">Futures in Rust</h4>
<p>In Rust, a <a href="https://github.com/rust-lang-nursery/futures-rs" rel="noopener" target="_top">future</a> is basically an implementation of relatively simple trait called <a href="https://github.com/rust-lang-nursery/futures-rs/blob/f484ffffaa39c6a8b5953825d45c9956048bfa23/futures-core/src/future/mod.rs#L47-L123" rel="noopener" target="_top">Future</a>. The following is the definition as of now, but there are <a href="https://github.com/rust-lang/rfcs/pull/2395" rel="noopener" target="_top">discussions</a> to change/simplify/generalize it currently and to also move it to the Rust standard library:</p>
<p></p><pre class="crayon-plain-tag">pub trait Future {
    type Item;
    type Error;

    fn poll(&amp;mut self, cx: &amp;mut task::Context) -&gt; Poll&lt;Self::Item, Self::Error&gt;;
}</pre><p> </p>
<p>Anything that implements this trait can be considered an asynchronous operation that resolves to either an <i>Item</i> or an <i>Error</i>. Consumers of the future would call the <i>poll</i> method to check if the future has resolved already (to a result or error), or if the future is not ready yet. In case of the latter, the future itself would at a later point, once it is ready to proceed, notify the consumer about that. It would get a way for notifications from the <i>Context</i> that is passed, and proceeding does not necessarily mean that the future will resolve after this but it could just advance its internal state closer to the final resolution.</p>
<p>Calling <i>poll</i> manually is kind of inconvenient, so generally this is handled by an <i>Executor</i> on which the futures are scheduled and which is running them until their resolution. Equally, it’s inconvenient to have to implement that trait directly so for most common operations there are combinators that can be used on futures to build new futures, usually via closures in one way or another. For example the following would run the passed closure with the successful result of the future, and then have it return another future (<i>Ok(())</i> is converted via <i>IntoFuture</i> to the future that always resolves successfully with <i>()</i>), and also maps any errors to <i>()</i></p>
<p></p><pre class="crayon-plain-tag">fn our_future() -&gt; impl Future&lt;Item = (), Err = ()&gt; {
    some_future
        .and_then(|res| {
            do_something(res);
            Ok(())
        })
        .map_err(|_| ())
}</pre><p> </p>
<p>A future represents only a single value, but there is also a trait for something producing multiple values: a <i>Stream</i>. For more details, best to check the <a href="https://docs.rs/futures/0.2.1/futures/" rel="noopener" target="_top">documentation</a>.</p>
<h4 id="futures-async-await">Async/Await</h4>
<p>The above way of combining futures via combinators and closures is still not too great, and is still close to callback hell. In other languages (e.g. C#, <a href="https://javascript.info/async-await" rel="noopener" target="_top">JavaScript</a>, Python, …) this was solved by introducing new features to the language: <i>async</i> for declaring futures with normal code flow, and <i>await</i> for suspending execution transparently and resuming at that point in the code with the result of a future.</p>
<p>Of course this was also <a href="https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-await.md" rel="noopener" target="_top">implemented</a> in Rust. Currently based on procedural macros, but there are <a href="https://github.com/rust-lang/rfcs/pull/2394" rel="noopener" target="_top">discussions</a> to actually move this also directly into the language and standard library.</p>
<p>The above example would look something like the following with the current version of the macros</p>
<p></p><pre class="crayon-plain-tag">#[async]
fn our_future() -&gt; Result&lt;(), ()&gt; {
    let res = await!(some_future)
        .map_err(|_| ())?;

    do_something(res);
    Ok(())
}</pre><p></p>
<p>This looks almost like normal, synchronous code but is internally converted into a future and completely asynchronous.</p>
<p>Unfortunately this is currently only available on the nightly version of Rust until various bits and pieces get stabilized.</p>
<h4 id="futures-tokio">Tokio</h4>
<p>Most of the time when people talk about futures in Rust, they implicitly also mean <a href="https://tokio.rs/" rel="noopener" target="_top">Tokio</a>. Tokio is a pure Rust, cross-platform asynchronous IO library and based on the futures abstraction above. It provides a futures executor and various types for asynchronous IO, e.g. sockets and socket streams.</p>
<p>But while Tokio is a great library, we’re not going to use it here and instead implement a futures executor around <a href="https://developer.gnome.org/glib/stable/" rel="noopener" target="_top">GLib</a>. And on top of that implement various futures, also around GLib’s sister library <a href="https://developer.gnome.org/gio/stable/" rel="noopener" target="_top">GIO</a>, which is providing lots of API for synchronous and asynchronous IO.</p>
<p>Just like all IO operations in Tokio, all GLib/GIO asynchronous operations are dependent on running with their respective event loop (i.e. the futures executor) and while it’s possible to use both in the same process, each operation has to be scheduled on the correct one.</p>
<h3 id="futures-glib">Futures &amp; GLib/GIO</h3>
<p>Asynchronous operations and generally everything event related (timeouts, …) are based on callbacks that you have to register, and are running via a <i><a href="https://developer.gnome.org/glib/stable/glib-The-Main-Event-Loop.html" rel="noopener" target="_top">GMainLoop</a></i> that is executing events from a <i>GMainContext</i>. The latter is just something that stores everything that is scheduled and provides API for polling if something is ready to be executed now, while the former does exactly that: executing.</p>
<h4 id="glib-callbacks">Callbacks</h4>
<p>The callback based API is also available via the Rust bindings, and would for example look as follows</p>
<p></p><pre class="crayon-plain-tag">glib::timeout_add(20, || {
    do_something_after_20ms();
    glib::Continue(false) // don't call again
});

glib::idle_add(|| {
    do_something_from_the_main_loop();
    glib::Continue(false) // don't call again
});

some_async_operation(|res| {
    match res {
        Err(err) =&gt; report_error_somehow(),
        Ok(res) =&gt; {
            do_something_with_result(res);
            some_other_async_operation(|res| {
                do_something_with_other_result(res);
            });
        }
    }
});</pre><p></p>
<p>As can be seen here already, the callback-based approach leads to quite non-linear code and deep indentation due to all the closures. Also error handling becomes quite tricky due to somehow having handle them from a completely different call stack.</p>
<p>Compared to C this is still far more convenient due to actually having closures that can capture their environment, but we can definitely do better in Rust.</p>
<p>The above code also assumes that somewhere a main loop is running on the default main context, which could be achieved with the following e.g. inside <i>main()</i></p>
<p></p><pre class="crayon-plain-tag">let ctx = glib::MainContext::default();
let l = glib::MainLoop::new(Some(&amp;ctx), false);
ctx.push_thread_default();

// All operations here would be scheduled on this main context
do_things(&amp;l);

// Run everything until someone calls l.quit()
l.run();
ctx.pop_thread_default();</pre><p></p>
<p>It is also possible to explicitly select for various operations on which main context they should run, but that’s just a minor detail.</p>
<h4 id="glib-futures">GLib Futures</h4>
<p>To make this situation a bit nicer, I’ve implemented support for futures in the Rust bindings. This means, that the GLib <i>MainContext</i> is now a futures executor (and arbitrary futures can be scheduled on it), all the <i><a href="https://developer.gnome.org/glib/stable/glib-The-Main-Event-Loop.html#GSource" rel="noopener" target="_top">GSource</a></i> related operations in GLib (timeouts, UNIX signals, …) have futures- or stream-based variants and all the GIO asynchronous operations also come with futures variants now. The latter are autogenerated with the <a href="https://github.com/gtk-rs/gir/" rel="noopener" target="_top">gir</a> bindings code generator.</p>
<p>For enabling usage of this, the <i>futures</i> feature of the <i><a href="https://crates.io/crates/glib" rel="noopener" target="_top">glib</a></i> and <i><a href="https://crates.io/crates/gio" rel="noopener" target="_top">gio</a></i> crates have to be enabled, but that’s about it. It is currently still hidden behind a feature gate because the futures infrastructure is still going to go through some API incompatible changes in the near future.</p>
<p>So let’s take a look at how to use it. First of all, setting up the main context and executing a trivial future on it</p>
<p></p><pre class="crayon-plain-tag">let c = glib::MainContext::default();
let l = glib::MainLoop::new(Some(&amp;c), false);

c.push_thread_default();

// Spawn a future that is called from the main context
// and after printing something just quits the main loop
let l_clone = l.clone();
c.spawn(futures::lazy(move |_| {
    println!("we're called from the main context");
    l_clone.quit();
    Ok(())
});

l.run();

c.pop_thread_default();</pre><p> </p>
<p>Apart from <i>spawn()</i>, there is also a <i>spawn_local()</i>. The former can be called from any thread but requires the future to implement the <i>Send</i> trait (that is, it must be safe to send it to other threads) while the latter can only be called from the thread that owns the main context but it allows any kind of future to be spawned. In addition there is also a <i>block_on()</i> function on the main context, which allows to run non-static futures up to their completion and returns their result. The spawn functions only work with static futures (i.e. they have no references to any stack frame) and requires the futures to be infallible and resolve to <i>()</i>.</p>
<p>The above code already showed one of the advantages of using futures: it is possible to use all generic futures (that don’t require a specific executor), like <i><a href="https://docs.rs/futures/0.2.1/futures/future/fn.lazy.html" rel="noopener" target="_top">futures::lazy</a></i> or the <a href="https://docs.rs/futures/0.2.1/futures/channel/index.html" rel="noopener" target="_top">mpsc/oneshot channels</a> with GLib now. And any of the combinators that are available on futures</p>
<p></p><pre class="crayon-plain-tag">let c = MainContext::new();
                                                                                                       
let res = c.block_on(timeout_future(20)
    .and_then(move |_| {
        // Called after 20ms
        Ok(1)
    })
);

assert_eq!(res, Ok(1));</pre><p> </p>
<p>This example also shows the <i>block_on</i> functionality to return an actual value from the future (1 in this case).</p>
<h4 id="glib-gio">GIO Asynchronous Operations</h4>
<p>Similarly, all asynchronous GIO operations are now available as futures. For example to open a file asynchronously and getting a <i>gio::InputStream</i> to read from, the following could be done</p>
<p></p><pre class="crayon-plain-tag">let file = gio::File::new_for_path("Cargo.toml");

let l_clone = l.clone();
c.spawn_local(
    // Try to open the file
    file.read_async_future(glib::PRIORITY_DEFAULT)
        .map_err(|(_file, err)| {
            format!("Failed to open file: {}", err)
        })
        .and_then(move |(_file, strm)| {
            // Here we could now read from the stream, but
            // instead we just quit the main loop
            l_clone.quit();

            Ok(())
        })
);</pre><p> </p>
<p>A bigger example can be found in the gtk-rs examples repository <a href="https://github.com/gtk-rs/examples/blob/a684d90e4233d6eeab8aff5e65013cd9208b409f/src/bin/gio_futures.rs" rel="noopener" target="_top">here</a>. This example is basically reading a file asynchronously in 64 byte chunks and printing it to stdout, then closing the file.</p>
<p>In the same way, network operations or any other asynchronous operation can be handled via futures now.</p>
<h4 id="glib-async-await">Async/Await</h4>
<p>Compared to a callback-based approach, that bigger example is already a lot nicer but still quite heavy to read. With the async/await extension that I mentioned above already, the code looks much nicer in comparison and really almost like synchronous code. Except that it is not synchronous.</p>
<p></p><pre class="crayon-plain-tag">#[async]
fn read_file(file: gio::File) -&gt; Result&lt;(), String&gt; {
    // Try to open the file
    let (_file, strm) = await!(file.read_async_future(glib::PRIORITY_DEFAULT))
        .map_err(|(_file, err)| format!("Failed to open file: {}", err))?;

    Ok(())
}

fn main() {
    [...]
    let future = async_block! {
        match await!(read_file(file)) {
            Ok(()) =&gt; (),
            Err(err) =&gt; eprintln!("Got error: {}", err),
        }
        l_clone.quit();
        Ok(())
    };

    c.spawn_local(future);
    [...]
}</pre><p> </p>
<p>For compiling this code, the <i>futures-nightly</i> feature has to be enabled for the <i>glib</i> crate, and a nightly compiler must be used.</p>
<p>The bigger example from before with async/await can be found <a href="https://github.com/gtk-rs/examples/blob/a684d90e4233d6eeab8aff5e65013cd9208b409f/src/bin/gio_futures_await.rs" rel="noopener" target="_top">here</a>.</p>
<p>With this we’re already very close in Rust to having the same convenience as in other languages with asynchronous programming. And also it is very similar to what is possible in <a href="https://wiki.gnome.org/Projects/Vala/AsyncSamples" rel="noopener" target="_top">Vala</a> with GIO asynchronous operations.</p>
<h3 id="future">The Future</h3>
<p>For now this is all finished and available from GIT of the <i>glib</i> and <i>gio</i> crates. This will have to be updated in the future whenever the futures API is changing, but it is planned to stabilize all this in Rust until the end of this year.</p>
<p>In the future it might also make sense to add futures variants for all the GObject signal handlers, so that e.g. handling a click on a GTK+ button could be done similarly from a future (or rather from a <i>Stream</i> as a signal can be emitted multiple times). If this is in the end more convenient than the callback-based approach that is currently used, is to be seen. Some experimentation would be necessary here. Also how to handle return values of signal handlers would have to be figured out.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.igalia.com/vjaquez/2018/04/17/how-to-setup-a-gst-build-environment-with-intels-va-api-stack/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">How to setup a gst-build environment with Intel’s VA-API stack</span></a><div class="lastUpdated">2018年4月17日 22:43</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p><code>gst-build</code> is far superior than <code>gst-uninstalled</code> scripts for developing <a href="https://gstreamer.freedesktop.org/">GStreamer</a>, mainly because its <a href="http://mesonbuild.com/">meson</a> and <a href="https://ninja-build.org/">ninja</a> usage. Nonetheless, to integrate external dependencies it is not as easy as in <code>gst-uninstalled</code>.</p>
<p>This guide aims to show how to integrate GStreamer-VAAPI dependencies, in this case with the <a href="https://github.com/intel/intel-vaapi-driver">Intel VA-API driver</a>.</p>
<h2>Dependencies</h2>
<p>For now we will need meson master, since it <a href="https://github.com/mesonbuild/meson/pull/3269">this patch</a> is required. The pull request is already merged but it is unreleased yet.</p>
<h2>Installation</h2>
<h3>clone gst-build repository</h3>
<pre><code class="shell">$ git clone git://anongit.freedesktop.org/gstreamer/gst-build
$ cd gst-build
</code></pre>
<h3>apply custom patch for gst-build</h3>
<p>The patch will add the repositories for  <code>libva</code> and <code>intel-vaapi-driver</code>.</p>
<pre><code class="shell">$ wget https://people.igalia.com/vjaquez/gst-build-vaapi/0001-add-libva-and-intel-vaapi-driver-as-subprojects.patch
$ git am 0001-add-libva-and-intel-vaapi-driver-as-subprojects.patch
</code></pre>
<p><a href="https://people.igalia.com/vjaquez/gst-build-vaapi/0001-add-libva-and-intel-vaapi-driver-as-subprojects.patch">0001-add-libva-and-intel-vaapi-driver-as-subprojects.patch</a></p>
<h3>configure</h3>
<p>Running this command, all dependency repositories will be cloned, symbolic links created, and the <code>build</code> directory configured.</p>
<pre><code class="shell">$ meson bulid
</code></pre>
<h3>apply custom patches for libva, intel-vaapi-driver and gstreamer-vaapi</h3>
<h4>libva</h4>
<p>This patch is required since the headers files uninstalled paths doesn’t match with the ones in the “include” directives.</p>
<pre><code class="shell">$ cd libva
$ wget https://people.igalia.com/vjaquez/gst-build-vaapi/0001-build-add-headers-for-uninstalled-setup.patch
$ git am 0001-build-add-headers-for-uninstalled-setup.patch
$ cd -
</code></pre>
<p><a href="https://people.igalia.com/vjaquez/gst-build-vaapi/0001-build-add-headers-for-uninstalled-setup.patch">0001-build-add-headers-for-uninstalled-setup.patch</a></p>
<h4>intel-vaapi-driver</h4>
<p>The patch handles <code>libva</code> dependency as a subproject.</p>
<pre><code class="shell">$ cd intel-vaapi-driver
$ wget https://people.igalia.com/vjaquez/gst-build-vaapi/0001-meson-support-libva-as-subproject.patch
$ git am 0001-meson-support-libva-as-subproject.patch
$ cd -
</code></pre>
<p><a href="https://people.igalia.com/vjaquez/gst-build-vaapi/0001-meson-support-libva-as-subproject.patch">0001-meson-support-libva-as-subproject.patch</a></p>
<h4>gstreamer-vaapi</h4>
<p>Note to myself: this patch must be split and merged in upstream.</p>
<pre><code class="shell">$ cd gstreamer-vaapi
$ wget https://people.igalia.com/vjaquez/gst-build-vaapi/0001-build-meson-libva-gst-uninstall-friendly.patch
$ git am 0001-build-meson-libva-gst-uninstall-friendly.patch
$ cd -
</code></pre>
<p><a href="https://people.igalia.com/vjaquez/gst-build-vaapi/0001-build-meson-libva-gst-uninstall-friendly.patch">0001-build-meson-libva-gst-uninstall-friendly.patch</a> <em>updated: 2018/04/24</em></p>
<h3>build</h3>
<pre><code class="shell">$ ninja -C build
</code></pre>
<p>And wait a couple minutes.</p>
<h3>run uninstalled environment for testing</h3>
<pre><code class="shell">$ ninja -C build uninstalled
[gst-master] $ gst-inspect-1.0 vaapi
Plugin Details:
  Name                     vaapi
  Description              VA-API based elements
  Filename                 /opt/gst/gst-build/build/subprojects/gstreamer-vaapi/gst/vaapi/libgstvaapi.so
  Version                  1.15.0.1
  License                  LGPL
  Source module            gstreamer-vaapi
  Binary package           gstreamer-vaapi
  Origin URL               http://bugzilla.gnome.org/enter_bug.cgi?product=GStreamer

  vaapih264enc: VA-API H264 encoder
  vaapimpeg2enc: VA-API MPEG-2 encoder
  vaapisink: VA-API sink
  vaapidecodebin: VA-API Decode Bin
  vaapipostproc: VA-API video postprocessing
  vaapivc1dec: VA-API VC1 decoder
  vaapih264dec: VA-API H264 decoder
  vaapimpeg2dec: VA-API MPEG2 decoder
  vaapijpegdec: VA-API JPEG decoder

  9 features:
  +-- 9 elements

</code></pre></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://blog.nirbheek.in/2018/04/a-simple-method-of-measuring-audio.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">A simple method of measuring audio latency</span></a><div class="lastUpdated">2018年4月11日 22:13</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div dir="ltr">In my <a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html">previous blog post</a>, I talked about how I improved the latency of GStreamer's default audio capture and render elements on Windows.<br /><br />An important part of any such work is a way to accurately measure the latencies in your audio path.<br /><br />Ideally, one would use a mechanism that can track your buffers and give you a detailed breakdown of how much latency each component of your system adds. For instance, with an audio pipeline like this:<br /><br />audio-capture → filter1 → filter2 → filter3 → audio-output<br /><br />If you use GStreamer, you can use the <a href="https://gstreamer.freedesktop.org/documentation/design/tracing.html#print-processing-latencies">latency tracer</a> to measure how much latency filter1 adds, filter2 adds, and so on.<br /><br />However, sometimes you need to measure latencies added by components <i>outside</i> of your control, for instance the audio APIs provided by the operating system, the audio drivers, or even the hardware itself. In that case it's really difficult, bordering on impossible, to do an automated breakdown.<br /><br />But we do need some way of measuring those latencies, and I needed that for the aforementioned work. Maybe we can get an aggregated (total) number?<br /><br />There's a simple way to do that if we can create a loopback connection in the audio setup. What's a <i>loopback</i> you ask?<br /><br /><div><img alt="Ouroboros snake biting its tail" src="ouroboros-simple.svg" width="40%" border="0" /></div><br />Essentially, if we can redirect the audio output back to the audio input, that's called a loopback. The simplest way to do this is to connect the speaker-out/line-out to the microphone-in/line-in with a two-sided 3.5mm jack.<br /><br /><div class="separator"><a href="http://4.bp.blogspot.com/-qn_LfimYsiY/Wrn_akYmgGI/AAAAAAAACGk/H4Tm1KfMGtE6-9Fhwk3k0W6plIMAqF65QCK4BGAYYCw/s1600/photo_2018-03-27_13-52-08.jpg"><img alt="photo of male-to-male 3.5mm jack connecting speaker-out to mic-in" src="photo_2018-03-27_13-52-08.jpg" width="400" height="300" border="0" /></a></div><br />Now, when we send an audio wave down to the audio output, it'll show up on the audio input.<br /><br />Hmm, what if we store the <a href="https://developer.gnome.org/glib/stable/glib-Date-and-Time-Functions.html#g-get-monotonic-time">current time</a> when we send the wave out, and compare it with the current time when we get it back? Well, that's the total end-to-end latency!<br /><br />If we send out a wave periodically, we can measure the latency continuously, even as things are switched around or the pipeline is dynamically reconfigured.<br /><br />Some of you may notice that this is somewhat similar to how the `ping` command measures latencies across the Internet.<br /><br /><div class="separator"><a href="http://1.bp.blogspot.com/-pTpGe6rVoIs/WrZBZhJKgOI/AAAAAAAACGA/gMvhESNqozAD4DJzXMBD8eeTcG0FfGqywCK4BGAYYCw/s1600/ping.png"><img alt="screenshot of ping to 192.168.1.1" src="ping.png" border="0" /></a></div><br /><br />Just like a network connection, the loopback connection can be lossy or noisy, f.ex. if you use loudspeakers and a microphone instead of a wire, or if you have (ugh) noise in your line. But unlike network packets, we lose all context once the waves leave our  pipeline and we have no way of uniquely identifying each wave.<br /><br />So the simplest reliable implementation  is to have only one wave traveling down the pipeline at a time. If we send a wave out, say, once a second, we can wait about one second for it to show up, and otherwise presume that it was lost.<br /><br />That's exactly how the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-audiolatency.html">audiolatency GStreamer plugin</a> that I wrote works! Here you can see its output while measuring the combined latency of the <a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html">WASAPI source and sink elements</a>:<br /><br /><div class="separator"><a href="http://1.bp.blogspot.com/-9j3Hs5bzz_M/WsPOvM-bkKI/AAAAAAAACHA/go0r1u8AX-847lId2whNuiwPsyYrZwYFgCK4BGAYYCw/s1600/wasapi-latency.png"><img src="wasapi-latency.png" border="0" /></a></div><br />The first measurement will always be wrong because of various implementation details in the audio stack, but the next measurements should all be correct.<br /><br />This mechanism does place an upper bound on the latency that we can measure, and on how often we can measure it, but it should be possible to take more frequent measurements by sending a new wave as soon as the previous one was received (with a 1 second timeout). So this is an enhancement that can be done if people need this feature.<br /><br />Hope  you find the element useful; <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-audiolatency.html#gst-plugins-bad-plugins-audiolatency.description">go forth and measure</a>!</div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://coaxion.net/blog/2018/04/improving-gstreamer-performance-on-a-high-number-of-network-streams-by-sharing-threads-between-elements-with-rusts-tokio-crate/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Improving GStreamer performance on a high number of network streams by sharing threads between elements with Rust’s tokio crate</span></a><div class="lastUpdated">2018年4月5日 23:21</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>For one of our customers at <a href="https://centricular.com/" rel="noopener" target="_top">Centricular</a> we were working on a quite interesting project. Their use-case was basically to receive an as-high-as-possible number of audio <a href="https://en.wikipedia.org/wiki/Real-time_Transport_Protocol" rel="noopener" target="_top">RTP</a> streams over UDP, transcode them, and then send them out via UDP again. Due to how <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> usually works, they were running into some performance issues.</p>
<p>This blog post will describe the first set of improvements that were implemented for this use-case, together with a minimal benchmark and the results. My colleague <a href="https://mathieuduponchelle.github.io/" rel="noopener" target="_top">Mathieu</a> will follow up with one or two other blog posts with the other improvements and a more full-featured benchmark.</p>
<p>The short version is that <strong>CPU usage decreased by about 65-75%</strong>, i.e. allowing <strong>3-4x more streams</strong> with the same CPU usage. Also parallelization works better and usage of different CPU cores is more controllable, allowing for better scalability. And a fixed, but configurable number of threads is used, which is independent of the number of streams.</p>
<p>The code for this blog post can be found <a href="https://github.com/sdroege/gst-plugin-threadshare" rel="noopener" target="_top">here</a>.</p>
<h4 id="toc">Table of Contents</h4>
<ol>
<li><a href="https://coaxion.net/blog/feed/#threads">GStreamer &amp; Threads</a></li>
<li><a href="https://coaxion.net/blog/feed/#elements">Thread-Sharing GStreamer Elements</a></li>
<li><a href="https://coaxion.net/blog/feed/#available">Available Elements</a></li>
<li><a href="https://coaxion.net/blog/feed/#benchmark">Little Benchmark</a></li>
<li><a href="https://coaxion.net/blog/feed/#conclusion">Conclusion</a></li>
</ol>
<h4 id="threads">GStreamer &amp; Threads</h4>
<p>In GStreamer, by default each source is running from its own OS thread. Additionally, for receiving/sending RTP, there will be another thread in the RTP jitterbuffer, yet another thread for receiving RTCP (another source) and a last thread for sending RTCP at the right times. And RTCP has to be received and sent for the receiver and sender side part of the pipeline, so the number of threads doubles. In the sum this gives at least <strong>1 + 1 + (1 + 1) * 2 = 6 threads per RTP stream</strong> in this scenario. In a normal audio scenario, there will be one packet received/sent e.g. every 20ms on each stream, and every now and then an RTCP packet. So most of the time all these threads are only waiting.</p>
<p>Apart from the obvious waste of OS resources (<strong>1000 streams would be 6000 threads</strong>), this also brings down performance as all the time threads are being woken up. This means that context switches have to happen basically all the time.</p>
<p>To solve this we implemented a mechanism to share threads, and in the end as a result we have a fixed, but configurable number of threads that is independent from the number of streams. And can run e.g. <strong>500 streams just fine on a single thread</strong> with a single core, which was completely impossible before. In addition we also did some work to reduce the number of allocations for each packet, so that after startup no additional allocations happen per packet anymore for buffers. See Mathieu’s upcoming blog post for details. </p>
<p>In this blog post, I’m going to write about a generic mechanism for sources, queues and similar elements to share their threads between each other. For the RTP related bits (RTP jitterbuffer and RTCP timer) this was not used due to reuse of existing C codebases.</p>
<h4 id="elements">Thread-Sharing GStreamer Elements</h4>
<p>The code in question can be found <a href="https://github.com/sdroege/gst-plugin-threadshare" rel="noopener" target="_top">here</a>, a small benchmark is in the <i>examples</i> directory and it is going to be used for the results later. A full-featured benchmark will come in Mathieu’s blog post.</p>
<p>This is a new GStreamer plugin, written in <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a> and around the <a href="https://tokio.rs/" rel="noopener" target="_top">Tokio</a> crate for asynchronous IO and generally a “task scheduler”.</p>
<p>While this could certainly also have been written in C around something like <a href="https://libuv.org/" rel="noopener" target="_top">libuv</a>, doing this kind of work in Rust is simply more productive and fun due to its safety guarantees and the strong type system, which definitely reduced the amount of debugging a lot. And in addition “modern” language features like closures, which make working with futures much more ergonomic.</p>
<p>When using these elements it is important to have full control over the pipeline and its elements, and the dataflow inside the pipeline has to be carefully considered to properly configure how to share threads. For example the following two restrictions should be kept in mind all the time:</p>
<ol>
<li>Downstream of such an element, the streaming thread must never ever block for considerable amounts of time. Otherwise all other elements inside the same thread-group would be blocked too, even if they could do any work now</li>
<li>This generally all works better in live pipelines, where media is produced in real-time and not as fast as possible</li>
</ol>
<h4 id="available">Available Elements</h4>
<p>So this repository currently contains the generic infrastructure (see the <i>src/iocontext.rs</i> source file) and a couple of elements:</p>
<ul>
<li>an UDP source: <i>ts-udpsrc</i>, a replacement for udpsrc</li>
<li>an app source: <i>ts-appsrc</i>, a replacement for appsrc to inject packets into the pipeline from the application</li>
<li>a queue: <i>ts-queue</i>, a replacement for queue that is useful for adding buffering to a pipeline part. The upstream side of the queue will block if not called from another thread-sharing element, but if called from another thread-sharing element it will pause the current task asynchronously. That is, stop the upstream task from producing more data.</li>
<li>a proxysink/src element: <i>ts-proxysrc</i>, <i>ts-proxysink</i>, replacements for proxysink/proxysrc for connecting two pipelines with each other. This basically works like the queue, but split into two elements.</li>
<li>a tone generator source around <a href="https://www.soft-switch.org/" rel="noopener" target="_top">spandsp</a>: <i>ts-tonesrc</i>, a replacement for tonegeneratesrc. This also contains some minimal FFI bindings for that part of the spandsp C library.</li>
</ul>
<p>All these elements have more or less the same API as their non-thread-sharing counterparts.</p>
<p>API-wise, each of these elements has a set of properties for controlling how it is sharing threads with other elements, and with which elements:</p>
<ul>
<li><i>context</i>: A string that defines in which group this element is. All elements with the same context are running on the same thread or group of threads, </li>
<li><i>context-threads</i>: Number of threads to use in this context. <i>-1</i> means exactly one thread, <i>1</i> and above used N+1 threads (1 thread for polling fds, N worker threads) and <i>0</i> sets N to the number of available CPU cores. As long as no considerable work is done in these threads, <i>-1</i> has shown to be the most efficient. See also <a href="https://github.com/tokio-rs/tokio/issues/265" rel="noopener" target="_top">this tokio GitHub issue</a></li>
<li><i>context-wait</i>: Number of milliseconds that the threads will wait on each iteration. This allows to reduce CPU usage even further by handling all events/packets that arrived during that timespan to be handled all at once instead of waking up the thread every time a little event happens, thus reducing context switches again</li>
</ul>
<p>The elements are all pushing data downstream from a tokio thread whenever data is available, assuming that downstream does not block. If downstream is another thread-sharing element and it would have to block (e.g. a full queue), it instead returns a new future to upstream so that upstream can asynchronously wait on that future before producing more output. By this, back-pressure is implemented between different GStreamer elements without ever blocking any of the tokio threads. All this is implemented around the normal GStreamer data-flow mechanisms, there is no “tokio fast-path” between elements.</p>
<h4 id="benchmark">Little Benchmark</h4>
<p>As mentioned above, there’s a small benchmark application in the <i>examples</i> directory. This basically sets up a configurable number of streams and directly connects them to a fakesink, throwing away all packets. Additionally there is another thread that is sending all these packets. As such, this is really the most basic benchmark and not very realistic but nonetheless it shows the same performance improvement as the real application. Again, see Mathieu’s upcoming blog post for a more realistic and complete benchmark.</p>
<p>When running it, make sure that your user can create enough fds. The benchmark will just abort if not enough fds can be allocated. You can control this with <i>ulimit -n SOME_NUMBER</i>, and allowing a couple of thousands is generally a good idea. The benchmarks below were running with 10000.</p>
<p>After running <i>cargo build –release</i> to build the plugin itself, you can run the benchmark with:</p>
<p></p><pre class="crayon-plain-tag">cargo run --release --example udpsrc-benchmark -- 1000 ts-udpsrc -1 1 20</pre><p> </p>
<p>and in another shell the UDP sender with</p>
<p></p><pre class="crayon-plain-tag">cargo run --release --example udpsrc-benchmark-sender -- 1000</pre><p> </p>
<p>This runs 1000 streams, uses <i>ts-udpsrc</i> (alternative would be <i>udpsrc</i>), configures exactly one thread <i>-1</i>, 1 context, and a wait time of 20ms. See above for what these settings mean. You can check CPU usage with e.g. <i>top</i>. Testing was done on an Intel i7-4790K, with Rust 1.25 and GStreamer 1.14. One packet is sent every 20ms for each stream.</p>
<table>
<tbody><tr>
<th>Source</th>
<th>Streams</th>
<th>Threads</th>
<th>Contexts</th>
<th>Wait</th>
<th>CPU</th>
</tr>
<tr>
<td>udpsrc</td>
<td>1000</td>
<td>1000</td>
<td>x</td>
<td>x</td>
<td>44%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>1000</td>
<td>-1</td>
<td>1</td>
<td>0</td>
<td>18%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>1000</td>
<td>-1</td>
<td>1</td>
<td>20</td>
<td>13%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>1000</td>
<td>-1</td>
<td>2</td>
<td>20</td>
<td>15%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>1000</td>
<td>2</td>
<td>1</td>
<td>20</td>
<td>16%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>1000</td>
<td>2</td>
<td>2</td>
<td>20</td>
<td>27%</td>
</tr>
<tr>
<th>Source</th>
<th>Streams</th>
<th>Threads</th>
<th>Contexts</th>
<th>Wait</th>
<th>CPU</th>
</tr>
<tr>
<td>udpsrc</td>
<td>2000</td>
<td>2000</td>
<td>x</td>
<td>x</td>
<td>95%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>2000</td>
<td>-1</td>
<td>1</td>
<td>20</td>
<td>29%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>2000</td>
<td>-1</td>
<td>2</td>
<td>20</td>
<td>31%</td>
</tr>
<tr>
<th>Source</th>
<th>Streams</th>
<th>Threads</th>
<th>Contexts</th>
<th>Wait</th>
<th>CPU</th>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>3000</td>
<td>-1</td>
<td>1</td>
<td>20</td>
<td>36%</td>
</tr>
<tr>
<td>ts-udpsrc</td>
<td>3000</td>
<td>-1</td>
<td>2</td>
<td>20</td>
<td>47%</td>
</tr>
</tbody></table>
<p>Results for 3000 streams for the old udpsrc are not included as starting up that many threads needs too long.</p>
<p>The best configuration is apparently a single thread per context (see <a href="https://github.com/tokio-rs/tokio/issues/265" rel="noopener" target="_top">this tokio GitHub issue</a>) and waiting 20ms for every iterations. Compared to the old udpsrc, CPU usage is about one third in that setting, and generally it seems to parallelize well. It’s not clear to me why the last test has 11% more CPU with two contexts, while in every other test the number of contexts does not really make a difference, and also not for that many streams in the real test-case.</p>
<p>The waiting does not reduce CPU usage a lot in this benchmark, but on the real test-case it does. The reason is most likely that this benchmark basically sends all packets at once, then waits for the remaining time, then sends the next packets.</p>
<p>Take these numbers with caution, the real test-case in Mathieu’s blog post will show the improvements in the bigger picture, where it was generally a quarter of CPU usage and almost perfect parallelization when increasing the number of contexts.</p>
<h4 id="conclusion">Conclusion</h4>
<p>Generally this was a fun exercise and we’re quite happy with the results, especially the real results. It took me some time to understand how tokio works internally so that I can implement all kinds of customizations on top of it, but for normal usage of tokio that should not be required and the overall design makes a lot of sense to me, as well as the way how futures are implemented in Rust. It requires some learning and understanding how exactly the API can be used and behaves, but once that point is reached it seems like a very productive and performant solution for asynchronous IO. And modelling asynchronous IO problems based on the Rust-style futures seems a nice and intuitive fit.</p>
<p>The performance measurements also showed that GStreamer’s default usage of threads is not always optimal, and a model like in <a href="http://upipe.org/" rel="noopener" target="_top">upipe</a> or <a href="https://pipewire.org/" rel="noopener" target="_top">pipewire</a> (or rather SPA) can provide better performance. But as this also shows, it is possible to implement something like this on top of GStreamer and for the common case, using threads like in GStreamer reduces the cognitive load on the developer a lot.</p>
<p>For a future version of GStreamer, I don’t think we should make the threading “manual” like in these two other projects, but instead provide some API additions that make it nicer to implement thread-sharing elements and to add ways in the GStreamer core to make streaming threads non-blocking. All this can be implemented already, but it could be nicer.</p>
<p>All this “only” improved the number of threads, and thus the threading and context switching overhead. Many other optimizations in other areas are still possible on top of this, for example optimizing receive performance and reducing the number of memory copies inside the pipeline even further. If that’s something you would be interested in, feel free to get in touch.</p>
<p>And with that: Read Mathieu’s upcoming blog posts about the other parts, RTP jitterbuffer / RTCP timer thread sharing, and no allocations, and the full benchmark.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Latency in Digital Audio</span></a><div class="lastUpdated">2018年4月5日 12:04</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div dir="ltr">We've come a long way since <a href="https://en.wikipedia.org/wiki/Invention_of_the_telephone" target="_top">Alexander Graham Bell</a>, and everything's turned digital.<br /><br />Compared to analog audio, <a href="https://en.wikipedia.org/wiki/Digital_signal_processing" target="_top">digital audio processing </a>is extremely versatile, is much easier to design and implement than  analog processing, and also adds effectively zero noise along the way. With rising computing power and dropping costs, every operating system has had drivers, engines, and libraries to record, process, playback, transmit, and store audio for over 20 years.<br /><br /><div>Today we'll talk about the some of the differences between analog and digital audio, and how the widespread use of digital audio adds a new challenge: <i>latency</i>.</div><br /><h2>Analog vs Digital</h2><div><br /></div><div><b>Analog data</b> flows like water through an empty pipe. You open the tap, and the time it takes for the first drop of water to reach you is the latency. When analog audio is transmitted through, say, an <a href="https://en.wikipedia.org/wiki/RCA_connector" target="_top">RCA cable</a>, the transmission happens at the speed of electricity and your latency is:<code></code><br /><br /><div><img alt="wire length/speed of electricity" src="analog-latency.svg" /></div><br />This number is ridiculously small<span class="st">—</span>especially when compared to the speed of sound. An electrical signal takes 0.001 milliseconds to travel 300 metres (984 feet). Sound takes 874 milliseconds (almost a second).<br /><br />All analog effects and filters obey similar equations. If you're using, say, an analog pedal with an electric guitar, the signal is transformed continuously by  an electrical circuit, so the latency is a function of the wire length (plus capacitors/transistors/etc), and is almost always negligible.<br /><br /><b>Digital audio</b> is transmitted in "packets" (buffers) of a particular size, like a <a href="https://en.wikipedia.org/wiki/Bucket_brigade" target="_top">bucket brigade</a>, but at the speed of electricity. Since the real world is analog, this means to record audio, you must use an <a href="https://en.wikipedia.org/wiki/Analog-to-digital_converter" target="_top">Analog-Digital Converter</a>. The <abbr title="Analog-Digital Converter">ADC</abbr> <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)" target="_top">quantizes</a> <a href="https://wiki.xiph.org/Videos/A_Digital_Media_Primer_For_Geeks" target="_top">the signal</a> into digital measurements (samples), packs multiple samples into  a buffer, and sends it forward. This means your latency is now: </div><br /><div><img alt="(wire length/speed of electricity) + buffer size" src="digital-latency.svg" /></div><div><br />We saw above that the first part is insignificant, what about the second part?<br /><br />Latency is measured in time, but buffer size is measured in bytes. For <a href="https://en.wikipedia.org/wiki/Audio_bit_depth" target="_top">16-bit integer audio</a>, each measurement (sample) is stored as a 16-bit integer, which is 2 bytes. That's the theoretical lower limit on the buffer size. The <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Sampling_rate" target="_top">sample rate</a> defines how often measurements are made, and these days, is usually 48KHz. This means each sample contains ~0.021ms of audio. To go lower, we need to increase the sample rate to 96KHz or 192KHz.<br /><br />However, when general-purpose computers are involved, the buffer size is almost never lower than 32 bytes, and is usually 128 bytes or larger. For <a href="https://en.wikipedia.org/wiki/Multichannel_audio">single-channel</a> 16-bit integer audio at 48KHz, a 32 byte buffer is 0.33ms, and a 128 byte buffer is 1.33ms. This is our buffer size and hence the base latency while recording (or playing) digital audio.<br /><br />Digital effects operate on individual buffers, and will add an additional amount of latency depending on the delay added by the CPU processing required by the effect. Such effects may also add latency if the algorithm used requires that, but that's the same with analog effects.<br /><br /><h2>The Digital Age</h2><br />So everyone's using digital. But isn't 1.33ms a lot of additional latency?<br /><br />It might seem that way till you think about it in real-world terms. Sound travels less than half a meter (1<span class="st">½</span> feet) in that time, and that sort of delay is completely unnoticeable by humans<span class="st">—</span>otherwise we'd notice people's lips moving before we heard their words.<br /><br />In fact, 1.33ms is too small for the majority of audio applications!<br /><br />To process such small buffer sizes, you'd have to wake the CPU up <abbr title="1000 / 1.33">750 times a second</abbr>, just for audio. This is highly inefficient, and wastes a lot of power. You really don't want that on your phone or your laptop, and is completely unnecessary in most cases anyway. <br /><br />For instance, your music player will usually use a buffer size of ~200ms, which is just <i>5</i> CPU wakeups per second. Note that this doesn't mean that you will hear sound 200ms after hitting "play". The audio player will just send 200ms of audio to the sound card at once, and playback will begin immediately.<br /><br />Of course, you can't do that with live playback such as video calls<span class="st">—y</span>ou can't "read-ahead" data you don't have. You'd have to invent a time machine first. As a result, apps that use real-time communication have to use smaller buffer sizes because that directly affects the latency of live playback.<br /><br />That brings us back to efficiency. These apps also need to conserve power, and 1.33ms buffers are really wasteful. Most consumer apps that require low latency use 10-15ms buffers, and that's good enough for things like voice/video calling, video games, notification sounds, and so on.<br /><br /><h2>Ultra Low Latency</h2><br />There's one category left: musicians, sound engineers, and other folk that work in the pro-audio business. For them, 10ms of latency is much too high!<br /><br />You usually can't notice a 10ms delay between an event and the sound for it, but when making music, you <i>can</i> hear it when two instruments are out-of-sync by 10ms or if the sound for an instrument you're playing is delayed. Instruments such as drum snare are more susceptible to this problem than others, which is why the <a href="https://en.wikipedia.org/wiki/Stage_monitor_system" target="_top">stage monitors</a> used in live concerts must not add any latency.<br /><br />The standard in the music business is to use buffers that are 5ms or lower, down to the 0.33ms number that we talked about  above.<br /><br />Power consumption is absolutely no concern, and the real problems are the accumulation of small amounts of latencies everywhere in your stack, and ensuring that you're able to read buffers from the hardware or write buffers to the hardware fast enough.<br /><br />Let's say you're using an app on your computer to apply digital effects to a guitar that you're playing. This involves capturing audio from the line-in port, sending it to the application for processing, and playing it from the sound card to your amp.<br /><br />The latency while capturing and outputting audio are both multiples of the buffer size, so it adds up very quickly. The effects app itself will also add a variable amount of latency, and at 1.33ms buffer sizes you will find yourself quickly approaching a 10ms latency from line-in to amp-out. The only way to lower this is to use a smaller buffer size, which is precisely what pro-audio hardware and software enables.<br /><br />The second problem is that of CPU scheduling. You need to ensure that the threads that are fetching/sending audio data to the hardware and processing the audio have the highest priority, so that nothing else will steal CPU-time away from them and cause glitching due to buffers arriving late.<br /><br />This gets harder as you lower the buffer size because the audio stack has to do more work for each bit of audio. The fact that we're doing this on a general-purpose operating system makes it even harder, and requires implementing <a href="https://en.wikipedia.org/wiki/Real-time_computing" target="_top">real-time scheduling</a> features across several layers. But that's a story for another time!<br /><br />I hope you found this dive into digital audio interesting! My next post <span>will be</span> is about my journey in <a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html">implementing ultra low latency capture and render on Windows</a> in the <a href="https://msdn.microsoft.com/library/windows/desktop/dd371455.aspx" target="_top">WASAPI</a> plugin for <a href="https://en.wikipedia.org/wiki/GStreamer" target="_top">GStreamer</a>. This was already possible on Linux with the JACK GStreamer plugin and on macOS with the CoreAudio GStreamer plugin, so it will be interesting to see how the same problems are solved on Windows. Tune in!</div></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-03-28T23:30:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.12.5 old-stable bugfix release</span></a><div class="lastUpdated">2018年3月29日 7:30</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce the fifth and likely last bugfix
release in the old stable 1.12 release series of your favourite cross-platform
multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from
1.12.x.
</p><p>
The 1.12 stable series is now superseded by the 1.14 stable series, and 1.12.5
will likely be the last bugfix release in the 1.12 series.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.12/#1.12.5">/releases/1.12/</a>
for the details.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Download tarballs directly here: 
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.12.5.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.12.5.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.12.5.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.12.5.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.12.5.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.12.5.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.12.5.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.12.5.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.12.5.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.12.5.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.12.5.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.12.5.tar.xz">gst-omx</a>.
        </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.igalia.com/vjaquez/2018/03/28/gstreamer-va-api-troubleshooting/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer VA-API Troubleshooting</span></a><div class="lastUpdated">2018年3月29日 2:11</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>GStreamer VA-API is not a trivial piece of software. Even though, in my opinion it is a bit over-engineered, the complexity relies on its layered architecture: the user must troubleshoot in which layer is the failure.</p>
<p>So, bear in mind this architecture:</p>
<p>GStreamer VA-API is not a trivial piece of software. Even though, in my opinion it is a bit over-engineered, the complexity relies on its layered architecture: the user must troubleshoot in which layer is the failure.</p>
<p>So, bear in mind this architecture:</p>
<a href="http://blogs.igalia.com/vjaquez/files/2018/03/va.png"><img src="va-580x804.png" alt="libva architecture" class="size-medium wp-image-639" width="580" height="804" /></a>libva architecture
<p>And the point of failure could be anywhere.</p>
<h2>Drivers</h2>
<p><a href="https://github.com/intel/libva">libva</a> is a library designed to load another library called driver or back-end. This driver is responsible to <em>talk</em> with the kernel, windowing platform, memory handling library, or any other piece of software or hardware that actually will do the video processing.</p>
<p>There are many drivers in the wild. As it is an API aiming to stateless video processing, and the industry is moving towards that way to process video, it is expected more drivers would appear in the future.</p>
<p>Nonetheless, not all the drivers have the same level of maturity, and some of them are abandon-ware. For this reason we decided in GStreamer VA-API, some time ago, to add a white list of <em>functional drivers</em>, basically, those developed by <a href="https://mesa3d.org/">Mesa3D</a> and <a href="https://github.com/intel/intel-vaapi-driver">this one</a> from Intel<img src="2122.png" alt="™" class="wp-smiley" />. If you wish to disable that white-list, you can do it by setting an environment variable:</p>
<pre><code class="sh">$ export GST_VAAPI_ALL_DRIVERS=1
</code></pre>
<p><strong>Remember</strong>, if you set it, you are on your own, since we do not trust on the maturity of that driver yet.</p>
<h3>Internal libva<img src="2194.png" alt="↔" class="wp-smiley" />driver version</h3>
<p>Thus, there is an internal API between <code>libva</code> and the driver and it is versioned, meaning that the internal API version of the installed <code>libva</code> library must match with the internal API exposed by the driver. One of the causes that <code>libva</code> could not initialize a driver could be because the internal API version does not match.</p>
<h3>Drivers path and driver name</h3>
<p>By default there is a path where <code>libva</code> looks for drivers to load. That path is defined at compilation time. Following Debian’s file-system hierarchy standard (<a href="https://wiki.linuxfoundation.org/lsb/fhs">FHS</a>) it should be set by distributions in <code>/usr/lib/x86_64-linux-gnu/dri/</code>. But the user can control this path with an environment variable:</p>
<pre><code class="sh">$ export LIBVA_DRIVERS_PATH=${HOME}/src/intel-vaapi-driver/src/.libs
</code></pre>
<p>The driver path, as a directory, might contain several drivers. <code>libva</code> will try to guess the <em>correct</em> one by querying the instantiated VA display (which could be either KMS/DRM, Wayland, Android or X11). If the user instantiates a VA display different of his running environment, the guess will be erroneous, the library loading will fail.</p>
<p>Although, there is a way for the user to set the driver’s name too. Again, by setting an environment variable:</p>
<pre><code class="sh">$ export LIBVA_DRIVER_NAME=iHD
</code></pre>
<p>With this setting, <code>libva</code> will try to load <code>iHD_drv_video.so</code> (a new and experimental open source driver from Intel<img src="2122.png" alt="™" class="wp-smiley" />, targeted for <a href="https://github.com/Intel-Media-SDK/MediaSDK">MediaSDK</a> —do not use it yet with GStreamer VAAPI—).</p>
<h2>vainfo</h2>
<p><code>vainfo</code> is <em>the diagnostic tool for VA-API</em>. In a couple words, it will iterate on a list of VA displays, in try-and-error strategy, and try to initialize VA. In case of success, <code>vainfo</code> will report the driver signature, and it will query the driver for the available profiles and entry-points.</p>
<p>For example, my skylake board for development will report</p>
<pre><code class="sh">$ vainfo
error: can't connect to X server!
libva info: VA-API version 1.1.0
libva info: va_getDriverName() returns 0
libva info: Trying to open /home/vjaquez/gst/master/intel-vaapi-driver/src/.libs/i965_drv_video.so
libva info: Found init function __vaDriverInit_1_1
libva info: va_openDriver() returns 0
vainfo: VA-API version: 1.1 (libva 2.1.1.pre1)
vainfo: Driver version: Intel i965 driver for Intel(R) Skylake - 2.1.1.pre1 (2.1.0-41-g99c3748)
vainfo: Supported profile and entrypoints
      VAProfileMPEG2Simple            : VAEntrypointVLD
      VAProfileMPEG2Simple            : VAEntrypointEncSlice
      VAProfileMPEG2Main              : VAEntrypointVLD
      VAProfileMPEG2Main              : VAEntrypointEncSlice
      VAProfileH264ConstrainedBaseline: VAEntrypointVLD
      VAProfileH264ConstrainedBaseline: VAEntrypointEncSlice
      VAProfileH264ConstrainedBaseline: VAEntrypointEncSliceLP
      VAProfileH264ConstrainedBaseline: VAEntrypointFEI
      VAProfileH264ConstrainedBaseline: VAEntrypointStats
      VAProfileH264Main               : VAEntrypointVLD
      VAProfileH264Main               : VAEntrypointEncSlice
      VAProfileH264Main               : VAEntrypointEncSliceLP
      VAProfileH264Main               : VAEntrypointFEI
      VAProfileH264Main               : VAEntrypointStats
      VAProfileH264High               : VAEntrypointVLD
      VAProfileH264High               : VAEntrypointEncSlice
      VAProfileH264High               : VAEntrypointEncSliceLP
      VAProfileH264High               : VAEntrypointFEI
      VAProfileH264High               : VAEntrypointStats
      VAProfileH264MultiviewHigh      : VAEntrypointVLD
      VAProfileH264MultiviewHigh      : VAEntrypointEncSlice
      VAProfileH264StereoHigh         : VAEntrypointVLD
      VAProfileH264StereoHigh         : VAEntrypointEncSlice
      VAProfileVC1Simple              : VAEntrypointVLD
      VAProfileVC1Main                : VAEntrypointVLD
      VAProfileVC1Advanced            : VAEntrypointVLD
      VAProfileNone                   : VAEntrypointVideoProc
      VAProfileJPEGBaseline           : VAEntrypointVLD
      VAProfileJPEGBaseline           : VAEntrypointEncPicture
      VAProfileVP8Version0_3          : VAEntrypointVLD
      VAProfileVP8Version0_3          : VAEntrypointEncSlice
      VAProfileHEVCMain               : VAEntrypointVLD
      VAProfileHEVCMain               : VAEntrypointEncSlice
</code></pre>
<p>And my AMD board with stable packages replies:</p>
<pre><code class="sh">$ vainfo
libva info: VA-API version 0.40.0
libva info: va_getDriverName() returns 0
libva info: Trying to open /usr/lib64/dri/radeonsi_drv_video.so
libva info: Found init function __vaDriverInit_0_40
libva info: va_openDriver() returns 0
vainfo: VA-API version: 0.40 (libva )
vainfo: Driver version: mesa gallium vaapi
vainfo: Supported profile and entrypoints
      VAProfileMPEG2Simple            : VAEntrypointVLD
      VAProfileMPEG2Main              : VAEntrypointVLD
      VAProfileVC1Simple              : VAEntrypointVLD
      VAProfileVC1Main                : VAEntrypointVLD
      VAProfileVC1Advanced            : VAEntrypointVLD
      VAProfileH264ConstrainedBaseline: VAEntrypointVLD
      VAProfileH264ConstrainedBaseline: VAEntrypointEncSlice
      VAProfileH264Main               : VAEntrypointVLD
      VAProfileH264Main               : VAEntrypointEncSlice
      VAProfileH264High               : VAEntrypointVLD
      VAProfileH264High               : VAEntrypointEncSlice
      VAProfileNone                   : VAEntrypointVideoProc
</code></pre>
<p>Does this mean that VA-API processes video? No. It means that there is an usable VA display which could open a driver correctly and <code>libva</code> can extract symbols from it.</p>
<p>I would like to mention another tool, not official, but I like it a lot, since it extracts almost of the VA information available in the driver: <a href="http://ixia.jkqxz.net/~mrt/vadumpcaps.c">vadumpcaps.c</a>, written by Mark Thompson.</p>
<h2>GStreamer VA-API registration</h2>
<p>When GStreamer is launched, normally it will register all the available plugins and plugin features (elements, device providers, etc.). All that data is cache and keep until the cache file is deleted or the cache invalidated by some event.</p>
<p>At registration time, GStreamer VA-API will instantiate a DRM-based VA display, which works with no need of a real display (in other words, headless), and will query the driver for the profiles and entry-points tuples, in order to register only the available elements (encoders, decoders. sink, post-processor). If the DRM VA display fails, a list of VA displays will be tried.</p>
<p>In the case that <code>libva</code> could not load any driver, or the driver is not in the white-list, GStreamer VA-API will not register any element. Otherwise <code>gst-inspect-1.0</code> will show the registered elements:</p>
<pre><code class="sh">$ gst-inspect-1.0 vaapi
Plugin Details:
  Name                     vaapi
  Description              VA-API based elements
  Filename                 /usr/lib/x86_64-linux-gnu/gstreamer-1.0/libgstvaapi.so
  Version                  1.12.4
  License                  LGPL
  Source module            gstreamer-vaapi
  Source release date      2017-12-07
  Binary package           gstreamer-vaapi
  Origin URL               http://bugzilla.gnome.org/enter_bug.cgi?product=GStreamer

  vaapijpegdec: VA-API JPEG decoder
  vaapimpeg2dec: VA-API MPEG2 decoder
  vaapih264dec: VA-API H264 decoder
  vaapivc1dec: VA-API VC1 decoder
  vaapivp8dec: VA-API VP8 decoder
  vaapih265dec: VA-API H265 decoder
  vaapipostproc: VA-API video postprocessing
  vaapidecodebin: VA-API Decode Bin
  vaapisink: VA-API sink
  vaapimpeg2enc: VA-API MPEG-2 encoder
  vaapih265enc: VA-API H265 encoder
  vaapijpegenc: VA-API JPEG encoder
  vaapih264enc: VA-API H264 encoder

  13 features:
  +-- 13 elements
</code></pre>
<p>Beside the normal behavior, GStreamer VA-API will also invalidate GStreamer’s cache at every boot, or when any of the mentioned environment variables change.</p>
<h2>Conclusion</h2>
<p>A simple task list to review when GStreamer VA-API is not working at all is this:</p>
<p>#. Check your <code>LIBVA_*</code> environment variables<br />
#. Verify that <code>vainfo</code> returns sensible information<br />
#. Invalidate GStreamer’s cache (or just delete the file)<br />
#. Check the output of <code>gst-inspect-1.0 vaapi</code></p>
<p>And, if you decide to file a bug in bugzilla, please do not forget to attach the output of <code>vainfo</code> and the logs if the developer asks for them.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.igalia.com/vjaquez/2018/03/27/gstreamer-va-api-1-14-whats-new/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer VA-API 1.14: what’s new?</span></a><div class="lastUpdated">2018年3月27日 18:52</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>As you may already know, there is a new release of GStreamer, 1.14. In this blog post we will talk about the new features and improvements of GStreamer VA-API module, though you have a more comprehensive list of changes in the <a href="https://gstreamer.freedesktop.org/releases/1.14/">release notes</a>.</p>
<p>Most of the topics explained along this blog post are already mentioned in the release notes, but a bit more detailed.</p>
<h2>DMABuf usage</h2>
<p>We have improved DMA-buf’s usage, mostly at downstream.</p>
<p>In the case of upstream, we just got rid a nasty hack which detected when to instantiate and use a buffer pool in sink pad with a dma-buf based allocator. This functionality has been already broken for a while, and that code was the wrong way to enabled it. The sharing of a dma-buf based buffer pool to upstream is going to be re-enabled after <a href="https://bugzilla.gnome.org/show_bug.cgi?id=792034">bug 792034</a> is merged.</p>
<p>For downstream, we have added the handling of <code>memory:DMABuf</code> caps feature. The purpose of this caps feature is to negotiate a media when the buffers <strong>are not</strong> map-able onto user space, because of digital rights or platform restrictions.</p>
<p>For example, currently <a href="https://github.com/intel/intel-vaapi-driver">intel-vaapi-driver</a> doesn’t allow the mapping of its produced dma-buf descriptors. But, as we cannot know if a back-end produces or not map-able dma-buf descriptors, <code>gstreamer-vaapi</code>, when the allocator is instantiated, creates a dummy buffer and tries to map it, if it fails, <code>memory:DMABuf</code> caps feature is negotiated, otherwise, normal video caps are used.</p>
<h2>VA-API usage</h2>
<p>First of all, GStreamer VA-API has support now for libva-2.0, this means VA-API 1.10. We had to guard some deprecated symbols and the new ones. Nowadays most of distributions have upgraded to libva-2.0.</p>
<p>We have improved the initialization of the VA display internal structure (<code>GstVaapiDisplay</code>). Previously, if a X based display was instantiated, immediately it tried to grab the screen resolution. Obviously, this broke the usage of headless systems. We just delay the screen resolution check to when the VA display truly requires that information.</p>
<p>New API were added into VA, particularly for log handling. Now it is possible to redirect the log messages into a callback. Thus, we use it to redirect VA-API message into the GStreamer log mechanisms, uncluttering the console’s output.</p>
<p>Also, we have blacklisted, in autoconf and meson, libva version 0.99.0, because that version is used internally by the closed-source version of <a href="http://mediasdk.intel.com/">Intel MediaSDK</a>, which is incompatible with official libva. By the way, there is a new open-source version of <a href="https://github.com/Intel-Media-SDK/MediaSDK/">MediaSDK</a>, but we will talk about it in a future blog post.</p>
<h2>Application VA Display sharing</h2>
<p>Normally, the object <code>GstVaapiDisplay</code> is shared among the pipeline through the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstContext.html">GstContext</a> mechanism. But this class is defined internally and it is not exposed to users since release 1.6. This posed a problem when an application wanted to create its own VA Display and share it with an embedded pipeline. The solution is a new context application message: <code>gst.vaapi.app.Display</code>, defined as a <code>GstStructure</code> with two fields: <code>va-display</code> with the application’s <code>vaDisplay</code>, and <code>x11-display</code> with the application’s <a href="https://tronche.com/gui/x/xlib/display/opening.html">X11 native display</a>. In the future, a Wayland’s native handler will be processed too. Please note that this context message is only processed by <code>vaapisink</code>.</p>
<p><a href="http://blogs.igalia.com/vjaquez/files/2018/03/vaapicontext.png"><img src="vaapicontext-580x326.png" alt="" class="alignnone size-medium wp-image-633" width="580" height="326" /></a></p>
<p>One precondition for this solution was the removal of the VA display cache mechanism, a lingered request from users, which, of course, we did.</p>
<h2>Interoperability with appsink and similar</h2>
<p>A hardware accelerated driver, as the Intel one, may have custom offsets and strides for specific memory regions. We use the <code>GstVideoMeta</code> to set this custom values. The problem comes when downstream does not handle this meta, for example, <code>appsink</code>. Then, the user expect the “normal” values for those variable, but in the case of GStreamer VA-API with a hardware based driver, when the user displays the frame, it is shown corrupted.</p>
<p>In order to fix this, we have to make a memory copy, from our custom VA-API images to an allocated system memory. Of course there is a big CPU penalty, but it is better than delivering wrong video frames. If the user wants a better performance, then they should seek for a different approach.</p>
<h2>Resurrection of GstGLUploadTextureMeta for EGL renders</h2>
<p>I know, <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-libs/html/gst-plugins-base-libs-gstvideometa.html#GstVideoGLTextureUploadMeta">GstGLUploadTextureMeta</a> must die, right? I am convinced of it. But, Clutter video sink uses it, an it has a vast number of users, so we still have to support it.</p>
<p>Last release we had remove the support for EGL/Wayland in the last minute because we found a terrible bug just before the release. GLX support has always been there.</p>
<p>With Daniel van Vugt efforts, we resurrected the support for that meta in EGL. Though I expect the replacement of Clutter sink with <code>glimagesink</code> someday, soon.</p>
<h2>vaapisink demoted in Wayland</h2>
<p><code>vaapisink</code> was demoted to marginal rank on Wayland because COGL cannot display YUV surfaces.</p>
<p>This means, by default, <code>vaapisink</code> won’t be auto-plugged when playing in Wayland.</p>
<p>The reason is because Mutter (aka GNOME) cannot display the frames processed by <code>vaapisink</code> in Wayland. Nonetheless, please note that in Weston, it works just fine.</p>
<h2>Decoders</h2>
<p>We have improved a little bit upstream renegotiation: if the new stream is compatible with the previous one, there is no need to reset the internal parser, with the exception of changes in codec-data.</p>
<h3>low-latency property in H.264</h3>
<p>A new property has added only to H.264 decoder: <code>low-latency</code>. Its purpose is for live streams that do not conform the H.264 specification (sadly there are many in the wild) and they need to twitch the spec implementation. This property force to push the frames in the <em>decoded picture buffer</em> as soon as possible.</p>
<h3>base-only property in H.264</h3>
<p>This is the result of the Google Summer of Code 2017, by Orestis Floros. When this property is enabled, all the MVC (Multiview Video Coding) or SVC (Scalable Video Coding) frames are dropped. This is useful if you want to reduce the processing time or if your VA-API driver does not support those kind of streams.</p>
<h2>Encoders</h2>
<p>In this release we have put a lot of effort in encoders.</p>
<h3>Processing Regions of Interest</h3>
<p>It is possible, for certain back-ends and profiles (for example, H.264 and H.265 encoders with Intel driver), to specify a set of <em>regions of interest</em> per frame, with a <code>delta-qp</code> per region. This mean that we would ask more quality in those regions.</p>
<p>In order to process regions of interest, upstream must add to the video frame, a list of <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-libs/html/gst-plugins-base-libs-gstvideometa.html#GstVideoRegionOfInterestMeta">GstVideoRegionOfInterestMeta</a>. This list then is traversed by the encoder and it requests them if the VA-API profile, in the driver, supports it.</p>
<p>The common use-case for this feature is if you want to higher definition in regions with faces or text messages in the picture, for example.</p>
<h3>New encoding properties</h3>
<ul>
<li><code>quality-level</code>: For all the available encoders. This is number   between 1 to 8, where a lower number means higher quality (and slower processing).</li></ul>

<li>
<p><code>aud</code>: This is for H.264 encoder only and it is available for certain drivers and platforms. When it is enabled, an AU delimiter is inserted for each encoded frame. This is useful for network streaming, and more particularly for Apple clients.</p>
</li>
<li>
<p><code>mbbrc</code>: For H.264 only. Controls (auto/on/off) the macro-block bit-rate.</p>
</li>
<li>
<p><code>temporal-levels</code>: For H.264 only. It specifies the number of temporal levels to include a the hierarchical frame prediction.</p>
</li>
<li>
<p><code>prediction-type</code>: For H.264 only. It selects the reference picture selection mode.</p>
<p>The frames are encoded as different layers. A frame in a particular layer will use pictures in lower or same layer as references. This means decoder can drop frames in upper layer but still decode lower layer frames.</p>
<ul>
<li>hierarchical-p: P frames, except in top layer, are reference frames. Base layer frames are I or B.</li></ul></li>

<li>
<p>hierarchical-b: B frames , except in top most layer, are reference frames. All the base layer frames are I or P.</p>
</li>


<li>
<p><code>refs</code>: Added for H.265 (it was already supported for H.264). It specifies the number of reference pictures.</p>
</li>
<li>
<p><code>qp-ip</code> and <code>qp-ib</code>: For H.264 and H.265 encoders. They handle the QP (quality parameters) difference between the I and P frames, the the I and B frames respectively.</p>
</li>

<h3>Set media profile via downstream caps</h3>
<p>H.264 and H.265 encoders now can configure the desired media profile through the downstream caps.</p>
<h2>Contributors</h2>
<p>Many thanks to all the contributors and bug reporters.</p>
<pre><code>     1  Daniel van Vugt
    46  Hyunjun Ko
     1  Jan Schmidt
     3  Julien Isorce
     1  Matt Staples
     2  Matteo Valdina
     2  Matthew Waters
     1  Michael Tretter
     4  Nicolas Dufresne
     9  Orestis Floros
     1  Philippe Normand
     4  Sebastian Dröge
    24  Sreerenj Balachandran
     1  Thibault Saunier
    13  Tim-Philipp Müller
     1  Tomas Rataj
     2  U. Artie Eoff
     1  VaL Doroshchuk
   172  Víctor Manuel Jáquez Leal
     3  XuGuangxin
     2  Yi A Wang
</code></pre></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Low-latency audio on Windows with GStreamer</span></a><div class="lastUpdated">2018年3月24日 14:09</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div dir="ltr">Digital audio is so ubiquitous that we rarely stop to think or wonder  how the gears turn underneath our all-pervasive apps for entertainment. Today we'll look at one specific piece of the machinery: latency.<br /><br />Let's say you're making a video of someone's birthday party with an app on  your phone. Once the recording starts, you don't care when the app starts writing it to disk<span class="st">—</span>as long as everything is there in the end.<br /><br />However, if you're having a Skype call with your friend, it matters a <i>whole lot</i>  how long it takes for the video to reach the other end and vice versa.  It's impossible to have a conversation if the lag (latency) is too  high.<br /><br />The difference is, do you need real-time feedback or not?<br /><br />Other examples, in order of increasingly stricter latency requirements are: live video streaming, security cameras, augmented reality games such as <a href="https://en.wikipedia.org/wiki/Pok%C3%A9mon_Go" target="_top">Pokémon Go</a>, multiplayer video games in general, audio effects apps for live music recording, and many many more.<br /><br />“But Nirbheek”, you might ask, “why doesn't everyone always ‘immediately’ send/store/show whatever is recorded? Why do people have to worry about latency?” and that's a great question!<br /><br />To understand that, checkout my previous blog post, <a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html" target="_top">Latency in Digital Audio</a>. It's also a good primer on analog vs digital audio!<br /><br /><h2>Low latency on consumer operating systems</h2><div><br /></div><div>Each operating system has its own set of application APIs for audio, and each has a lower bind on the achievable latency:</div><div><br /></div><ul><li>Linux has <a href="https://www.alsa-project.org/main/index.php/ALSA_Library_API" target="_top">alsa-lib</a> (old), <a href="https://en.wikipedia.org/wiki/Pulseaudio" target="_top">Pulseaudio</a> (standard), <a href="https://en.wikipedia.org/wiki/JACK_Audio_Connection_Kit" target="_top">JACK</a> (pro-audio), and <a href="https://pipewire.org/" target="_top">Pipewire</a> (<a href="https://blogs.gnome.org/uraeus/2018/01/26/an-update-on-pipewire-the-multimedia-revolution-an-update/" target="_top">under development</a>)</li><li>macOS and iOS have <a href="https://en.wikipedia.org/wiki/Core_Audio" target="_top">CoreAudio</a> (standard, pro-audio)</li><li>Android has <a href="https://source.android.com/devices/audio/" target="_top">AudioFlinger</a> (Java API, android.media), <a href="https://en.wikipedia.org/wiki/OpenSL_ES" target="_top">OpenSL ES</a> (C/C++ API), and <a href="https://source.android.com/devices/audio/aaudio" target="_top">AAudio</a> (C/C++ API, new, pro-audio)</li><li>Windows has <a href="https://en.wikipedia.org/wiki/Directsound" target="_top">DirectSound</a> (deprecated), <a href="https://en.wikipedia.org/wiki/Technical_features_new_to_Windows_Vista#Audio_stack_architecture" target="_top">WASAPI</a> (standard), and <a href="https://en.wikipedia.org/wiki/Audio_Stream_Input/Output" target="_top">ASIO</a> (proprietary, old, pro-audio).</li><li>BSDs still use <a href="https://en.wikipedia.org/wiki/Open_Sound_System">OSS</a></li></ul><div><br /></div><div>GStreamer already has plugins for almost all of these<a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#gst-plugins">¹</a> (plus others that aren't listed here), and on Windows, GStreamer has been using the DirectSound API by default for  audio capture and output since the very beginning.<br /><br />However, the DirectSound API was deprecated in Windows XP, and with Vista, it was removed and replaced with an emulation layer on top of the newly-released WASAPI. As a result, the plugin can't be configured to have less than 200ms of latency, which makes it unsuitable for all the low-latency use-cases mentioned above. The DirectSound API is quite crufty and unnecessarily complex anyway.<br /><br />GStreamer is rarely used in video games, but it is widely used for live streaming, audio/video calls, and other real-time applications. Worse, the WASAPI GStreamer plugins were effectively untouched and unused since the initial implementation in 2008 and were completely broken<a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#gst-windows">²</a>.<br /><br />This left no way to achieve low-latency audio capture or playback on Windows using GStreamer.<br /><br />The situation became particularly dire when GStreamer added a new <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html">implementation of the WebRTC spec</a> in this <a href="https://gstreamer.freedesktop.org/releases/1.14/">release cycle</a>. People that try it out on Windows were going to see much higher latencies than they should.<br /><br />Luckily, I rewrote most of the WASAPI plugin code in January and February, and it should now work well on all versions of Windows from Vista to 10! You can get <a href="https://gstreamer.freedesktop.org/data/pkg/windows/1.14.0.1/">binary installers for GStreamer</a> or <a href="https://gstreamer.freedesktop.org/documentation/installing/building-from-source-using-cerbero.html">build it from source</a>.<br /><br /><h2>Shared and Exclusive WASAPI</h2><br />WASAPI allows applications to open sound devices in two modes: <i>shared</i> and <i>exclusive</i>. As the name suggests, <i>shared</i> mode allows multiple applications to output to (or capture from) an audio device at the same time, whereas <i>exclusive</i> mode does not.<br /><br />Almost all applications should open audio devices in shared mode. It would be quite disastrous if your YouTube videos played without sound because Spotify decided to open your speakers in exclusive mode.<br /><br />In shared mode, the audio engine has to resample and mix audio streams from all the applications that want to output to that device. This increases latency because it must maintain its own audio ringbuffer for doing all this, from which audio buffers will be periodically written out to the audio device.<br /><br />In theory, hardware mixing could be used if the sound card supports it, but very few sound cards implement that now since it's so cheap to do in software. On Windows, only high-end audio interfaces used for professional audio implement this.<br /><br />Another option is to allocate your audio engine buffers directly in the sound card's memory with DMA, but that complicates the implementation and relies on good drivers from hardware manufacturers. Microsoft has tried similar approaches in the past with DirectSound and been burned by it, so it's not a route they took with WASAPI<a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#ms-audio-history">³</a>.<br /><br />On the other hand, some applications know they will be the only ones using a device, and for them all this machinery is a hindrance. This is why <i>exclusive</i> mode exists. In this mode, if the audio driver is implemented correctly, the application's buffers will be directly written out to the sound card, which will yield the lowest possible latency.<br /><br /><h2>Audio latency with WASAPI</h2><br />So what kind of latencies <i>can</i> we get with WASAPI?<br /><br />That depends on the <i>device period</i> that is being used. The term <i>device period</i> is a fancy way of saying <i>buffer size</i>; specifically the buffer size that is used in each call to your application that fetches audio data.<br /><br />This is the same period with which audio data will be written out to the actual device, so it is the major contributor of latency in the entire machinery.<i></i><br /><br />If you're using the <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/dd370865">AudioClient</a> interface in WASAPI to initialize your streams, the default period is 10ms. This means the theoretical <i>minimum</i> latency you can get in <i>shared mode</i> would be 10ms (audio engine) + 10ms (driver) = 20ms. In practice, it'll be somewhat higher due to various inefficiencies in the subsystem.<br /><br />When using <i>exclusive mode</i>, there's no engine latency, so the same number goes down to ~10ms.<br /><br />These numbers are decent for most use-cases, but like I explained in my <a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html">previous blog post</a>, this is totally insufficient for pro-audio use-cases such as applying live effects to music recordings. You really need latencies that are lower than 10ms there.<br /><br /><h2>Ultra-low latency with WASAPI</h2><br />Starting with Windows 10, WASAPI removed most of its  aforementioned inefficiencies, and introduced a new interface: <a href="https://msdn.microsoft.com/library/windows/desktop/dn911487">AudioClient3</a>. If you initialize your streams with this interface, and if your audio driver is implemented correctly, you can configure a device period of just <i>2.67ms</i> at 48KHz.<br /><br />The best part is that this is the period not just in exclusive mode but <i>also in shared mode</i>, which brings WASAPI almost at-par with JACK and CoreAudio  <br /><br />So that was the good news. Did I mention there's bad news too? Well, now you know.<br /><br />The first bit is that these numbers are only achievable if you use Microsoft's implementation of the Intel HD Audio standard for consumer drivers. This is fine; you follow <a href="https://blogs.msdn.microsoft.com/matthew_van_eerde/2010/08/23/troubleshooting-how-to-install-the-microsoft-hd-audio-class-driver/">some badly-documented steps</a> and it turns out fine.<br /><br />Then you realize that if you want to use something more high-end than an Intel HD Audio sound card, unless you use <a href="http://www.motu.com/newsitems/windows-wave-rt-support-is-now-shipping">one of the rare</a> pro-audio interfaces that have drivers that use the new <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/audio/understanding-the-wavert-port-driver">WaveRT</a> driver model instead of the old <a href="https://msdn.microsoft.com/en-us/library/windows/hardware/ff538767">WaveCyclic</a> model, you still see 10ms device periods.<br /><br />It seems the pro-audio industry made the decision to stick with ASIO since it already provides &lt;5ms latency. They don't care that the API is proprietary, and that most applications can't actually use it because of that. All the apps that are used in the pro-audio world already work with it.<br /><br />The strange part is that all this information is nowhere on the Internet and seems to lie solely in the minds of the Windows audio driver cabals across the US and Europe. It's surprising and frustrating for someone used to working in the open to see such counterproductive information asymmetry, and <a href="https://github.com/kinetiknz/cubeb/issues/324">I'm not the only one</a>.<br /><br />This is where I plug open-source and talk about how Linux has had ultra-low latencies for years since all the audio drivers are open-source, follow the same <a href="https://www.kernel.org/doc/html/v4.10/sound/kernel-api/index.html">ALSA driver model</a><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#alsa-kernel">⁴</a>, and are constantly improved. JACK is probably the most well-known low-latency audio engine in existence, and was born on Linux. People are even using Pulseaudio these days to work with &lt;5ms latencies.<br /><br />But this blog post is about Windows and WASAPI, so let's get back on track.<br /><br />To be fair, Microsoft is not to blame here. Decades ago they made the decision of not working more closely with the companies that write drivers for their standard hardware components, and they're still paying the price for it. Blue screens of death were the most user-visible consequences, but the current audio situation is an indication that losing control of your platform has more dire consequences.<br /><br />There is one more bit of bad news. In my testing, I wasn't able to get glitch-free <i>capture</i> of audio in the source element using the AudioClient3 interface at the minimum configurable latency in shared mode, even with <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/sys/wasapi/gstwasapiutil.c#n980">critical thread priorities</a> unless there was nothing else running on the machine.<br /><br />As a result, this feature is disabled by default on the source element. This is unfortunate, but not a great loss since the same device period is achievable in exclusive mode without glitches.<br /><br /><h2>Measuring WASAPI latencies</h2><br />Now that we're back from our detour, the executive summary is that the GStreamer WASAPI <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-wasapisrc.html#gst-plugins-bad-plugins-wasapisrc.description">source</a> and <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-wasapisink.html#gst-plugins-bad-plugins-wasapisink.description">sink</a> elements now use the latest recommended WASAPI interfaces. You should test them out and see how well they work for you!<br /><br />By default, a device is opened in shared mode with a conservative latency setting. To force the stream into the lowest latency possible, set <i>low-latency=true</i>. If you're on Windows 10 and want to force-enable/disable the use of the AudioClient3 interface, toggle the <i>use-audioclient3</i> property.<br /><br />To open a device in exclusive mode, set <i>exclusive=true</i>. This will ignore the <i>low-latency</i> and <i>use-audioclient3</i> properties since they only apply to shared mode streams. When a device is opened in exclusive mode, the stream will always be configured for the lowest possible latency by WASAPI.<br /><br />To measure the actual latency in each configuration, you can use the new <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-audiolatency.html#gst-plugins-bad-plugins-audiolatency.description">audiolatency</a> plugin that I wrote to get hard numbers for the total end-to-end latency including the latency added by the GStreamer audio ringbuffers in the source and sink elements, the WASAPI audio engine (capture and render), the audio driver, and so on.<br /><br />I look forward to hearing what your numbers are on Windows 7, 8.1, and 10 in all these configurations! ;)<br /><br /><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="gst-plugins"></a><br /><span>1. The only ones missing are AAudio because it's very new and ASIO which is a proprietary API with licensing requirements.</span><br /><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="gst-windows"></a><br /><span>2. It's no secret that although lots of people use GStreamer on Windows, the majority of GStreamer developers work on Linux and macOS. As a result the Windows plugins haven't always gotten a lot of love. It doesn't help that <a href="https://gstreamer.freedesktop.org/documentation/installing/building-from-source-using-cerbero.html">building GStreamer on Windows</a> can be a daunting task . This is actually one of the major reasons why we're moving to Meson, but I've already <a href="http://blog.nirbheek.in/2016/05/gstreamer-and-meson-new-hope.html">written about that elsewhere</a>!</span><br /><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="ms-audio-history"></a><br /><span>3. My knowledge about the history of the decisions behind the Windows Audio API is spotty, so corrections and expansions on this are most welcome!</span><br /><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="alsa-kernel"></a><br /><span>4. The ALSA drivers in the Linux kernel should not be confused with the ALSA userspace library.</span></div></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://coaxion.net/blog/2018/03/gstreamer-rust-bindings-0-11-plugin-writing-infrastructure-0-2-release/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer Rust bindings 0.11 / plugin writing infrastructure 0.2 release</span></a><div class="lastUpdated">2018年3月20日 19:42</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Following the <a href="https://gstreamer.freedesktop.org/releases/1.14/" rel="noopener" target="_top">GStreamer 1.14 release</a> and the new <a href="http://gtk-rs.org/blog/2018/03/17/new-release.html" rel="noopener" target="_top">round of gtk-rs releases</a>, there are also new releases for the <a href="https://github.com/sdroege/gstreamer-rs" rel="noopener" target="_top">GStreamer Rust bindings</a> (0.11) and the <a href="https://github.com/sdroege/gst-plugin-rs/" rel="noopener" target="_top">plugin writing infrastructure</a> (0.2).</p>
<p>Thanks also to all the contributors for making these releases happen and adding lots of valuable changes and API additions.</p>
<h4>GStreamer Rust Bindings</h4>
<p>The main changes in the Rust bindings were the update to GStreamer 1.14 (which brings in quite some new API, like <i>GstPromise</i>), a couple of API additions (<i>GstBufferPool</i> specifically) and the addition of the <i>GstRtspServer</i> and <i>GstPbutils</i> crates. The former allows writing a full <a href="https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol" rel="noopener" target="_top">RTSP</a> server in a <a href="https://github.com/sdroege/gstreamer-rs/blob/0.11/examples/src/bin/rtsp-server.rs" rel="noopener" target="_top">couple of lines of code</a> (with lots of potential for customizations), the latter provides access to the <i>GstDiscoverer</i> helper object that allows inspecting files and streams for their container format, codecs, tags and all kinds of other metadata.</p>
<p>The <i>GstPbutils</i> crate will also get other features added in the near future, like <a href="https://github.com/sdroege/gstreamer-rs/pull/96" rel="noopener" target="_top">encoding profile bindings</a> to allow using the <i>encodebin</i> GStreamer element (a helper element for automatically selecting/configuring encoders and muxers) from Rust.</p>
<p>But the biggest changes in my opinion is some refactoring that was done to the <i>Event</i>, <i>Message</i> and <i>Query</i> APIs. Previously you would have to use a view on a newly created query to be able to use the type-specific functions on it</p>
<p></p><pre class="crayon-plain-tag">let mut q = gst::Query::new_position(gst::Format::Time);
if pipeline.query(q.get_mut().unwrap()) {
    match q.view() {
        QueryView::Position(ref p) =&gt; Some(p.get_result()),
        _ =&gt; None,
    }
} else {
    None
}</pre><p> </p>
<p>Now you can directly use the type-specific functions on a newly created query</p><pre class="crayon-plain-tag">let mut q = gst::Query::new_position(gst::Format::Time);
if pipeline.query(&amp;mut q) {
    Some(q.get_result())
} else {
    None
}</pre><p> </p>
<p>In addition, the views can now dereference directly to the event/message/query itself and provide access to their API, which simplifies some code even more.</p>
<h4>Plugin Writing Infrastructure</h4>
<p>While the plugin writing infrastructure did not see that many changes apart from a couple of bugfixes and updating to the new versions of everything else, this does not mean that development on it stalled. Quite the opposite. The existing code works very well already and there was just no need for adding anything new for the projects I and others did on top of it, most of the required API additions were in the GStreamer bindings.</p>
<p>So the status here is the same as last time, get started writing GStreamer plugins in Rust. It works well!</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-03-19T20:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.14.0 new major stable release</span></a><div class="lastUpdated">2018年3月20日 4:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is proud to announce a new major feature release of
your favourite cross-platform multimedia framework!
</p><p>
The 1.14 release series adds new features on top of the previous 1.12 series
and is part of the API and ABI-stable 1.x release series of the GStreamer
multimedia framework.
</p><p>
  <b>Highlights:</b>
  </p><ul>
  <li>WebRTC support: real-time audio/video streaming to and from web browsers</li>
  <li>Experimental support for the next-gen royalty-free AV1 video codec</li>
  <li>Video4Linux: encoding support, stable element names and faster device probing</li>
  <li>Support for the Secure Reliable Transport (SRT) video streaming protocol</li>
  <li>RTP Forward Error Correction (FEC) support (ULPFEC)</li>
  <li>RTSP 2.0 support in <tt>rtspsrc</tt> and gst-rtsp-server</li>
  <li>ONVIF audio backchannel support in gst-rtsp-server and <tt>rtspsrc</tt></li>
  <li><tt>playbin3</tt> gapless playback and pre-buffering support</li>
  <li><tt>tee</tt>, our stream splitter/duplication element, now does allocation query
     aggregation which is important for efficient data handling and zero-copy</li>
  <li>QuickTime muxer has a new prefill recording mode that allows file import
      in Adobe Premiere and FinalCut Pro while the file is still being written.</li>
  <li><tt>rtpjitterbuffer</tt> fast-start mode and timestamp offset adjustment smoothing</li>
  <li><tt>souphttpsrc</tt> connection sharing, which allows for connection reuse, cookie sharing, etc.</li>
  <li><tt>nvdec</tt>: new plugin for hardware-accelerated video decoding using the NVIDIA NVDEC API</li>
  <li>Adaptive DASH trick play support</li>
  <li><tt>ipcpipeline</tt>: new plugin that allows splitting a pipeline across
      multiple processes</li>
  <li>Major <tt>gobject-introspection</tt> annotation improvements for large parts of the library API</li>
  <li>GStreamer C# bindings have been revived and seen many updates and fixes</li>
  <li>The externally-maintained GStreamer Rust bindings have many
      usability improvements and cover most of the API now</li>
  </ul>
<p></p><p>
Full release notes can be found <a href="https://gstreamer.freedesktop.org/releases/1.14/">here</a>.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided in the next days.
</p><p>
You can download release tarballs directly here: 
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.14.0.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.14.0.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.14.0.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.0.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.14.0.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.0.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.0.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.14.0.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.14.0.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.14.0.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.14.0.tar.xz">gstreamer-vaapi</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.14.0.tar.xz">gstreamer-sharp</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.14.0.tar.xz">gst-omx</a>.
        </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://base-art.net/Articles/gstreamers-playbin3-overview-for-application-developers/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer’s playbin3 overview for application developers</span></a><div class="lastUpdated">2018年3月19日 15:13</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Multimedia applications based on GStreamer usually handle playback with the
<a class="reference external" href="https://gstreamer.freedesktop.org/documentation/tutorials/playback/playbin-usage.html">playbin</a> element. I recently added support for <a class="reference external" href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-plugins/html/gst-plugins-base-plugins-playbin3.html">playbin3</a> in WebKit. This post
aims to document the changes needed on application side to support this new
generation flavour of playbin.</p>
<p>So, first of, why is it named playbin3 anyway? The GStreamer …</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://base-art.net/Articles/moving-to-pelican/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Moving to Pelican</span></a><div class="lastUpdated">2018年3月18日 17:18</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Time for a change! Almost 10 years ago I was starting to hack on a
Blog engine with two friends, it was called <a class="reference external" href="http://github.com/philn/alinea">Alinea</a> and it powered
this website for a long time. Back then hacking on your own Blog
engine was the pre-requirement to host your blog :) But nowadays …</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://base-art.net/Articles/the-gnome-shell-gajim-extension-maintenance/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">The GNOME-Shell Gajim extension maintenance</span></a><div class="lastUpdated">2018年3月18日 17:18</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Back in January 2011 I wrote a <span class="caps">GNOME</span>-Shell <a class="reference external" href="https://extensions.gnome.org/extension/565/gajim-im-integration/">extension</a> allowing Gajim users to
carry on with their chats using the Empathy infrastructure and <span class="caps">UI</span> present in the
Shell. For some time the extension was also part of the official
gnome-shell-extensions module and then I had to move it to …</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://base-art.net/Articles/web-engines-hackfest-2014/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Web Engines Hackfest 2014</span></a><div class="lastUpdated">2018年3月18日 17:18</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>Last week I attended the <a class="reference external" href="http://www.webengineshackfest.org/">Web Engines Hackfest</a>. The
event was sponsored by Igalia (also hosting the event), Adobe and Collabora.</p>
<p>As usual I spent most of the time working on the WebKitGTK+ GStreamer
backend and <a class="reference external" href="https://coaxion.net/">Sebastian Dröge</a> kindly joined and helped out quite a
bit, make sure to read …</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-03-12T23:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.13.91 pre-release (1.14 rc2)</span></a><div class="lastUpdated">2018年3月13日 7:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce the second and hopefully last
release candidate for the upcoming stable 1.14 release series.
</p><p>
The 1.14 release series adds new features on top of the current stable 1.12
series and is part of the API and ABI-stable 1.x release series of the
GStreamer multimedia framework.
</p><p>
The 1.13.91 pre-release is for testing and development purposes in the lead-up
to the stable 1.14 series which is now feature frozen and scheduled for release
soon.
</p><p>
Full release notes can be found on the <a href="https://gstreamer.freedesktop.org/releases/1.14/">1.14 release notes page</a>,
highlighting all the new features, bugfixes, performance optimizations and
other important changes.
</p><p>
Packagers: please note that quite a few plugins and libraries have moved
between modules, so please take extra care and make sure inter-module
version dependencies are such that users can only upgrade all modules in
one go, instead of seeing a mix of 1.13 and 1.12 on their system.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided shortly.
</p><p>
Release tarballs can be downloaded directly here:
</p><ul>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.13.91.tar.xz">gstreamer-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.13.91.tar.xz">gst-plugins-base-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.13.91.tar.xz">gst-plugins-good-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.13.91.tar.xz">gst-plugins-ugly-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.13.91.tar.xz">gst-plugins-bad-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.13.91.tar.xz">gst-libav-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.13.91.tar.xz">gst-rtsp-server-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.13.91.tar.xz">gst-python-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.13.91.tar.xz">gst-editing-services-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.13.91.tar.xz">gst-validate-1.13.91.tar.xz</a>,</li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.13.91.tar.xz">gstreamer-vaapi-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.13.91.tar.xz">gst-omx-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.13.91.tar.xz">gstreamer-sharp-1.13.91.tar.xz</a></li>
</ul>
        <p></p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-03-03T23:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.13.90 pre-release (1.14 rc1)</span></a><div class="lastUpdated">2018年3月4日 7:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce the first release candidate for the
upcoming stable 1.14 release series.
</p><p>
The 1.14 release series adds new features on top of the current stable 1.12
series and is part of the API and ABI-stable 1.x release series of the
GStreamer multimedia framework.
</p><p>
The 1.13.90 pre-release is for testing and development purposes in the lead-up
to the stable 1.14 series which is now feature frozen and scheduled for release
soon.
</p><p>
Full release notes will be provided in the near future, highlighting all the
new features, bugfixes, performance optimizations and other important changes.
</p><p>
Packagers: please note that quite a few plugins and libraries have moved
between modules, so please take extra care and make sure inter-module
version dependencies are such that users can only upgrade all modules in
one go, instead of seeing a mix of 1.13 and 1.12 on their system.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided shortly.
</p><p>
Release tarballs can be downloaded directly here:
</p><ul>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.13.90.tar.xz">gstreamer-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.13.90.tar.xz">gst-plugins-base-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.13.90.tar.xz">gst-plugins-good-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.13.90.tar.xz">gst-plugins-ugly-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.13.90.tar.xz">gst-plugins-bad-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.13.90.tar.xz">gst-libav-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.13.90.tar.xz">gst-rtsp-server-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.13.90.tar.xz">gst-python-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.13.90.tar.xz">gst-editing-services-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.13.90.tar.xz">gst-validate-1.13.90.tar.xz</a>,</li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.13.90.tar.xz">gstreamer-vaapi-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.13.90.tar.xz">gst-omx-1.13.90.tar.xz</a></li>
</ul>
        <p></p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://blog.nirbheek.in/2018/02/decoupling-gstreamer-pipelines.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Decoupling GStreamer Pipelines</span></a><div class="lastUpdated">2018年2月27日 17:57</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div dir="ltr"><span><i>This post is best read with some prior familiarity with <a href="https://gstreamer.freedesktop.org/" target="_top">GStreamer</a> pipelines. If you want to learn more about that, a good place to start is the <a href="https://twitter.com/thaytan/status/956366764543111169" target="_top">tutorial Jan presented at LCA 2018</a>.</i></span><br /><br /><h2>Elevator Pitch</h2><br />GStreamer was designed with modularity, pluggability, and ease of use in mind, and the structure was somewhat inspired by UNIX pipes. With GStreamer, you start with an idea of what your dataflow will look like, and the pipeline  will map that quite closely.<br /><br />This is true whether you're working with a simple and static pipeline:<br /><br /><code>source ! transform ! sink</code><br /><br />Or if you need  complex and dynamic pipelines with varying rates of data flow:<br /><br /><div class="separator"><a href="https://i.imgur.com/GJC4y2y.png"><img src="foo.png" width="400" height="37" border="0" /></a></div><br />The inherent pluggability of the system allows for quick prototyping and makes a lot of changes simpler than they would be in other systems.<br /><br />At the same time, to achieve efficient multimedia processing, one must avoid onerous copying of data, excessive threading, or additional latency. Other features necessary are  varying rates of playback, seeking, branching, mixing, non-linear data flow, timing, and much more, but let's keep it simple for now.<br /><br /><h2>Modular Multimedia Processing</h2><br />A naive way to implement this would be to have one thread (or process) for each node, and use shared memory or message-passing. This can achieve high throughput if you use the right APIs for zerocopy message-passing, but because of a lack of realtime guarantees on all consumer operating systems, the latency will be jittery and much harder to achieve.<br /><br />So how does GStreamer solve these problems?<br /><br />Let's take a look at a simple pipeline to try and understand. We generate a sine wave, encode it with <a href="https://opus-codec.org/" target="_top">Opus</a>, mux it into an Ogg container, and write it to disk.<br /><br /><code><br />$ gst-launch-1.0 -e audiotestsrc ! opusenc ! oggmux ! filesink location=out.ogg<br /></code><br /><br />How does data make it from one end of this pipeline to the other in GStreamer? The answer lies in <i>source pads</i>, <i>sink pads</i> and the <a href="https://gstreamer.freedesktop.org/documentation/plugin-development/basics/chainfn.html" target="_top">chain function</a>.<br /><br />In this pipeline, the <code>audiotestsrc</code> element has one <i>source pad</i>. <code>opusenc</code> and <code>oggmux</code> have one <i>source pad</i> and one <i>sink pad</i> each, and <code>filesink</code> only has a <i>sink pad</i>. Buffers always move from source pads to sink pads. All elements that receive buffers (with sink pads) must implement a <i>chain function</i> to handle each buffer.<br /><br />Zooming in a bit more, to output buffers, an element will call <code>gst_pad_push()</code> on its <i>source pad</i>. This function will figure out what the corresponding <i>sink pad</i> is, and call the chain function of that element with a pointer to the buffer that was pushed earlier. This chain function can then apply a transformation to the buffer and push it (or a new buffer) onward with <code>gst_pad_push()</code> again.<br /><br />The net effect of this is that all buffer handling from one end of this pipeline to the other happens <b>in one series of chained  function calls</b>. This is a really important detail that allows GStreamer to be efficient by default.<br /><br /><h2>Pipeline Multithreading</h2><br />Of course, sometimes you <i>want</i> to decouple parts of the pipeline, and that brings us to the simplest mechanism for doing so: the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-plugins/html/gstreamer-plugins-queue.html#gstreamer-plugins-queue.description" target="_top"><code>queue</code> element</a>. The most basic use-case for this element is to ensure that the downstream of your pipeline <a href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/multithreading-and-pad-availability.html" target="_top">runs in a new thread</a>.<br /><br />In some applications, you want even greater decoupling of parts of your pipeline. For instance, if you're reading data from the network, you don't want a network error to bring down our entire pipeline, or if you're working with a hotpluggable device, device removal should be recoverable without needing to restart the pipeline.<br /><br />There are various  mechanisms to achieve such decoupling: <a href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/short-cutting-the-pipeline.html" target="_top"><code>appsrc</code>/<code>appsink</code></a>, <code>fdsrc</code>/<code>fdsink</code>, <code>shmsrc</code>/<code>shmsink</code>, <a href="https://www.collabora.com/news-and-blog/blog/2017/11/17/ipcpipeline-splitting-a-gstreamer-pipeline-into-multiple-processes/" target="_top"><code>ipcpipeline</code></a>, etc.  However, each of those have their own limitations and complexities. In particular, <a href="https://gstreamer.freedesktop.org/documentation/application-development/basics/data.html#events" target="_top">events</a>, <a href="https://gstreamer.freedesktop.org/documentation/design/negotiation.html" target="_top">negotiation</a>, and <a href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/time-management.html" target="_top">synchronization</a> usually need to be handled or serialized manually at the boundary.<br /><br /><h2>Seamless Pipeline Decoupling</h2><br />We recently merged a new plugin that makes this job much simpler: <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/commit/?id=3f7e29d5b32f20dff75a58186533e40bb0ed4081" target="_top">gstproxy</a>. Essentially, you insert a <code>proxysink</code> element when you want to send data outside your pipeline, and use a <code>proxysrc</code> element to push that data into a different pipeline in the same process.<br /><br />The interesting thing about this plugin is that <i>everything</i> is proxied, not just buffers. Events, queries, and hence caps negotiation all happen seamlessly. This is particularly useful when you want to do dynamic reconfiguration of your pipeline, and want the decoupled parts to reconfigure automatically.<br /><br />Say you have a pipeline like this:<br /><br /><code><br />pulsesrc ! opusenc ! oggmux ! souphttpclientsink<br /></code><br /><br />Where the <code>souphttpclientsink</code> element is doing a <code>PUT</code> to a remote HTTP server. If the server suddenly closes the connection, you want to be able to immediately reconnect to the same server or a different one without interrupting the recording. One way to do this, would be to use <code>appsrc</code> and <code>appsink</code> to split it into two pipelines:<br /><br /><code><br />pulsesrc ! opusenc ! oggmux ! appsink<br /><br />appsrc ! souphttpclientsink<br /></code><br /><br />Now you need to write code to handle buffers that are received on the <code>appsink</code> and then manually push those into <code>appsrc</code>. With the <code>proxy</code> plugin, you split your pipeline like before:<br /><br /><code><br />pulsesrc ! opusenc ! oggmux ! proxysink<br /><br />proxysrc ! souphttpclientsink<br /></code><br /><br />Next, we connect the <code>proxysrc</code> and <code>proxysink</code> elements, and gstreamer will automatically push buffers from the first pipeline to the second one.<br /><br /><code>g_object_set (psrc, "proxysink", psink, NULL);</code><br /><br /><code>proxysink</code> also contains a <code>queue</code>, so the second pipeline will always run in a separate thread.<br /><br />Another option is the <a href="https://gstreamer.freedesktop.org/documentation/plugins.html"><code>inter</code> plugin</a>. If you use a pair of <code>interaudiosink/interaudiosrc</code> elements, buffers will be automatically moved between pipelines, but those only support raw audio or video, and drop events and queries at the boundary. The <code>proxy</code> elements push pointers to buffers without copying, and they do not care what the contents of the buffers are.<br /><br />This example was a trivial one, but with more complex pipelines, you usually have bins that automatically reconfigure themselves according to the events and caps sent by upstream elements; f.ex <code>decodebin</code> and <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html"><code>webrtcbin</code></a>. This metadata about the buffers is lost when using <code>appsrc</code>/<code>appsink</code>, and similar elements, but is transparently proxied by the <code>proxy</code> elements.<br /><br />The <code>ipcpipeline</code> elements also forward buffers, events, queries, etc (not zerocopy, but could be), but they are much more complicated since they were built for splitting pipelines across multiple processes, and are most often used in a security-sensitive context.<br /><br />The <code>proxy</code> elements only work when all the split pipelines are within the same process, are much simpler and as a result, more efficient. They should be used when you want graceful recovery from element errors, and your elements are not a vector for security attacks.<br /><br />For more details on how to use them, checkout the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/gst/proxy/gstproxysrc.c?id=HEAD#n22" target="_top">documentation and example</a>! The online docs will be generated from that when we're closer to the release of GStreamer 1.14. There are a few caveats, but a number of projects are already using it with great success.</div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://fortintam.com/blog/2018/02/25/journey-towards-a-reliable-linux-workstation/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">The Longest Debugging—The journey towards a reliable Linux workstation</span></a><div class="lastUpdated">2018年2月26日 0:17</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>I have this curse where I keep finding heisenbugs not only in the software I use, but also in hardware… The difference being that, unlike software misbehavior, hardware issues take me <em>months</em> to figure out. But hey, they say I’m a persistent bastard.</p>
<p><img class="wp-image-3636 aligncenter" src="hardware-debugging-meme-500x392.jpg" alt="" width="300" height="235" /></p>
<p>This blog post is mainly a tale about computer hardware, which is a bit unusual on this blog, but it ties back into software and Linux graphics troubleshooting, so there should be something interesting for everybody. Enjoy a solid 20 minutes of reading my walk through the inferno, hopefully you’ll find something insightful or funny in my unreasonably persistent path to hardware salvation. Anyway, the reading time doesn’t seem that long when it took me <em>10 months</em> to write this post over 37 revisions <img src="1f609.png" alt="😉" class="wp-smiley" /><span id="more-3558"></span></p>
<p>This divine comedy is divided in a série of chants:</p>
<ol>
<li><em>“Prologue: Beware of Hardware”</em> (two short case studies of strange hardware bugs from years past)</li>
<li><em>“I once thought 4GB of RAM ought to be enough for anybody”</em> (a tale of modern web browsing, and of operating system kernels gone rogue)</li>
<li><em>“Overwhelming power: the new Kusanagi”</em> (this is where today’s hardware story begins)</li>
<li><em>“The quest for stability begins”</em> (my descent into Hell)</li>
<li><em>“Finding the culprit”</em> (meeting with Virgilio)</li>
<li><em>“Reworking my workstation”</em> (salvation)</li>
<li>Epilogue</li>
</ol>
<h1>Prologue: Beware of Hardware</h1>
<p><img class="alignnone size-full wp-image-3796" src="tears_of_steel_frame_01_2a.jpg" alt="" width="1280" height="533" /></p>
<p>A bit over ten years ago (jeez, we’re old) <a href="https://www.linkedin.com/in/jeffschroeder/" target="_top" rel="noopener">herr Schroeder</a> gave me this advice: <em>“Never skimp on RAM or power supplies, because troubleshooting issues that involve these is so damned hard”</em> (I’m loosely quoting from memory). Sage advice, but somehow all the hardware heisenbugs I encountered so far turned out not to be in those particular components. For example:</p>
<ul>
<li><small>Some years ago, I spent months trying to figure out why my aunt’s brand new computer running Centos would experience random kernel panics. Whenever she would bring the computer to my office I would be unable to reproduce the issue no matter how hard I tried to “break it”, and when it went back home the issue would start reappearing again (I saw it with my own eyes). So I tested for peripherals combinations, software combinations, network combinations, tried torture-testing the thing with various benchmarks (including repeatedly opening and closing hundreds of tabs at once, heating up the system)… until I realized that the key to the problem was the fact that the computer was being physically transported across the two locations. Eventually I found the cause: a faulty SATA cable connected to the SSD!</small></li>
<li><small>Last year, it took me months (again) to figure out why my father’s desktop computer had started randomly freezing (the applications on the screen, and the mouse cursor, would just lock up out of the blue). Here again I did stress tests, tried systematically reverting and bisecting recent software updates to figure out what had “broken” the system—after all, the computer hadn’t moved at all, it could only be due to some software regression, right? Until I bothered to open the case and found <a href="https://en.wikipedia.org/wiki/Capacitor_plague" target="_top" rel="noopener">capacitor plague</a>.</small></li>
</ul>
<a href="https://fortintam.com/blog/wp-content/uploads/2016-05-30-capacitor-plague.jpg"><img class="wp-image-3622 size-large" src="2016-05-30-capacitor-plague-1024x539.jpg" alt="" width="525" height="276" /></a>Capacitor plague on my parents’ computer. I went to an electronics hardware store, bought three or four capacitors for a few cents each and replaced the blown capacitors on the motherboard. Since then, the computer is running like a champ.
<p>Well, today’s blog post is another story about taking months to find the solution to a problem—except this time it was worse. Much, much worse. Grab yourself a beer in the fridge.</p>
<hr />
<h1>I once thought, <em>“4 GB of RAM ought to be enough for anybody”</em></h1>
<p>These days, if you’re a marketeer or hardcore project manager, it seems 5-8 gigabytes of RAM is no longer enough to do anything serious in parallel to web browsing on a GNU+Linux system. See also:</p>
<ul>
<li><a href="http://idlewords.com/talks/website_obesity.htm" target="_top" rel="noopener">the website obesity crisis</a>,</li>
<li>the <a href="https://josephg.com/blog/electron-is-flash-for-the-desktop/" target="_top" rel="noopener">disease of web apps being used as replacements for native desktop applications</a>,</li>
<li>my <a href="https://fortintam.com/blog/2010/09/19/free-my-memory/">seven-years-old rant on Firefox</a>—which, given that Firefox is more “globally memory-efficient” than Chrome/Chromium these days, is in big part being fixed this year as we now have <a href="https://blog.mozilla.org/addons/2016/04/11/the-why-of-electrolysis/" target="_top" rel="noopener">Electrolysis</a> and <a href="https://metafluff.com/2017/07/21/i-am-a-tab-hoarder/" target="_top" rel="noopener">Quantum Flow startup times</a> (with Firefox 55, my startup time went from 1 minute and 13 seconds to… 5.8 seconds).</li>
</ul>
<p>Since I have a lot of tabs open in parallel to my research or graphics work, I tend to run into <a href="https://en.wikipedia.org/wiki/Out_of_memory">OoM</a> conditions <em>all the time</em>, no matter which web browser I use. Thanks, web 2.0!</p>
<img class="size-full wp-image-931" src="metal-gear-rex.jpg" alt="" width="600" height="427" />Pictured: a modern web browser.
<p>The issue wouldn’t be so bad if the Linux kernel actually did its job, but it doesn’t: when your RAM is full, the Linux kernel will just start trashing your hard disk for no good reason, and <em>if you’re lucky</em> after 30 minutes it <em>might</em> figure out “Oh, that web browser process thing that grew at hundreds of megabytes per minute or tried to allocate 20 times the amount of available RAM, maybe that’s the one I should kill instantly?”… Or, to put things succintly:</p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en"><a href="https://twitter.com/mairin?ref_src=twsrc%5Etfw">@mairin</a> I wish the OOM killer ignored small fry and chased down the real outlaws. Like that Firefox bastard that terrorizes the town.</p>
<p>— Jeff Fortin Tam (@nekohayo) <a href="https://twitter.com/nekohayo/status/460975304157048834?ref_src=twsrc%5Etfw">April 29, 2014</a></p></blockquote>
<p></p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en">Number of times I've seen OOM handled correctly ⇒ 0</p>
<p>— Christian Hergert (@hergertme) <a href="https://twitter.com/hergertme/status/704607950829068288?ref_src=twsrc%5Etfw">March 1, 2016</a></p></blockquote>
<p></p>
<p>This is in addition to having our desktop environments slow to a crawl whenever there is I/O (hard disk) trashing going on:</p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en">: My computer hardlocks due to IO craziness once or twice a week &amp; I lose data every week or two (when forcefully rebooting) too. <img src="1f622.png" alt="😢" class="wp-smiley" /></p>
<p>— Garrett LeSage (@garrett) <a href="https://twitter.com/garrett/status/740577649668546560?ref_src=twsrc%5Etfw">June 8, 2016</a></p></blockquote>
<p></p>
<p>I suspect there is only a handful of us crazies (“power users”) acutely aware of these issues. Not everybody experiences or notices this, but just like video tearing in X.org or the general <a href="https://bugzilla.gnome.org/show_bug.cgi?id=642652#c178">framerate dropping over time</a> in GNOME Shell, it’s the kind of thing where “once you become aware of its existence, you cannot un-see it”.</p>
<p>Overall, the Linux kernel is still pretty subpar (I’m being polite here) for “workstation” desktop usecases. It’s so, <em>so incredibly bad</em> at handling memory and I/O. Throughout the years, it has been the #1 component of my system that regularly made me want to throw furniture across the room as I hit “out of memory” conditions that destroyed my productivity. <em>All I wanted was to do design+photography work in parallel to tons of online research!</em></p>
<hr />
<h1>Overwhelming power:<br />
the new Kusanagi</h1>
<p>Hence, after a decade of waiting for the Linux kernel to get its act together, I gave up and decided to nuke the problem from orbit by replacing my “perfectly good” (but maxed out) computer—<em>Kusanagi—</em>by a workstation so stupidly powerful that I would not even be affected by the kernel’s lackluster resources management anymore.</p>
<p>Since this is GNU+Linux we’re talking about, I would just have to transplant its cyberbrain (hard drives and SSDs, containing the “ghost”) into the new “shell”—a trivial operation, no need to reinstall the operating system or anything.</p>
<p><img class="alignnone size-full wp-image-3799" src="batou-connecting-the-tank-and-kusanagi.jpg" alt="" width="1280" height="688" /></p>
<p>So, at the end of 2015, I bought this completely overpowered 2nd-hand cyborg shell, for about 370 Canadian yen:</p>
<p><img class="alignnone size-full wp-image-3800" src="newborn-kusanagi.jpg" alt="" width="1280" height="688" /></p>
<p>Er, I mean:</p>
<p><img class="alignnone size-full wp-image-3625" src="dell_precision-t3500.jpg" alt="" width="1024" height="768" /></p>
<p>…a second-hand Dell “Precision T3500” workstation maxed out with <em>24 gigabytes</em> of RAM and an 8-cores Xeon processor, lazily named “Kusanagi 2.0”<em>.</em> While it was made in 2009, it was a <a href="https://en.wikipedia.org/wiki/Workstation" target="_top" rel="noopener">workstation</a>-class machine, and remains completely overkill even today, in <del>2016</del> <del>2017</del> 2018.<em><br />
</em></p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2015-12-05-12.42.48.jpg"><img class="alignnone size-full wp-image-3623" src="2015-12-05-12.42.48.jpg" alt="" width="1000" height="659" /></a></p>
<p>This would let me kill two birds with one catapult boulder:</p>
<ul>
<li>Accomplish more parallel work, and be more productive in my daily work—I was utterly sick of “having to think about RAM” or experiencing debilitating lockups. I want to think about <em>business problems,</em> not the boundaries of my tools.</li>
<li>Accomplish more complex work, such as completing <a href="https://fortintam.com/blog/2017/06/11/painting-two-old-friends-tintin-vs-sephiroth/">a special painting project where I needed at least 10 gigabytes of RAM to work with</a> (Kusanagi 1.0 “only” had 5 GB).</li>
</ul>
<p>I did not expect to see an application-level performance difference compared to my previous quad-core Inspiron 530n, but there definitely is one—I was shocked at how ridiculously faster my “new” computer turned out to be, with the Xeon and DDR3 RAM (vs DDR2). With Firefox (before the Quantum days) or GTG, you could feel applications launching and responding noticeably quicker. Neat!</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2015-12-15-15.16.46.jpg"><img class="alignnone size-large wp-image-3624" src="2015-12-15-15.16.46-1024x714.jpg" alt="" width="525" height="366" /></a></p>
<h2>Basic care and tweaks</h2>
<p>First, the machine needed some cleanup (click to enlarge):</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2015-12-04-15.20.23.jpg"><img class="alignnone size-large wp-image-3627" src="2015-12-04-15.20.23-1024x636.jpg" alt="" width="525" height="326" /></a></p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2015-12-04-15.20.11.jpg"><img class="alignnone wp-image-3626 size-large" src="2015-12-04-15.20.11-1024x474.jpg" alt="" width="525" height="243" /></a></p>
<p>Then, I removed the metal faceplate in the front and custom-built an air intake dust filter—using stockings, some L-shaped metal enclosed in a plastic frame to create the filter’s support system, and some hooks to hold it in place.</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2016-05-08-14.48.56.jpg"><img class="alignnone size-large wp-image-3630" src="2016-05-08-14.48.56-1024x606.jpg" alt="" width="525" height="311" /></a></p>
<p>It now had a sober, unassuming “serious business” look, which I rather liked for its simplicity (and the lack of flashiness makes it less appealing to thieves):</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2016-05-09-14.38.45.jpg"><img class="alignnone size-large wp-image-3631" src="2016-05-09-14.38.45-972x1024.jpg" alt="" width="525" height="553" /></a></p>
<p>Here is the resulting battlestation:</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2016-11-20-21.00.23.jpg"><img class="alignnone wp-image-3629 size-full" src="2016-11-20-21.00.23.jpg" alt="" width="1000" height="893" /></a></p>
<p>With this “new” super powerful computer, I was ready to be immediately productive, right? Well, <em>almost. </em></p>
<h1>The quest for stability begins<br />
(descent into Hell)</h1>
<p><img class="alignnone size-full wp-image-3789" src="devil-may-cry-5-dante.jpg" alt="" width="1000" height="561" /></p>
<p>There turned out to be just <em>one</em> problem with the new workhorse: after a few weeks of use, I realized I kept running into a strange issue where <a href="https://bugs.freedesktop.org/show_bug.cgi?id=93341" target="_top" rel="noopener">the video card would randomly die on me and the Linux kernel would panic</a>, freezing the whole machine.</p>
<p>It would happen at any time: often when doing something that stresses the GPU (like watching videos or using an OpenGL/WebGL application), but also (a bit more rarely) when not doing anything in particular; I could be just sitting and staring at my desktop when suddenly the monitor would turn off and I would get the same errors in dmesg:</p>
<pre>radeon 0000:02:00.0: ring 0 stalled for more than 10252msec
radeon 0000:02:00.0: GPU lockup (current fence id 0x00000000006c9132 last fence id 0x00000000006c928b on ring 0)
radeon 0000:02:00.0: failed to get a new IB (-35)
[drm:radeon_cs_ioctl [radeon]] *ERROR* Failed to get ib !
BUG: unable to handle kernel paging request at ffffc90404239ffc
IP: [&lt;ffffffffa013736a&gt;] radeon_ring_backup+0xda/0x190 [radeon]
PGD 6068a8067 PUD 0 
Oops: 0000 [#1] SMP</pre>
<p>It was infuriating. I had this superb, powerful machine… that I couldn’t use except for the lightest tasks (like basic web surfing, email, office work), and still had to watch out for potential data loss as a result of unpredictable crashes.</p>
<p>Thus began a painful, expensive, nearly endless quest to figure out why my graphics card was randomly crashing. <em>Note: if you don’t care about the “investigation” educational part, Ctrl+F “Finding the culprit” to skip to the next part. Otherwise, read on, my geeky friend.</em></p>
<hr />
<p>Potential causes I suspected:</p>
<ol>
<li>Linux “radeonsi” driver bug/regression (“hey, it worked before December 2015!”): unsure, but presumed.</li>
<li>Some sort of SNAFU somewhere else in the stack in my Linux distribution (<a href="https://getfedora.org/workstation/" target="_top" rel="noopener">Fedora</a>), ie a distro-specific bug: unsure.</li>
<li>RAM errors: did memtests, no problems there.</li>
<li>Capacitor plague on the motherboard: nope, it all looked good to the naked eye.</li>
<li>Underpowered or failing power supply</li>
<li>Linux-incompatible motherboard or BIOS</li>
<li>Incompatible motherboard-and-GPU combination</li>
</ol>
<p>For #1 and 2, I placed my hopes on the (then upcoming) Wayland-based Fedora 25 (“gotta wait for nov/december 2016”): turned out to not be the solution. Still randomly crashing.</p>
<p>Hypotheses #6 and 7 would be dealbreakers, where it would mean I would have wasted my money on the workstation as I would have to replace it again. My geek honor was not ready to accept that scenario.</p>
<p>All along, I was trying to figure out how to trigger the bug, by:</p>
<ul>
<li>trying to overload the system (be it the graphics card or CPU or I/O, with games/compiling/webGL demos/video playback/etc.)</li>
<li>using a different graphics card (another older radeon, or a nVidia card which was impossible to get working drivers for)</li>
<li>putting the card back into my older computer (and indeed the issue didn’t seem to happen there)</li>
<li>transplanting (exchanging) power supplies</li>
<li>spreading the load across multiple power supplies (“What if one power supply is not enough to power the Xeon <em>and</em> the Radeon?”)</li>
</ul>
<p>It was absolute hell to debug/reproduce. It would sometimes crash within 10 minutes, an hour, or sometimes work for 2-3 days straight without issues, which made me question if it even worked 100.00% reliably on Kusanagi 1.0, muddying up the waters and making me waste countless week-ends and question my sanity. A true hardware heisenbug.</p>
<a href="https://fortintam.com/blog/wp-content/uploads/2016-07-01-19.04.45.jpg"><img class="size-large wp-image-3633" src="2016-07-01-19.04.45-785x1024.jpg" alt="" width="525" height="685" /></a>Running multiple simultaneous WebGL demos for days on end
<a href="https://fortintam.com/blog/wp-content/uploads/2016-07-01-multiple-web-benchmarks.jpg"><img class="size-large wp-image-3816" src="2016-07-01-multiple-web-benchmarks-1024x695.jpg" alt="" width="525" height="356" /></a>Some of the WebGL and WebRTC stress tests being run
<p>Throughout these tests, I was considering the possibility that Dell’s Foxconn power supply had received too much abuse from its previous owners, which  seemed plausible (but can you <em>really</em> be sure?) as the graphics card wouldn’t crash the kernel when it was inside “Kusanagi 1.0” or when Kusanagi 1.0’s power supply was connected siamese-style with a power link across to Kusanagi 2.0:</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2016-07-02-10.35.41.jpg"><img class="alignnone size-large wp-image-3634" src="2016-07-02-10.35.41-1024x518.jpg" alt="" width="525" height="266" /></a></p>
<p>This seemed like a good hypothesis, ergo #YOLO, I ordered a brand new power supply, the eVGA <em>Supernova G2</em> series (the best I could find on the market in late 2016—I spent way too much time researching and reading reviews) and… nope, it didn’t solve the issue.</p>
<p><img class="alignnone size-full wp-image-1962" src="rage-guy.jpg" alt="" width="418" height="357" /></p>
<p>125$ wasted for the sake of the experiment (it was not economically feasible to return the power supply, so I thought I might as well keep it). At least I could say I got an extremely good silent power supply that should last me a decade or more (considering my other branded power supply from 2003 still works today).</p>
<p>I also tested Windows on this machine to be sure, and no matter what I tried to do to stress the system, it wouldn’t crash, an observation that threw me back to the “it’s a bug somewhere in the Linux graphics stack” theory.</p>
<p>Okay, so the computer crashes <em>only</em> on Linux. Randomly. Counterproductively. At that point I had become quite disheartened with the Linux graphics stack that was causing me such grief for months on end, and I was looking at my options:</p>
<ul>
<li>Go back to Kusanagi 1.0 (and lose the copious amounts of RAM).</li>
<li>Run Windows: unbearable. I can’t stand this piece of crap, I feel handicapped everytime I use it.</li>
<li>Make a hackintosh out of it and run Mac OS: pain in the ass, and handicapping as well.</li>
<li>Buy nVidia and make it work. <em>Ha ha ha. No.</em></li>
<li>Resell Kusanagi 2.0, rebuild a 600-1000$ brand new Intel-only DIY workstation (Dell prices workstations at 2.5-3.5k$!) retrofitted in an old Mac Pro case.</li>
<li>Wait some months/years for <a href="https://en.wikipedia.org/wiki/Free_and_open-source_graphics_device_driver#amdgpu" target="_top" rel="noopener">AMDGPU</a> to replace radeonsi as the One and Only driver, and “hope” it is unaffected: I don’t believe in magic.</li>
<li>“Just don’t stress the system”, use the computer for menial tasks: a huge waste.</li>
</ul>
<hr />
<h1>Finding the culprit</h1>
<p><img class="alignnone size-full wp-image-3790" src="dante-vs-vergil.jpg" alt="" width="903" height="562" /></p>
<p>Eventually, after having spent weeks testing with games, videoconferencing, video playback and WebGL demos, I started truly <em>torturing</em> the graphics card with “furmark” and realized the issue occurred when the card was overheating to a really high temperature (113 degrees Celsius), but <em>only</em> on Linux. Finally, a way to reproduce the issue reliably! And so I wrote into <a href="https://bugs.freedesktop.org/show_bug.cgi?id=93341" target="_top" rel="noopener">the bug report</a>:</p>
<blockquote><p>My understanding is that on Radeons (well, at least the Radeon HD 7770), there is an emergency mechanism in the hardware (or firmware/microcode maybe) that activates self-throttling of performances when the GPU reaches a critical temperature. Normally, the video driver is supposed to handle this state change gracefully, however the radeonsi/radeon/amdgpu driver on Linux does not, so the kernel panics because the driver went belly up.</p></blockquote>
<p><img class="wp-image-3641 alignright" src="one-does-not-simply-say-well-it-worked-on-my-machine-500x295.jpg" alt="" width="351" height="207" />“Duh! Just get better cooling!” might sound like the solution, but technically it still is a software/driver issue: the radeonsi driver on Linux does not handle the event where the hardware force-throttles itself. In Windows, breaching the 110-113 degrees Celsius limit results in the video driver simply dropping frames massively, continuing to function at reduced performance (ie: going from 40-60 fps to 10-15 fps on one of my benchmarks). The system never crashes. The Linux driver <em>should</em> handle such scenarios gracefully just as well as the Windows driver. At least, that’s the theory.</p>
<p>In practice, it would be quicker for me to solve my cooling problem than to wait for a driver bugfix. Besides, my graphics card would thank me (and provide better performance).</p>
<p>But wait, I’ve had the GPU since 2012, so <strong>why didn’t I encounter this with my previous computer,</strong> prior to December 2015? Because Kusanagi 2.0’s case has a different airflow and cooling behavior from Kusanagi 1.0. So now you’re asking, <strong>why didn’t I realize this during my weeks of benchmarking</strong> then? Because it was very hard to get consistent crashes (the more I tried to investigate it, the less sense it made), due to these factors:</p>
<ul>
<li>The Radeon 7770 I have is an “open air” cooling system which spreads the heat into the case (not a “blower” fan that exhausts outside the case), which means that for the bug to occur, the whole system has to reach a temperature plateau which might require specific CPU and GPU interaction or ambient room temperature conditions;</li>
<li>I was sometimes testing with the case closed, sometimes with the case open (when trying different power supplies configurations), which threw off my results.</li>
</ul>
<p>Anyway. At least, now I knew the cause, and therefore had a basic workaround: just keep the computer’s case open, where the heat would evacuate naturally and the graphics card would never reach the critical temperature, preventing the issue. But this looks silly and lets the dust in, so I set out to find the “proper” solution to extract the heat without needing to keep the case open.</p>
<p><img class="alignnone size-full wp-image-3813" src="hhcib-topgear-2.png" alt="" width="560" height="315" /></p>
<h1>Reworking my workstation’s thermal design beyond what Dell intended it to be</h1>
<p>I had the following restrictions for the solution I wanted to find:</p>
<ol>
<li>Reasonably cheap. Otherwise I might as well just cut my losses and build a brand new machine.</li>
<li>No replacing my “perfectly good” and well-tested Radeon 7770: I didn’t want to go back into the “let’s wait for Free/Open drivers to be developed for your card” cycle again. Also, see point #1.</li>
<li>Super quiet. I’m an absolute maniac when it comes to having “silent” computers. I like to hear myself <em>think.</em> Therefore, the solution had to not only be efficient at exchanging the air between the case and the room, it also had to be nearly inaudible.</li>
<li>No drilling/cutting of the case if at all possible (I don’t like irreversible mods, given how much trial-and-error is involved here).</li>
</ol>
<p>The big challenge would be to devise something compatible with the T3500’s proprietary case and airflow design (2x120mm intake fans in a suspended cage in the front pushing air into a fanless CPU heatsink, and 2x80mm exhaust grilles at the bottom at the back):</p>
<a href="https://fortintam.com/blog/wp-content/uploads/original-Dell-T3500-thermal-design-airflow.png"><img class="wp-image-3646 size-full" src="original-dell-t3500-thermal-design-airflow.png" alt="" width="948" height="600" /></a>The Dell Precision T3500 moves air from front to back through the bottom half of the case, while the graphics card (which sucks air from the top and spreads it onto the card and everywhere into the case) is in the upper half of the case, with the power supply that does not extract anywhere near enough air from the GPU’s hotspot, and the GPU’s board blocking air exchange with the bottom half of the case—that clearly wasn’t going to work “as designed”.
<p>I considered the following possibilities:</p>
<ul>
<li>Just replace the thermal compound on the graphics card by Arctic Silver 5 (and wait 200 hours for it to cure), and try various fan combinations to see the impact on temperatures and time-to-crash.</li>
<li>Put an additional exhaust fan or two in the back in the lower half: nope, didn’t work: the hot air pocket remained in the upper half (and the power supply’s fans were not moving enough air to compensate it, either).</li>
<li>Put some small turbine <a href="https://www.amazon.com/StarTech-com-Expansion-Exhaust-Connector-FANCASE/dp/B0000510SS/" target="_top" rel="nofollow noopener">expansion slot fan</a> in the back: that would certainly be ineffective (since the card is an open design instead of a blower) and very noisy.</li>
<li>Leaving the case open and building a giant filter all over it.</li>
<li>Cutting out a hole in the case’s door to have an exhaust fan on the side, extracting the air from the GPU’s area: last resort only.</li>
<li>Watercooling the GPU, or replacing the GPU’s OEM air cooler by some aftermarket “blower” cooler… but that’s spending another pile of money that could get near the cost of a new graphics card, so it seemed beyond reason.</li>
</ul>
<a href="https://fortintam.com/blog/wp-content/uploads/2017-04-14-10.03.51.jpg"><img class="wp-image-3649 size-large" src="2017-04-14-10.03.51-1024x612.jpg" alt="" width="525" height="314" /></a>Taking apart the GPU’s stock cooling system
<a href="https://fortintam.com/blog/wp-content/uploads/2017-04-14-10.24.06.jpg"><img class="wp-image-3650 size-large" src="2017-04-14-10.24.06-1024x576.jpg" alt="" width="525" height="295" /></a>The GPU’s die after cleaning it up
<a href="https://fortintam.com/blog/wp-content/uploads/2017-04-14-10.27.37.jpg"><img class="wp-image-3651 size-large" src="2017-04-14-10.27.37-1024x576.jpg" alt="" width="525" height="295" /></a>Putting new thermal compound on the GPU
<p>Replacing the GPU’s thermal compound didn’t really improve things. There was a slight improvement, but not nearly enough.</p>
<p>Luckily, one day my pal Youness said, “I have an all-in-one watercooler that is gathering dust inside a decommissioned PC. I might as well just give it to you”, to which I replied, “Sure—wait, what did you just say?”</p>
<h2>A waterball’s chance in Hell</h2>
<p>I took the watercooler and proceeded to design the airflow around it. It had a very big radiator and fan (120mm), and a fairly short pair of flexible tubes, meaning I couldn’t place the radiator at the back because the exhaust grilles were too small (80mm) and the tubes would be too short to go around the bulky graphics card.</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-04-25-12.48.25.jpg"><img class="alignnone size-large wp-image-3652" src="2017-04-25-12.48.25-1024x576.jpg" alt="" width="525" height="295" /></a></p>
<p>So I had to reverse the case’s airflow: it would exhaust from the front, intake from the back, with the air passing through the CPU’s radiator while being pulled by the watercooler’s fan acting as a case exhaust fan. Essentially, two radiators from two different devices being cooled by one big fan—better hope your CPU heat doesn’t significantly affect the GPU’s radiator (luckily, it didn’t)!</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/kusanagi-2.0-Dell-T3500-thermal-airflow-design-take-2-indexed.png"><img class="alignnone size-full wp-image-3817" src="kusanagi-2.0-dell-t3500-thermal-airflow-design-take-2-indexed.png" alt="" width="948" height="600" /></a></p>
<p>The watercooler was a <em>CPU</em> watercooler, not actually meant to be installed on a GPU: it didn’t have compatible mounting brackets, and the contact surface was <em>immense</em> compared to the thumb-sized GPU die. So I used the “<a href="http://www.overclock.net/t/1203636/official-amd-ati-gpu-mod-club-aka-the-red-mod" target="_top" rel="nofollow noopener">red mod</a>” technique to fit it onto the GPU with <a href="https://en.wikipedia.org/wiki/Cable_tie" target="_top" rel="noopener">zip ties</a>. Serious business:</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-08-27-14.33.10.jpg"><img class="alignnone size-large wp-image-3819" src="2017-08-27-14.33.10-1024x576.jpg" alt="" width="525" height="295" /></a></p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-08-27-14.30.01.jpg"><img class="alignnone size-large wp-image-3820" src="2017-08-27-14.30.01-1024x685.jpg" alt="" width="525" height="351" /></a></p>
<p>I tested the new setup, and it seemed to work wonders: no matter what I did, the computer was now unable to overheat, and the watercooler’s radiator fan acting as the main case exhaust was sufficient to keep both the GPU and the CPU cool, even if both are being heavily loaded simultaneously for hours on end.</p>
<p>To complete the set up, I gave myself a treat and replaced the (also rattling) fan from that second-hand watercooler by a Noctua NF-P12, and added two Noctua redux NF-R8 intake fans to facilitate airflow to the CPU radiator.  Result: a computer that can handle any workload and stay whisper quiet.</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-08-27-14.29.36.jpg"><img class="alignnone size-large wp-image-3821" src="2017-08-27-14.29.36-1024x691.jpg" alt="" width="525" height="354" /></a></p>
<p>I just had to make a new custom intake filter with a wireframe instead of solid frame, to be installed on the back of the computer, which was much less elegant than the previous front intake filter due to the odd space and shape constraints on the back of the computer:</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-06-16-16.13.01.jpg"><img class="alignnone size-medium wp-image-3653" src="2017-06-16-16.13.01-500x282.jpg" alt="" width="500" height="282" /></a></p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-06-30-20.29.35.jpg"><img class="alignnone size-medium wp-image-3654" src="2017-06-30-20.29.35-500x282.jpg" alt="" width="500" height="282" /></a></p>
<p>As for the front exhaust, I screwed the watercooler’s radiator onto the front grille, and sealed everything with electric tape so that the ventilation holes would match the radiator exactly, keeping a direct airflow. It now looked like this (any resemblance with Frankenstein is purely accidental):</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-06-01-17.27.35.jpg"><img class="alignnone size-large wp-image-3656" src="2017-06-01-17.27.35-1024x882.jpg" alt="" width="525" height="452" /></a></p>
<p><strong>It all worked wonders… <em>until it didn’t.</em></strong> My modification worked for <em>exactly</em> three months, until I moved the computer a bit and the watercooler’s previously temporary rattling “air bubbles” noise became permanent, no matter how I shook or oriented the case. To top it all, while trying to solve the air bubbles issue, the watercooler’s block had now come loose from the GPU die and I would have to re-apply thermal compound and redo the whole zip tie setup (tightening the zip ties is fairly difficult, you need two people for that).</p>
<p>Welcome back to hell.</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/doom-4-cover-art-loli.jpg"><img class="alignnone size-large wp-image-3807" src="doom-4-cover-art-loli-1024x576.jpg" alt="" width="525" height="295" /></a></p>
<h2>Turning Hell upside down</h2>
<p>At that point in time, a new possible approach came to my mind: rip out the watercooler, revert to an “open” air-cooled GPU, and <strong>find a way to reorient the computer case itself</strong>. I figured that if the computer was just positioned differently, to let the GPU’s rising hot air escape “naturally” from the top of the computer case (instead of having the intake and exhaust fans move presumably cold air at the bottom of the case and letting the GPU sit in a stagnant air pocket above), it might make a difference. I was ready to try anything at this point.</p>
<p>To test my theory, and since I thought it was probably necessary to have the exhaust directed straight upwards (and the intake below the case), I devised a pretty silly scheme to create “legs” for the back of my computer to stand on (remember, the back is where the air intake fan now was, as well as the connectivity cabling). So I literally “bricked” my computer:</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-08-28-11.07.18.jpg"><img class="alignnone size-large wp-image-3804" src="2017-08-28-11.07.18-1024x680.jpg" alt="" width="525" height="349" /></a></p>
<p>The only other way would have been to have the computer suspended on steel wires, but that would be a big stress for my desk and it would also be highly impractical for servicing the machine.</p>
<p>Test results showed that my “bricked computer” now had the best “theoretical” airflow design indeed: with the intake at the bottom and the exhaust at the top, the computer was now completely immune to overheating, even with the case closed! Hurrah!</p>
<p>However, the whole set-up was a bit flimsy and looked quite silly—a 17 kg (38 lbs) computer standing in equilibrium by the edges of its chassis on <em>two bricks on top of a towel!</em> Seriously Anakin, look at this:</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/2017-08-28-11.07.01.jpg"><img class="alignnone size-large wp-image-3803" src="2017-08-28-11.07.01-1024x943.jpg" alt="" width="525" height="483" /></a></p>
<p>So I wondered: would it still work if I <strong>flip the entire computer case upside-down from its normal orientation,</strong> with both intake and exhaust fans being located in the top portion and moving air horizontally? I had doubts—after all, the power supply’s exhaust fan in the previous configurations had never been sufficient to get rid of the GPU’s hot air pocket, so would this really work any better?</p>
<p><a href="https://fortintam.com/blog/wp-content/uploads/kusanagi-2.0-Dell-T3500-thermal-airflow-design-take-3-indexed.png"><img class="alignnone size-full wp-image-3818" src="kusanagi-2.0-dell-t3500-thermal-airflow-design-take-3-indexed.png" alt="" width="948" height="600" /></a></p>
<p>Turns out that it did.</p>
<ul>
<li>The GPU remains cool in all my stress tests, as the rising heat is still correctly getting evacuated by new “horizontal air corridor” at the top of the case.</li>
<li>The Xeon CPU stays cool under normal working conditions; only under heavy CPU load will its temperature rise (fairly high, up to 80-85 Celsius, due to the recycled GPU air and the absence of fans mounted directly on the Xeon’s heatsink), though it never reaches “critical” thermal limits. <small>I thought of building an “air duct” system to force more air to pass through the CPU heatsink for the rare occasions when I’m pegging the CPU for extended periods of time, but you know what? <em>Screw that—</em>it works “well enough” (and running hot is what workstation-grade Xeons were <em>designed</em> for anyway).</small></li>
</ul>
<p>As you can see, the final solution turned out to be quite trivial. So simple, in fact, I can’t believe it took me so long to find it—and that’s not for lack of online research, discussion with fellow geeks, or thinking hard about the problem as a certified geek.</p>
<h1>Epilogue</h1>
<p><img class="alignnone size-full wp-image-3791" src="devil-may-cry-dmc-fanart.jpeg" alt="" width="1024" height="442" /></p>
<p>The story ends as I have achieved workstation nirvana (it really is my favorite computer now, making any work enjoyable), after spending <del>18</del> <strong>21</strong> months to troubleshoot and fix what I thought was a software issue, then a hardware issue, then “a little bit of both” <img src="1f613.png" alt="😓" class="wp-smiley" /></p>
<p>I ended up spending a grand total of 555 C$ on the whole setup (370$ for the computer, 125$ for the unecessary power supply, 60$ for top-of-the-line fans), but that is still quite inexpensive (a brand new silent computer with that kind of power would run in the thousands of dollars). Just ignore the “opportunity cost” of my hourly rate when it comes to the time I spent on this!</p>
<p>That said, I learned a lot in the process, and that’s priceless. I hope this troubleshooting tale can help others too—or that you at least had a good laugh at my persistence in repurposing a legacy system into an unstoppable silent powerhouse that crushes most machines we see out there even today.</p>
<p>Hmm? What is it you’re saying? All mainstream CPUs made since the original Pentium have now been found to be vulnerable to a <a href="https://spectreattack.com/" target="_top" rel="noopener">fundamental architectural flaw</a> and we need to get brand new CPUs designed as of 2018? Well,</p>
<p><img class="alignnone size-full wp-image-3797" src="cartman-damnit.gif" alt="" width="220" height="165" /></p>
<p>P.s.: feel free to retweet <a href="https://twitter.com/nekohayo/status/967800437524123648" target="_top" rel="noopener">this</a> ;)</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://k-d-w.org/blog/107/convolutional-autoencoder-as-tensorflow-estimator"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Convolutional Autoencoder as TensorFlow estimator</span></a><div class="lastUpdated">2018年2月25日 23:07</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><span class="field field-name-title field-formatter-string field-type-string field-label-hidden">Convolutional Autoencoder as TensorFlow estimator</span>
<div class="clearfix text-formatted field field-node--body field-formatter-text-default field-name-body field-type-text-with-summary field-label-hidden has-single"><div class="field__items"><div class="field__item"><div class="tex2jax_process"><p>In my previous <a href="https://k-d-w.org/node/103">post</a>, I explained how to implement autoencoders as TensorFlow <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span>. I thought it would be nice to add convolutional autoencoders in addition to the existing fully-connected autoencoder. So that's what I did. Moreover, I added the option to extract the low-dimensional encoding of the encoder and visualize it in TensorBoard.</p>
<p>The complete source code is available at <a href="https://github.com/sebp/tf_autoencoder">https://github.com/sebp/tf_autoencoder</a>.</p>
<h2>Why convolutions?</h2>
<p>For the fully-connected autoencoder, we reshaped each 28x28 image to a 784-dimensional feature vector. Next, we assigned a separate weight to each edge connecting one of 784 pixels to one of 128 neurons of the first hidden layer, which amounts to 100,352 weights (excluding biases) that need to be learned during training. For the last layer of the decoder, we need another 100,352 weights to reconstruct the full-size image. Considering that the whole autoencoder consists of 222,384 weights, it is obvious that these two layers dominate other layers by a large margin. When using higher resolution images, this imbalance becomes even more dramatic.</p>
<p>
Convolutional layers allow us to significantly reduce the number of weights by sharing weights across multiple edges connecting pixels in the input image to the first hidden layer. A convolutional layer takes a small matrix of weights, let's say 3x3, and slides it across the whole image as shown in the animation below (courtesy of <a href="https://github.com/vdumoulin/conv_arithmetic">Vincent Dumoulin and Francesco Visin</a>):</p>
<div align="center">
<img src="same_padding_no_strides.gif" alt="Convolution with padding and no strides" width="300" /></div>
<p>The blue tiles represent the pixels of the input image, and the green tiles represent the output of the convolutional layer after multiplying each 3x3 patch in the input image with the 3x3 weight matrix and summing the result. This operation is called a convolution. To obtain an output – called <em>feature map</em> – of the same size as the input, we need to add a margin of 1 pixel (white tiles). Typical implementations of convolutions use the value of the closest true pixel for padded pixels. Having just a single 3x3 weight matrix in each layer is quite restrictive, thus we apply multiple convolutions on the same input, each with it's own weight matrix. The big advantage is that the number of weights does not depend on the size of the input image, as it was the case for fully-connected layers, instead it is determined by the number of filters/kernels and their respective size (3x3 in the example above).</p>
<p>In the case of MNIST, inputs are $28 \times 28 \times 1$ gray-scale images with a single color channel, and the output of the first convolutional layer has as many "color" channels (feature maps) as there are filters, for example 16. The second convolutional layer will perform the same operation as the first layer, but on an "image" (or tensor to be more precise) of $28 \times 28 \times 16$.</p>
<h2>Encoder</h2>
<p>In the fully-connected autoencoder, we used layers with decreasing complexity by gradually decreasing the number of hidden units. When using convolutional layers in the encoder, we can reduce the complexity by lowering the number of filters or the resolution of the output. Two common approaches exist for down-scaling: 1) pooling values in a small window, 2) using convolutions with strides. The former introduces no additional weights, we simply compute the maximum/minimum/average over a small window – typically 2x2, which reduces width and height by a factor of 2. Note that the pooling operation is applied to each channel independently, thus the number of channels is not altered. Alternatively, a specific form of convolutional layer can be used, as depicted below:</p>
<div align="center">
<img src="padding_strides.gif" width="300" /></div>
<p>The difference to the standard convolution from above is that the weight matrix is moving by 2 instead 1 pixel, thus halving height and weight.</p>
<p>In TensorFlow, the encoder following the first approach (using max pooling) becomes:<br /></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> conv_encoder<span class="br0">(</span>inputs<span class="sy0">,</span> num_filters<span class="sy0">,</span> scope<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>:
    net <span class="sy0">=</span> inputs
    <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span>scope<span class="sy0">,</span> <span class="st0">'encoder'</span><span class="sy0">,</span> <span class="br0">[</span>inputs<span class="br0">]</span><span class="br0">)</span>:
        <span class="kw1">for</span> layer_id<span class="sy0">,</span> num_outputs <span class="kw1">in</span> <span class="kw2">enumerate</span><span class="br0">(</span>num_filters<span class="br0">)</span>:
            <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span><span class="st0">'block{}'</span>.<span class="me1">format</span><span class="br0">(</span>layer_id<span class="br0">)</span><span class="br0">)</span>:
                net <span class="sy0">=</span> slim.<span class="me1">repeat</span><span class="br0">(</span>net<span class="sy0">,</span> <span class="nu0">2</span><span class="sy0">,</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d</span><span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_outputs<span class="sy0">,</span>
                                  kernel_size<span class="sy0">=</span><span class="nu0">3</span><span class="sy0">,</span> stride<span class="sy0">=</span><span class="nu0">1</span><span class="sy0">,</span> padding<span class="sy0">=</span><span class="st0">"SAME"</span><span class="br0">)</span>
                net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">max_pool2d</span><span class="br0">(</span>net<span class="sy0">,</span> kernel_size<span class="sy0">=</span><span class="nu0">2</span><span class="br0">)</span>
 
        net <span class="sy0">=</span> tf.<span class="me1">identity</span><span class="br0">(</span>net<span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'output'</span><span class="br0">)</span>
    <span class="kw1">return</span> net</pre></div></div><br />
where <span class="geshifilter"><code class="text geshifilter-text">num_filters</code></span> is a list number of filters (in decreasing order), and we use a block of two convolutional layers before reducing the spatial resolution via max pooling. For the second approach, the <span class="geshifilter"><code class="text geshifilter-text">max_pool2d</code></span> layer is replaced by<br /><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d</span><span class="br0">(</span>net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_outputs<span class="sy0">,</span>
                         kernel_size<span class="sy0">=</span><span class="nu0">3</span><span class="sy0">,</span> stride<span class="sy0">=</span><span class="nu0">2</span><span class="sy0">,</span> padding<span class="sy0">=</span><span class="st0">"SAME"</span><span class="br0">)</span></pre></div></div><br />
where <span class="geshifilter"><code class="text geshifilter-text">stride=2</code></span> tells TensorFlow to slide the weight matrix by 2 pixels. In contrast to max pooling, adding another convolutional layer introduces additional weights when downscaling the image.<br /><br /><h2>Decoder</h2>
<p>In the decoder, we need to reverse the operations of the encoder and up-scale the image from the low-dimensional embedding of the encoder to its original size. In particular, we need to increase the spatial resolution to 28x28 and reduce the number of channels to 1. For up-scaling, we use a so called transposed convolution with stride 2, which performs the operation depicted in the animation below:</p>
<div align="center">
<img src="padding_strides_transposed.gif" width="300" /></div>
<p>Whereas using stride 2 in the conventional convolutional layer had the effect of sliding the weight matrix by 2 pixels, here, stride determines the dilation factor for the input feature map. For stride 2, a 1 pixel margin is introduced around each pixel. Thus, the input is up-scaled (weight and height double) and the convolution is applied, leading to a feature map with higher spatial resolution than the input. As before, we can apply this operation multiple times, to obtain a multi-channel output.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> conv_decoder<span class="br0">(</span>inputs<span class="sy0">,</span> num_filters<span class="sy0">,</span> output_shape<span class="sy0">,</span> scope<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>:
    net <span class="sy0">=</span> inputs
    <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span>scope<span class="sy0">,</span> <span class="st0">'decoder'</span><span class="sy0">,</span> <span class="br0">[</span>inputs<span class="br0">]</span><span class="br0">)</span>:
        <span class="kw1">for</span> layer_id<span class="sy0">,</span> num_outputs <span class="kw1">in</span> <span class="kw2">enumerate</span><span class="br0">(</span>num_filters<span class="br0">)</span>:
            <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span><span class="st0">'block_{}'</span>.<span class="me1">format</span><span class="br0">(</span>layer_id<span class="br0">)</span><span class="sy0">,</span>
                                   values<span class="sy0">=</span><span class="br0">(</span>net<span class="sy0">,</span><span class="br0">)</span><span class="br0">)</span>:
                net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d_transpose</span><span class="br0">(</span>
                        net<span class="sy0">,</span> num_outputs<span class="sy0">,</span>
                        kernel_size<span class="sy0">=</span><span class="nu0">3</span><span class="sy0">,</span> stride<span class="sy0">=</span><span class="nu0">2</span><span class="sy0">,</span> padding<span class="sy0">=</span><span class="st0">'SAME'</span><span class="br0">)</span>
 
        <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span><span class="st0">'linear'</span><span class="sy0">,</span> values<span class="sy0">=</span><span class="br0">(</span>net<span class="sy0">,</span><span class="br0">)</span><span class="br0">)</span>:
            net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d_transpose</span><span class="br0">(</span>
                net<span class="sy0">,</span> <span class="nu0">1</span><span class="sy0">,</span> activation_fn<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>
 
    <span class="kw1">return</span> net</pre></div></div>
<p>where this time <span class="geshifilter"><code class="text geshifilter-text">num_filters</code></span> is in decreasing order. The final transposed convolution is used to obtain a single-channel image (without non-linearity) and will be passed to the same loss function used in the fully-connected autoencoder.</p>
<p>There's one important aspect, we haven't considered yet. When the encoder takes an image of size 28x28 and outputs a low-dimensional feature map of size 4x4, which get's up-scaled three times, we end up with a reconstructed image of size 32x32, which is larger than the input image. We can simply solve this problem by cropping 2 pixels off each side of the image.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">shape <span class="sy0">=</span> tf.<span class="me1">shape</span><span class="br0">(</span>net<span class="br0">)</span>.<span class="me1">as_list</span><span class="br0">(</span><span class="br0">)</span>
output <span class="sy0">=</span> net<span class="br0">[</span>:<span class="sy0">,</span> <span class="nu0">2</span>:shape<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span> - <span class="nu0">2</span><span class="sy0">,</span> <span class="nu0">2</span>:shape<span class="br0">[</span><span class="nu0">2</span><span class="br0">]</span> - <span class="nu0">2</span><span class="sy0">,</span> :<span class="br0">]</span></pre></div></div><br />
 
<h2>The model</h2>
<p>It is now straight-forward to construct the autoencoder model</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> conv_autoencoder<span class="br0">(</span>inputs<span class="sy0">,</span> num_filters<span class="sy0">,</span> activation_fn<span class="sy0">,</span> weight_decay<span class="sy0">,</span> mode<span class="br0">)</span>:
    weights_init <span class="sy0">=</span> slim.<span class="me1">initializers</span>.<span class="me1">variance_scaling_initializer</span><span class="br0">(</span><span class="br0">)</span>
    <span class="kw1">if</span> weight_decay <span class="kw1">is</span> <span class="kw2">None</span>:
        weights_regularizer <span class="sy0">=</span> <span class="kw2">None</span>
    <span class="kw1">else</span>:
        weights_reg <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">l2_regularizer</span><span class="br0">(</span>weight_decay<span class="br0">)</span>
 
    <span class="kw1">with</span> slim.<span class="me1">arg_scope</span><span class="br0">(</span><span class="br0">[</span>tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d</span><span class="sy0">,</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d_transpose</span><span class="br0">]</span><span class="sy0">,</span>
                        weights_initializer<span class="sy0">=</span>weights_init<span class="sy0">,</span>
                        weights_regularizer<span class="sy0">=</span>weights_reg<span class="sy0">,</span>
                        activation_fn<span class="sy0">=</span>activation_fn<span class="br0">)</span>:
        net <span class="sy0">=</span> tf.<span class="me1">reshape</span><span class="br0">(</span>inputs<span class="sy0">,</span> <span class="br0">[</span>-<span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">1</span><span class="br0">]</span><span class="br0">)</span>
        net <span class="sy0">=</span> conv_encoder<span class="br0">(</span>net<span class="sy0">,</span> num_filters<span class="br0">)</span>
        net <span class="sy0">=</span> conv_decoder<span class="br0">(</span>net<span class="sy0">,</span> num_filters<span class="br0">[</span>::-<span class="nu0">1</span><span class="br0">]</span><span class="sy0">,</span> <span class="br0">[</span>-<span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">1</span><span class="br0">]</span><span class="br0">)</span>
 
        net <span class="sy0">=</span> tf.<span class="me1">reshape</span><span class="br0">(</span>net<span class="sy0">,</span> <span class="br0">[</span>-<span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">28</span> * <span class="nu0">28</span><span class="br0">]</span><span class="br0">)</span>
    <span class="kw1">return</span> net</pre></div></div>
<p>As before, the output is the input to <span class="geshifilter"><code class="text geshifilter-text">tf.losses.sigmoid_cross_entropy</code></span>, which is the loss function we want to minimize.</p>
<p>A convolutional autoencoder with 16 and two times 8 filters in the encoder and decoder has a mere 7873 weights and achieves a similar performance than the fully-connected auto-encoder with 222,384 weights (128, 64, and 32 nodes in encoder and decoder). The video below shows ten reconstructed images from the test data and their corresponding groundtruth after each epoch of training:<br />Your browser does not support the video tag.<br /><br /></p>
<h2>Visualizing the embedding</h2>
<p>Thanks to TensorBoard, we can also interactively visualize the low-dimensional embedding of our images, which looks something like the image below (click to see a larger version).</p>
<div align="center">
<a href="https://k-d-w.org/uploads/images/autoencoder/embedding.png"><img src="embedding_thumb.png" /></a>
</div>
<p>There are some clusters that are relatively homogeneous, like the left one, which is predominantly composed of 1s, or the red cluster composed of 2s. On the other hand, the low-dimensional embedding struggles to distinguish between 5s and 3s. If we wanted to classify images, the low-dimensional representation would likely not yield great results. Of course, one could make the autoencoder deeper or increase the size of the low-dimensional embedding, which I encourage you to explore.</p>
<h2>Convolutions and data format</h2>
<p>If you are running the code on a GPU, there is a technical detail related to how convolutions are implemented and how images are represented in memory. In the code above, I assumed that the <em>last dimension</em> corresponds to the color channel, which is of size 1 for the input and corresponds to the number of feature maps otherwise. Thus, convolutions would operate on 4D Tensors of size $\text{batch size} \times \text{height} \times \text{width} \times \text{channels}$. This is TensorFlow's default format. Unfortunately, NVIDIA's cuDNN routines are optimized for a different data format, where the channel dimension comes before the spatial dimensions, i.e., tensors are of the format $\text{batch size} \times \text{channels} \times \text{height} \times \text{width}$. After reordering dimensions, you have to call <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d">tf.contrib.layers.conv2d</a> with the argument <span class="geshifilter"><code class="text geshifilter-text">data_format="NCHW"</code></span>, instead of the default <span class="geshifilter"><code class="text geshifilter-text">data_format="NHWC"</code></span>. The speed-up can be substantial, on a p2xlarge AWS instance, this increased the training speed from 27 iterations per second to 40 iterations per second. In my code, you just have to change <a href="https://github.com/sebp/tf_autoencoder/blob/master/tf_autoencoder/layers.py#L243">this line</a> to use the alternative data format.</p>
<p>I hope my code provides a starting point for convolutional autoencoders in TensorFlow. If you want to learn more about convolutional neural networks, check out the links at the bottom.</p>
<h2>References</h2>
<ul><li><a href="https://arxiv.org/abs/1512.07108">Recent Advances in Convolutional Neural Networks</a></li>
<li><a href="https://github.com/vdumoulin/conv_arithmetic">Technical report on convolution arithmetic</a></li>
<li><a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d">An Introduction to different Types of Convolutions in Deep Learning</a></li>
</ul></div></div></div>
</div>
<span class="field field-name-uid field-formatter-author field-type-entity-reference field-label-hidden"><span>sebp</span></span>
<span class="field field-name-created field-formatter-timestamp field-type-created field-label-hidden">Sun, 02/25/2018 - 16:07</span>
<a name="comments" id="comments"></a>
  <h1 class="begin-comments">Comments</h1></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://coaxion.net/blog/2018/02/how-to-write-gstreamer-elements-in-rust-part-2-a-raw-audio-sine-wave-source/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">How to write GStreamer Elements in Rust Part 2: A raw audio sine wave source</span></a><div class="lastUpdated">2018年2月21日 22:05</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>A bit later than anticipated, this is now part two of the blog post series about writing <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> elements in <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a>. Part one can be found <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/" rel="noopener" target="_top">here</a>, and I’ll assume that everything written there is known already.</p>
<p>In this part, a raw audio sine wave source element is going to be written. It will be similar to the one Mathieu was writing in his <a href="https://mathieuduponchelle.github.io/2018-02-01-Python-Elements.html" rel="noopener" target="_top">blog post</a> about writing such a GStreamer element in Python. Various details will be different though, but more about that later.</p>
<p>The final code can be found <a href="https://github.com/sdroege/gst-plugin-rs/blob/0.1/gst-plugin-tutorial/src/sinesrc.rs" rel="noopener" target="_top">here</a>.</p>
<h3 id="toc">Table of Contents</h3>
<ol>
<li><a href="https://coaxion.net/blog/feed/#boilerplate">Boilerplate</a></li>
<li><a href="https://coaxion.net/blog/feed/#caps">Caps Negotiation</a></li>
<li><a href="https://coaxion.net/blog/feed/#query-handling">Query Handling</a></li>
<li><a href="https://coaxion.net/blog/feed/#buffer-creation">Buffer Creation</a></li>
<li><a href="https://coaxion.net/blog/feed/#live">(Pseudo) Live Mode</a></li>
<li><a href="https://coaxion.net/blog/feed/#unlock">Unlocking</a></li>
<li><a href="https://coaxion.net/blog/feed/#seeking">Seeking</a></li>
</ol>
<h3 id="boilerplate">Boilerplate</h3>
<p>The first part here will be all the boilerplate required to set up the element. You can safely <a href="https://coaxion.net/blog/feed/#caps">skip</a> this if you remember all this from the previous blog post.</p>
<p>Our sine wave element is going to produce raw audio, with a number of channels and any possible sample rate with both 32 bit and 64 bit floating point samples. It will produce a simple sine wave with a configurable frequency, volume/mute and number of samples per audio buffer. In addition it will be possible to configure the element in (pseudo) live mode, meaning that it will only produce data in real-time according to the pipeline clock. And it will be possible to seek to any time/sample position on our source element. It will basically be a more simply version of the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-plugins/html/gst-plugins-base-plugins-audiotestsrc.html" rel="noopener" target="_top">audiotestsrc</a> element from gst-plugins-base.</p>
<p>So let’s get started with all the boilerplate. This time our element will be based on the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseSrc.html" rel="noopener" target="_top">BaseSrc</a> base class instead of <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseTransform.html" rel="noopener" target="_top">BaseTransform</a>.</p>
<p></p><pre class="crayon-plain-tag">use glib;
use gst;
use gst::prelude::*;
use gst_base::prelude::*;
use gst_audio;

use byte_slice_cast::*;

use gst_plugin::properties::*;
use gst_plugin::object::*;
use gst_plugin::element::*;
use gst_plugin::base_src::*;

use std::{i32, u32};
use std::sync::Mutex;
use std::ops::Rem;

use num_traits::float::Float;
use num_traits::cast::NumCast;

// Default values of properties
const DEFAULT_SAMPLES_PER_BUFFER: u32 = 1024;
const DEFAULT_FREQ: u32 = 440;
const DEFAULT_VOLUME: f64 = 0.8;
const DEFAULT_MUTE: bool = false;
const DEFAULT_IS_LIVE: bool = false;

// Property value storage
#[derive(Debug, Clone, Copy)]
struct Settings {
    samples_per_buffer: u32,
    freq: u32,
    volume: f64,
    mute: bool,
    is_live: bool,
}

impl Default for Settings {
    fn default() -&gt; Self {
        Settings {
            samples_per_buffer: DEFAULT_SAMPLES_PER_BUFFER,
            freq: DEFAULT_FREQ,
            volume: DEFAULT_VOLUME,
            mute: DEFAULT_MUTE,
            is_live: DEFAULT_IS_LIVE,
        }
    }
}

// Metadata for the properties
static PROPERTIES: [Property; 5] = [
    Property::UInt(
        "samples-per-buffer",
        "Samples Per Buffer",
        "Number of samples per output buffer",
        (1, u32::MAX),
        DEFAULT_SAMPLES_PER_BUFFER,
        PropertyMutability::ReadWrite,
    ),
    Property::UInt(
        "freq",
        "Frequency",
        "Frequency",
        (1, u32::MAX),
        DEFAULT_FREQ,
        PropertyMutability::ReadWrite,
    ),
    Property::Double(
        "volume",
        "Volume",
        "Output volume",
        (0.0, 10.0),
        DEFAULT_VOLUME,
        PropertyMutability::ReadWrite,
    ),
    Property::Boolean(
        "mute",
        "Mute",
        "Mute",
        DEFAULT_MUTE,
        PropertyMutability::ReadWrite,
    ),
    Property::Boolean(
        "is-live",
        "Is Live",
        "(Pseudo) live output",
        DEFAULT_IS_LIVE,
        PropertyMutability::ReadWrite,
    ),
];

// Stream-specific state, i.e. audio format configuration
// and sample offset
struct State {
    info: Option,
    sample_offset: u64,
    sample_stop: Option,
    accumulator: f64,
}

impl Default for State {
    fn default() -&gt; State {
        State {
            info: None,
            sample_offset: 0,
            sample_stop: None,
            accumulator: 0.0,
        }
    }
}

// Struct containing all the element data
struct SineSrc {
    cat: gst::DebugCategory,
    settings: Mutex,
    state: Mutex,
}

impl SineSrc {
    // Called when a new instance is to be created
    fn new(element: &amp;BaseSrc) -&gt; Box&gt; {
        // Initialize live-ness and notify the base class that
        // we'd like to operate in Time format
        element.set_live(DEFAULT_IS_LIVE);
        element.set_format(gst::Format::Time);

        Box::new(Self {
            cat: gst::DebugCategory::new(
                "rssinesrc",
                gst::DebugColorFlags::empty(),
                "Rust Sine Wave Source",
            ),
            settings: Mutex::new(Default::default()),
            state: Mutex::new(Default::default()),
        })
    }

    // Called exactly once when registering the type. Used for
    // setting up metadata for all instances, e.g. the name and
    // classification and the pad templates with their caps.
    //
    // Actual instances can create pads based on those pad templates
    // with a subset of the caps given here. In case of basesrc,
    // a "src" and "sink" pad template are required here and the base class
    // will automatically instantiate pads for them.
    //
    // Our element here can output f32 and f64
    fn class_init(klass: &amp;mut BaseSrcClass) {
        klass.set_metadata(
            "Sine Wave Source",
            "Source/Audio",
            "Creates a sine wave",
            "Sebastian Dröge ",
        );

        // On the src pad, we can produce F32/F64 with any sample rate
        // and any number of channels
        let caps = gst::Caps::new_simple(
            "audio/x-raw",
            &amp;[
                (
                    "format",
                    &amp;gst::List::new(&amp;[
                        &amp;gst_audio::AUDIO_FORMAT_F32.to_string(),
                        &amp;gst_audio::AUDIO_FORMAT_F64.to_string(),
                    ]),
                ),
                ("layout", &amp;"interleaved"),
                ("rate", &amp;gst::IntRange::::new(1, i32::MAX)),
                ("channels", &amp;gst::IntRange::::new(1, i32::MAX)),
            ],
        );
        // The src pad template must be named "src" for basesrc
        // and specific a pad that is always there
        let src_pad_template = gst::PadTemplate::new(
            "src",
            gst::PadDirection::Src,
            gst::PadPresence::Always,
            &amp;caps,
        );
        klass.add_pad_template(src_pad_template);

        // Install all our properties
        klass.install_properties(&amp;PROPERTIES);
    }
}

impl ObjectImpl for SineSrc {
    // Called whenever a value of a property is changed. It can be called
    // at any time from any thread.
    fn set_property(&amp;self, obj: &amp;glib::Object, id: u32, value: &amp;glib::Value) {
        let prop = &amp;PROPERTIES[id as usize];
        let element = obj.clone().downcast::().unwrap();

        match *prop {
            Property::UInt("samples-per-buffer", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let samples_per_buffer = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing samples-per-buffer from {} to {}",
                    settings.samples_per_buffer,
                    samples_per_buffer
                );
                settings.samples_per_buffer = samples_per_buffer;
                drop(settings);

                let _ =
                    element.post_message(&amp;gst::Message::new_latency().src(Some(&amp;element)).build());
            }
            Property::UInt("freq", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let freq = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing freq from {} to {}",
                    settings.freq,
                    freq
                );
                settings.freq = freq;
            }
            Property::Double("volume", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let volume = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing volume from {} to {}",
                    settings.volume,
                    volume
                );
                settings.volume = volume;
            }
            Property::Boolean("mute", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let mute = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing mute from {} to {}",
                    settings.mute,
                    mute
                );
                settings.mute = mute;
            }
            Property::Boolean("is-live", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let is_live = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing is-live from {} to {}",
                    settings.is_live,
                    is_live
                );
                settings.is_live = is_live;
            }
            _ =&gt; unimplemented!(),
        }
    }

    // Called whenever a value of a property is read. It can be called
    // at any time from any thread.
    fn get_property(&amp;self, _obj: &amp;glib::Object, id: u32) -&gt; Result {
        let prop = &amp;PROPERTIES[id as usize];

        match *prop {
            Property::UInt("samples-per-buffer", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.samples_per_buffer.to_value())
            }
            Property::UInt("freq", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.freq.to_value())
            }
            Property::Double("volume", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.volume.to_value())
            }
            Property::Boolean("mute", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.mute.to_value())
            }
            Property::Boolean("is-live", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.is_live.to_value())
            }
            _ =&gt; unimplemented!(),
        }
    }
}

// Virtual methods of gst::Element. We override none
impl ElementImpl for SineSrc { }

impl BaseSrcImpl for SineSrc {
    // Called when starting, so we can initialize all stream-related state to its defaults
    fn start(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // Reset state
        *self.state.lock().unwrap() = Default::default();

        gst_info!(self.cat, obj: element, "Started");

        true
    }

    // Called when shutting down the element so we can release all stream-related state
    fn stop(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // Reset state
        *self.state.lock().unwrap() = Default::default();

        gst_info!(self.cat, obj: element, "Stopped");

        true
    }
}

struct SineSrcStatic;

// The basic trait for registering the type: This returns a name for the type and registers the
// instance and class initializations functions with the type system, thus hooking everything
// together.
impl ImplTypeStatic for SineSrcStatic {
    fn get_name(&amp;self) -&gt; &amp;str {
        "SineSrc"
    }

    fn new(&amp;self, element: &amp;BaseSrc) -&gt; Box&gt; {
        SineSrc::new(element)
    }

    fn class_init(&amp;self, klass: &amp;mut BaseSrcClass) {
        SineSrc::class_init(klass);
    }
}

// Registers the type for our element, and then registers in GStreamer under
// the name "sinesrc" for being able to instantiate it via e.g.
// gst::ElementFactory::make().
pub fn register(plugin: &amp;gst::Plugin) {
    let type_ = register_type(SineSrcStatic);
    gst::Element::register(plugin, "rssinesrc", 0, type_);
}</pre><p></p>
<p>If any of this needs explanation, please see the <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/" rel="noopener" target="_top">previous</a> blog post and the comments in the code. The explanation for all the structs fields and what they’re good for will follow in the next sections.</p>
<p>With all of the above and a small addition to <i>src/lib.rs</i> this should compile now.</p>
<p></p><pre class="crayon-plain-tag">mod sinesrc;
[...]

fn plugin_init(plugin: &amp;gst::Plugin) -&gt; bool {
    [...]
    sinesrc::register(plugin);
    true
}</pre><p></p>
<p>Also a couple of new crates have to be added to <i>Cargo.toml</i> and <i>src/lib.rs</i>, but you best check the code in the <a href="https://github.com/sdroege/gst-plugin-rs/tree/0.1/gst-plugin-tutorial" rel="noopener" target="_top">repository</a> for details.</p>
<h3 id="caps">Caps Negotiation</h3>
<p>The first part that we have to implement, just like last time, is caps negotiation. We already notified the base class about any caps that we can potentially handle via the caps in the pad template in <i>class_init</i> but there are still two more steps of behaviour left that we have to implement.</p>
<p>First of all, we need to get notified whenever the caps that our source is configured for are changing. This will happen once in the very beginning and then whenever the pipeline topology or state changes and new caps would be more optimal for the new situation. This notification happens via the <i>BaseTransform::set_caps</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn set_caps(&amp;self, element: &amp;BaseSrc, caps: &amp;gst::CapsRef) -&gt; bool {
        use std::f64::consts::PI;

        let info = match gst_audio::AudioInfo::from_caps(caps) {
            None =&gt; return false,
            Some(info) =&gt; info,
        };

        gst_debug!(self.cat, obj: element, "Configuring for caps {}", caps);

        element.set_blocksize(info.bpf() * (*self.settings.lock().unwrap()).samples_per_buffer);

        let settings = *self.settings.lock().unwrap();
        let mut state = self.state.lock().unwrap();

        // If we have no caps yet, any old sample_offset and sample_stop will be
        // in nanoseconds
        let old_rate = match state.info {
            Some(ref info) =&gt; info.rate() as u64,
            None =&gt; gst::SECOND_VAL,
        };

        // Update sample offset and accumulator based on the previous values and the
        // sample rate change, if any
        let old_sample_offset = state.sample_offset;
        let sample_offset = old_sample_offset
            .mul_div_floor(info.rate() as u64, old_rate)
            .unwrap();

        let old_sample_stop = state.sample_stop;
        let sample_stop =
            old_sample_stop.map(|v| v.mul_div_floor(info.rate() as u64, old_rate).unwrap());

        let accumulator =
            (sample_offset as f64).rem(2.0 * PI * (settings.freq as f64) / (info.rate() as f64));

        *state = State {
            info: Some(info),
            sample_offset: sample_offset,
            sample_stop: sample_stop,
            accumulator: accumulator,
        };

        drop(state);

        let _ = element.post_message(&amp;gst::Message::new_latency().src(Some(element)).build());

        true
    }</pre><p></p>
<p>In here we parse the caps into a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_audio/struct.AudioInfo.html" rel="noopener" target="_top"><i>AudioInfo</i></a> and then store that in our internal state, while updating various fields. We tell the base class about the number of bytes each buffer is usually going to hold, and update our current sample position, the stop sample position (when a seek with stop position happens, we need to know when to stop) and our accumulator. This happens by scaling both positions by the old and new sample rate. If we don’t have an old sample rate, we assume nanoseconds (this will make more sense once seeking is implemented). The scaling is done with the help of the <a href="https://crates.io/crates/muldiv" rel="noopener" target="_top"><i>muldiv</i></a> crate, which implements scaling of integer types by a fraction with protection against overflows by doing up to 128 bit integer arithmetic for intermediate values.</p>
<p>The accumulator is the updated based on the current phase of the sine wave at the current sample position.</p>
<p>As a last step we post a new <i>LATENCY</i> message on the bus whenever the sample rate has changed. Our latency (in live mode) is going to be the duration of a single buffer, but more about that later.</p>
<p>BaseSrc is by default already selecting possible caps for us, if there are multiple options. However these defaults might not be (and often are not) ideal and we should override the default behaviour slightly. This is done in the <i>BaseSrc::fixate</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn fixate(&amp;self, element: &amp;BaseSrc, caps: gst::Caps) -&gt; gst::Caps {
        // Fixate the caps. BaseSrc will do some fixation for us, but
        // as we allow any rate between 1 and MAX it would fixate to 1. 1Hz
        // is generally not a useful sample rate.
        //
        // We fixate to the closest integer value to 48kHz that is possible
        // here, and for good measure also decide that the closest value to 1
        // channel is good.
        let mut caps = gst::Caps::truncate(caps);
        {
            let caps = caps.make_mut();
            let s = caps.get_mut_structure(0).unwrap();
            s.fixate_field_nearest_int("rate", 48_000);
            s.fixate_field_nearest_int("channels", 1);
        }

        // Let BaseSrc fixate anything else for us. We could've alternatively have
        // called Caps::fixate() here
        element.parent_fixate(caps)
    }</pre><p></p>
<p>Here we take the caps that are passed in, truncate them (i.e. remove all but the very first <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/structure/struct.Structure.html" rel="noopener" target="_top"><i>Structure</i></a>) and then manually fixate the sample rate to the closest value to 48kHz. By default, caps fixation would result in the lowest possible sample rate but this is usually not desired.</p>
<p>For good measure, we also fixate the number of channels to the closest value to 1, but this would already be the default behaviour anyway. And then chain up to the parent class’ implementation of <i>fixate</i>, which for now basically does the same as <i>Caps::fixate()</i>. After this, the caps are fixated, i.e. there is only a single <i>Structure</i> left and all fields have concrete values (no ranges or sets).</p>
<h3 id="query-handling">Query Handling</h3>
<p>As our source element will work by generating a new audio buffer from a specific offset, and especially works in <i>Time</i> format, we want to notify downstream elements that we don’t want to run in <i>Pull</i> mode, only in <i>Push</i> mode. In addition would prefer sequential reading. However we still allow seeking later. For a source that does not know about <i>Time</i>, e.g. a file source, the format would be configured as <i>Bytes</i>. Other values than <i>Time</i> and <i>Bytes</i> generally don’t make any sense.</p>
<p>The main difference here is that otherwise the base class would ask us to produce data for arbitrary <i>Byte</i> offsets, and we would have to produce data for that. While possible in our case, it’s a bit annoying and for other audio sources it’s not easily possible at all.</p>
<p>Downstream elements will try to query this very information from us, so we now have to override the default query handling of <i>BaseSrc</i> and handle the <i>SCHEDULING</i> query differently. Later we will also handle other queries differently.</p>
<p></p><pre class="crayon-plain-tag">fn query(&amp;self, element: &amp;BaseSrc, query: &amp;mut gst::QueryRef) -&gt; bool {
        use gst::QueryView;

        match query.view_mut() {
            // We only work in Push mode. In Pull mode, create() could be called with
            // arbitrary offsets and we would have to produce for that specific offset
            QueryView::Scheduling(ref mut q) =&gt; {
                q.set(gst::SchedulingFlags::SEQUENTIAL, 1, -1, 0);
                q.add_scheduling_modes(&amp;[gst::PadMode::Push]);
                return true;
            }
            _ =&gt; (),
        }
        BaseSrcBase::parent_query(element, query)
    }</pre><p></p>
<p>To handle the <i>SCHEDULING</i> query specifically, we first have to match on a view (mutable because we want to modify the view) of the query check the type of the query. If it indeed is a scheduling query, we can set the <i>SEQUENTIAL</i> flag and specify that we handle only <i>Push</i> mode, then return <i>true</i> directly as we handled the query already.</p>
<p>In all other cases we fall back to the parent class’ implementation of the <i>query</i> virtual method.</p>
<h3 id="buffer-creation">Buffer Creation</h3>
<p>Now we have everything in place for a working element, apart from the virtual method to actually generate the raw audio buffers with the sine wave. From a high-level <i>BaseSrc</i> works by calling the <i>create</i> virtual method over and over again to let the subclass produce a buffer until it returns an error or signals the end of the stream.</p>
<p>Let’s first talk about how to generate the sine wave samples themselves. As we want to operate on 32 bit and 64 bit floating point numbers, we implement a generic function for generating samples and storing them in a mutable byte slice. This is done with the help of the <a href="https://crates.io/crates/num-traits" rel="noopener" target="_top"><i>num_traits</i></a> crate, which provides all kinds of useful traits for abstracting over numeric types. In our case we only need the <a href="https://docs.rs/num-traits/0.2.0/num_traits/float/trait.Float.html" rel="noopener" target="_top"><i>Float</i></a> and <a href="https://docs.rs/num-traits/0.2.0/num_traits/cast/trait.NumCast.html" rel="noopener" target="_top"><i>NumCast</i></a> traits.</p>
<p>Instead of writing a generic implementation with those traits, it would also be possible to do the same with a simple macro that generates a function for both types. Which approach is nicer is a matter of taste in the end, the compiler output should be equivalent for both cases.</p>
<p></p><pre class="crayon-plain-tag">fn process(
        data: &amp;mut [u8],
        accumulator_ref: &amp;mut f64,
        freq: u32,
        rate: u32,
        channels: u32,
        vol: f64,
    ) {
        use std::f64::consts::PI;

        // Reinterpret our byte-slice as a slice containing elements of the type
        // we're interested in. GStreamer requires for raw audio that the alignment
        // of memory is correct, so this will never ever fail unless there is an
        // actual bug elsewhere.
        let data = data.as_mut_slice_of::().unwrap();

        // Convert all our parameters to the target type for calculations
        let vol: F = NumCast::from(vol).unwrap();
        let freq = freq as f64;
        let rate = rate as f64;
        let two_pi = 2.0 * PI;

        // We're carrying a accumulator with up to 2pi around instead of working
        // on the sample offset. High sample offsets cause too much inaccuracy when
        // converted to floating point numbers and then iterated over in 1-steps
        let mut accumulator = *accumulator_ref;
        let step = two_pi * freq / rate;

        for chunk in data.chunks_mut(channels as usize) {
            let value = vol * F::sin(NumCast::from(accumulator).unwrap());
            for sample in chunk {
                *sample = value;
            }

            accumulator += step;
            if accumulator &gt;= two_pi {
                accumulator -= two_pi;
            }
        }

        *accumulator_ref = accumulator;
    }</pre><p></p>
<p>This function takes the mutable byte slice from our buffer as argument, as well as the current value of the accumulator and the relevant settings for generating the sine wave.</p>
<p>As a first step, we “cast” the byte slice to one of the target type (f32 or f64) with the help of the <a href="https://crates.io/crates/byte-slice-cast" rel="noopener" target="_top"><i>byte_slice_cast</i></a> crate. This ensures that alignment and sizes are all matching and returns a mutable slice of our target type if successful. In case of GStreamer, the buffer alignment is guaranteed to be big enough for our types here and we allocate the buffer of a correct size later.</p>
<p>Now we convert all the parameters to the types we will use later, and store them together with the current accumulator value in local variables. Then we iterate over the whole floating point number slice in chunks with all channels, and fill each channel with the current value of our sine wave.</p>
<p>The sine wave itself is calculated by <i>val = volume * sin(2 * PI * frequency * (i + accumulator) / rate)</i>, but we actually calculate it by simply increasing the accumulator by <i>2 * PI * frequency / rate</i> for every sample instead of doing the multiplication for each sample. We also make sure that the accumulator always stays between <i>0</i> and <i>2 * PI</i> to prevent any inaccuracies from floating point numbers to affect our produced samples.</p>
<p>Now that this is done, we need to implement the <i>BaseSrc::create</i> virtual method for actually allocating the buffer, setting timestamps and other metadata and it and calling our above function.</p>
<p></p><pre class="crayon-plain-tag">fn create(
        &amp;self,
        element: &amp;BaseSrc,
        _offset: u64,
        _length: u32,
    ) -&gt; Result {
        // Keep a local copy of the values of all our properties at this very moment. This
        // ensures that the mutex is never locked for long and the application wouldn't
        // have to block until this function returns when getting/setting property values
        let settings = *self.settings.lock().unwrap();

        // Get a locked reference to our state, i.e. the input and output AudioInfo
        let mut state = self.state.lock().unwrap();
        let info = match state.info {
            None =&gt; {
                gst_element_error!(element, gst::CoreError::Negotiation, ["Have no caps yet"]);
                return Err(gst::FlowReturn::NotNegotiated);
            }
            Some(ref info) =&gt; info.clone(),
        };

        // If a stop position is set (from a seek), only produce samples up to that
        // point but at most samples_per_buffer samples per buffer
        let n_samples = if let Some(sample_stop) = state.sample_stop {
            if sample_stop = state.sample_offset {
                gst_log!(self.cat, obj: element, "At EOS");
                return Err(gst::FlowReturn::Eos);
            }

            sample_stop - state.sample_offset
        } else {
            settings.samples_per_buffer as u64
        };

        // Allocate a new buffer of the required size, update the metadata with the
        // current timestamp and duration and then fill it according to the current
        // caps
        let mut buffer =
            gst::Buffer::with_size((n_samples as usize) * (info.bpf() as usize)).unwrap();
        {
            let buffer = buffer.get_mut().unwrap();

            // Calculate the current timestamp (PTS) and the next one,
            // and calculate the duration from the difference instead of
            // simply the number of samples to prevent rounding errors
            let pts = state
                .sample_offset
                .mul_div_floor(gst::SECOND_VAL, info.rate() as u64)
                .unwrap()
                .into();
            let next_pts: gst::ClockTime = (state.sample_offset + n_samples)
                .mul_div_floor(gst::SECOND_VAL, info.rate() as u64)
                .unwrap()
                .into();
            buffer.set_pts(pts);
            buffer.set_duration(next_pts - pts);

            // Map the buffer writable and create the actual samples
            let mut map = buffer.map_writable().unwrap();
            let data = map.as_mut_slice();

            if info.format() == gst_audio::AUDIO_FORMAT_F32 {
                Self::process::(
                    data,
                    &amp;mut state.accumulator,
                    settings.freq,
                    info.rate(),
                    info.channels(),
                    settings.volume,
                );
            } else {
                Self::process::(
                    data,
                    &amp;mut state.accumulator,
                    settings.freq,
                    info.rate(),
                    info.channels(),
                    settings.volume,
                );
            }
        }
        state.sample_offset += n_samples;
        drop(state);

        gst_debug!(self.cat, obj: element, "Produced buffer {:?}", buffer);

        Ok(buffer)
    }</pre><p></p>
<p>Just like last time, we start with creating a copy of our properties (settings) and keeping a mutex guard of the internal state around. If the internal state has no <i>AudioInfo</i> yet, we error out. This would mean that no caps were negotiated yet, which is something we can’t handle and is not really possible in our case.</p>
<p>Next we calculate how many samples we have to generate. If a sample stop position was set by a seek event, we have to generate samples up to at most that point. Otherwise we create at most the number of samples per buffer that were set via the property. Then we allocate a buffer of the corresponding size, with the help of the <i>bpf</i> field of the <i>AudioInfo</i>, and then set its metadata and fill the samples.</p>
<p>The metadata that is set is the timestamp (PTS), and the duration. The duration is calculated from the difference of the following buffer’s timestamp and the current buffer’s. By this we ensure that rounding errors are not causing the next buffer’s timestamp to have a different timestamp than the sum of the current’s and its duration. While this would not be much of a problem in GStreamer (inaccurate and jitterish timestamps are handled just fine), we can prevent it here and do so.</p>
<p>Afterwards we call our previously defined function on the writably mapped buffer and fill it with the sample values.</p>
<p>With all this, the element should already work just fine in any GStreamer-based application, for example <i>gst-launch-1.0</i>. Don’t forget to set the <i>GST_PLUGIN_PATH</i> environment variable correctly like last time. Before running this, make sure to turn down the volume of your speakers/headphones a bit.</p>
<p></p><pre class="crayon-plain-tag">export GST_PLUGIN_PATH=`pwd`/target/debug
gst-launch-1.0 rssinesrc freq=440 volume=0.9 ! audioconvert ! autoaudiosink</pre><p></p>
<p>You should hear a 440Hz sine wave now.</p>
<h3 id="live">(Pseudo) Live Mode</h3>
<p>Many audio (and video) sources can actually only produce data in real-time and data is produced according to some clock. So far our source element can produce data as fast as downstream is consuming data, but we optionally can change that. We simulate a live source here now by waiting on the pipeline clock, but with a real live source you would only ever be able to have the data in real-time without any need to wait on a clock. And usually that data is produced according to a different clock than the pipeline clock, in which case translation between the two clocks is needed but we ignore this aspect for now. For details check the <a href="https://gstreamer.freedesktop.org/documentation/application-development/advanced/clocks.html" rel="noopener" target="_top">GStreamer documentation</a>.</p>
<p>For working in live mode, we have to add a few different parts in various places. First of all, we implement waiting on the clock in the <i>create</i> function.</p>
<p></p><pre class="crayon-plain-tag">fn create(...
        [...]
        state.sample_offset += n_samples;
        drop(state);

        // If we're live, we are waiting until the time of the last sample in our buffer has
        // arrived. This is the very reason why we have to report that much latency.
        // A real live-source would of course only allow us to have the data available after
        // that latency, e.g. when capturing from a microphone, and no waiting from our side
        // would be necessary..
        //
        // Waiting happens based on the pipeline clock, which means that a real live source
        // with its own clock would require various translations between the two clocks.
        // This is out of scope for the tutorial though.
        if element.is_live() {
            let clock = match element.get_clock() {
                None =&gt; return Ok(buffer),
                Some(clock) =&gt; clock,
            };

            let segment = element
                .get_segment()
                .downcast::()
                .unwrap();
            let base_time = element.get_base_time();
            let running_time = segment.to_running_time(buffer.get_pts() + buffer.get_duration());

            // The last sample's clock time is the base time of the element plus the
            // running time of the last sample
            let wait_until = running_time + base_time;
            if wait_until.is_none() {
                return Ok(buffer);
            }

            let id = clock.new_single_shot_id(wait_until).unwrap();

            gst_log!(
                self.cat,
                obj: element,
                "Waiting until {}, now {}",
                wait_until,
                clock.get_time()
            );
            let (res, jitter) = id.wait();
            gst_log!(
                self.cat,
                obj: element,
                "Waited res {:?} jitter {}",
                res,
                jitter
            );
        }

        gst_debug!(self.cat, obj: element, "Produced buffer {:?}", buffer);

        Ok(buffer)
    }</pre><p></p>
<p>To be able to wait on the clock, we first of all need to calculate the clock time until when we want to wait. In our case that will be the clock time right after the end of the last sample in the buffer we just produced. Simply because you can’t capture a sample before it was produced.</p>
<p>We calculate the running time from the PTS and duration of the buffer with the help of the currently configured segment and then add the base time of the element on this to get the clock time as result. Please check the <a href="https://gstreamer.freedesktop.org/documentation/application-development/advanced/clocks.html" rel="noopener" target="_top">GStreamer documentation</a> for details, but in short the running time of a pipeline is the time since the start of the pipeline (or the last reset of the running time) and the running time of a buffer can be calculated from its PTS and the segment, which provides the information to translate between the two. The base time is the clock time when the pipeline went to the <i>Playing</i> state, so just an offset.</p>
<p>Next we wait and then return the buffer as before.</p>
<p>Now we also have to tell the base class that we’re running in live mode now. This is done by calling <i>set_live(true)</i> on the base class before changing the element state from <i>Ready</i> to <i>Paused</i>. For this we override the <i>Element::change_state</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">impl ElementImpl for SineSrc {
    fn change_state(
        &amp;self,
        element: &amp;BaseSrc,
        transition: gst::StateChange,
    ) -&gt; gst::StateChangeReturn {
        // Configure live'ness once here just before starting the source
        match transition {
            gst::StateChange::ReadyToPaused =&gt; {
                element.set_live(self.settings.lock().unwrap().is_live);
            }
            _ =&gt; (),
        }

        element.parent_change_state(transition)
    }
}</pre><p></p>
<p>And as a last step, we also need to notify downstream elements about our <a href="https://gstreamer.freedesktop.org/documentation/application-development/advanced/clocks.html#latency" rel="noopener" target="_top">latency</a>. Live elements always have to report their latency so that synchronization can work correctly. As the clock time of each buffer is equal to the time when it was created, all buffers would otherwise arrive late in the sinks (they would appear as if they should’ve been played already at the time when they were created). So all the sinks will have to compensate for the latency that it took from capturing to the sink, and they have to do that in a coordinated way (otherwise audio and video would be out of sync if both have different latencies). For this the pipeline is querying each sink for the latency on its own branch, and then configures a global latency on all sinks according to that.</p>
<p>This querying is done with the <i>LATENCY</i> query, which we will now also have to handle.</p>
<p></p><pre class="crayon-plain-tag">fn query(&amp;self, element: &amp;BaseSrc, query: &amp;mut gst::QueryRef) -&gt; bool {
        use gst::QueryView;

        match query.view_mut() {
            // We only work in Push mode. In Pull mode, create() could be called with
            // arbitrary offsets and we would have to produce for that specific offset
            QueryView::Scheduling(ref mut q) =&gt; {
                [...]
            }
            // In Live mode we will have a latency equal to the number of samples in each buffer.
            // We can't output samples before they were produced, and the last sample of a buffer
            // is produced that much after the beginning, leading to this latency calculation
            QueryView::Latency(ref mut q) =&gt; {
                let settings = *self.settings.lock().unwrap();
                let state = self.state.lock().unwrap();

                if let Some(ref info) = state.info {
                    let latency = gst::SECOND
                        .mul_div_floor(settings.samples_per_buffer as u64, info.rate() as u64)
                        .unwrap();
                    gst_debug!(self.cat, obj: element, "Returning latency {}", latency);
                    q.set(settings.is_live, latency, gst::CLOCK_TIME_NONE);
                    return true;
                } else {
                    return false;
                }
            }
            _ =&gt; (),
        }
        BaseSrcBase::parent_query(element, query)
    }</pre><p></p>
<p>The latency that we report is the duration of a single audio buffer, because we’re simulating a real live source here. A real live source won’t be able to output the buffer before the last sample of it is captured, and the difference between when the first and last sample were captured is exactly the latency that we add here. Other elements further downstream that introduce further latency would then add their own latency on top of this.</p>
<p>Inside the latency query we also signal that we are indeed a live source, and additionally how much buffering we can do (in our case, infinite) until data would be lost. The last part is important if e.g. the video branch has a higher latency, causing the audio sink to have to wait some additional time (so that audio and video stay in sync), which would then require the whole audio branch to buffer some data. As we have an artificial live source, we can always generate data for the next time but a real live source would only have a limited buffer and if no data is read and forwarded once that runs full, data would get lost.</p>
<p>You can test this again with e.g. <i>gst-launch-1.0</i> by setting the <i>is-live</i> property to true. It should write in the output now that the pipeline is live.</p>
<p>In Mathieu’s blog post this was implemented without explicit waiting and the usage of the <i>get_times</i> virtual method, but as this is only really useful for pseudo live sources like this one I decided to explain how waiting on the clock can be achieved correctly and even more important how that relates to the next section.</p>
<h3 id="unlock">Unlocking</h3>
<p>With the addition of the live mode, the <i>create</i> function is now blocking and waiting on the clock for some time. This is suboptimal as for example a (flushing) seek would have to wait now until the clock waiting is done, or when shutting down the application would have to wait.</p>
<p>To prevent this, all waiting/blocking in GStreamer streaming threads should be interruptible/cancellable when requested. And for example the <i>ClockID</i> that we got from the clock for waiting can be cancelled by calling <i>unschedule()</i> on it. We only have to do it from the right place and keep it accessible. The right place is the <i>BaseSrc::unlock</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">struct ClockWait {
    clock_id: Option,
    flushing: bool,
}

struct SineSrc {
    cat: gst::DebugCategory,
    settings: Mutex,
    state: Mutex,
    clock_wait: Mutex,
}

[...]

    fn unlock(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // This should unblock the create() function ASAP, so we
        // just unschedule the clock it here, if any.
        gst_debug!(self.cat, obj: element, "Unlocking");
        let mut clock_wait = self.clock_wait.lock().unwrap();
        if let Some(clock_id) = clock_wait.clock_id.take() {
            clock_id.unschedule();
        }
        clock_wait.flushing = true;

        true
    }</pre><p></p>
<p>We store the clock ID in our struct, together with a boolean to signal whether we’re supposed to flush already or not. And then inside <i>unlock</i> unschedule the clock ID and set this boolean flag to true.</p>
<p>Once everything is unlocked, we need to reset things again so that data flow can happen in the future. This is done in the <i>unlock_stop</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn unlock_stop(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // This signals that unlocking is done, so we can reset
        // all values again.
        gst_debug!(self.cat, obj: element, "Unlock stop");
        let mut clock_wait = self.clock_wait.lock().unwrap();
        clock_wait.flushing = false;

        true
    }</pre><p></p>
<p>To make sure that this struct is always initialized correctly, we also call <i>unlock</i> from <i>stop</i>, and <i>unlock_stop</i> from <i>start</i>.</p>
<p>Now as a last step, we need to actually make use of the new struct we added around the code where we wait for the clock.</p>
<p></p><pre class="crayon-plain-tag">// Store the clock ID in our struct unless we're flushing anyway.
            // This allows to asynchronously cancel the waiting from unlock()
            // so that we immediately stop waiting on e.g. shutdown.
            let mut clock_wait = self.clock_wait.lock().unwrap();
            if clock_wait.flushing {
                gst_debug!(self.cat, obj: element, "Flushing");
                return Err(gst::FlowReturn::Flushing);
            }

            let id = clock.new_single_shot_id(wait_until).unwrap();
            clock_wait.clock_id = Some(id.clone());
            drop(clock_wait);

            gst_log!(
                self.cat,
                obj: element,
                "Waiting until {}, now {}",
                wait_until,
                clock.get_time()
            );
            let (res, jitter) = id.wait();
            gst_log!(
                self.cat,
                obj: element,
                "Waited res {:?} jitter {}",
                res,
                jitter
            );
            self.clock_wait.lock().unwrap().clock_id.take();

            // If the clock ID was unscheduled, unlock() was called
            // and we should return Flushing immediately.
            if res == gst::ClockReturn::Unscheduled {
                gst_debug!(self.cat, obj: element, "Flushing");
                return Err(gst::FlowReturn::Flushing);
            }</pre><p></p>
<p>The important part in this code is that we first have to check if we are already supposed to unlock, before even starting to wait. Otherwise we would start waiting without anybody ever being able to unlock. Then we need to store the clock id in the struct and make sure to drop the mutex guard so that the <i>unlock</i> function can take it again for unscheduling the clock ID. And once waiting is done, we need to remove the clock id from the struct again and in case of <i>ClockReturn::Unscheduled</i> we directly return <i>FlowReturn::Flushing</i> instead of the error.</p>
<p>Similarly when using other blocking APIs it is important that they are woken up in a similar way when <i>unlock</i> is called. Otherwise the application developer’s and thus user experience will be far from ideal.</p>
<h3 id="seeking">Seeking</h3>
<p>As a last feature we implement seeking on our source element. In our case that only means that we have to update the <i>sample_offset</i> and <i>sample_stop</i> fields accordingly, other sources might have to do more work than that.</p>
<p>Seeking is implemented in the <i>BaseSrc::do_seek</i> virtual method, and signalling whether we can actually seek in the <i>is_seekable</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn is_seekable(&amp;self, _element: &amp;BaseSrc) -&gt; bool {
        true
    }

    fn do_seek(&amp;self, element: &amp;BaseSrc, segment: &amp;mut gst::Segment) -&gt; bool {
        // Handle seeking here. For Time and Default (sample offset) seeks we can
        // do something and have to update our sample offset and accumulator accordingly.
        //
        // Also we should remember the stop time (so we can stop at that point), and if
        // reverse playback is requested. These values will all be used during buffer creation
        // and for calculating the timestamps, etc.

        if segment.get_rate()  0.0 {
            gst_error!(self.cat, obj: element, "Reverse playback not supported");
            return false;
        }

        let settings = *self.settings.lock().unwrap();
        let mut state = self.state.lock().unwrap();

        // We store sample_offset and sample_stop in nanoseconds if we
        // don't know any sample rate yet. It will be converted correctly
        // once a sample rate is known.
        let rate = match state.info {
            None =&gt; gst::SECOND_VAL,
            Some(ref info) =&gt; info.rate() as u64,
        };

        if let Some(segment) = segment.downcast_ref::() {
            use std::f64::consts::PI;

            let sample_offset = segment
                .get_start()
                .unwrap()
                .mul_div_floor(rate, gst::SECOND_VAL)
                .unwrap();

            let sample_stop = segment
                .get_stop()
                .map(|v| v.mul_div_floor(rate, gst::SECOND_VAL).unwrap());

            let accumulator =
                (sample_offset as f64).rem(2.0 * PI * (settings.freq as f64) / (rate as f64));

            gst_debug!(
                self.cat,
                obj: element,
                "Seeked to {}-{:?} (accum: {}) for segment {:?}",
                sample_offset,
                sample_stop,
                accumulator,
                segment
            );

            *state = State {
                info: state.info.clone(),
                sample_offset: sample_offset,
                sample_stop: sample_stop,
                accumulator: accumulator,
            };

            true
        } else if let Some(segment) = segment.downcast_ref::() {
            use std::f64::consts::PI;

            if state.info.is_none() {
                gst_error!(
                    self.cat,
                    obj: element,
                    "Can only seek in Default format if sample rate is known"
                );
                return false;
            }

            let sample_offset = segment.get_start().unwrap();
            let sample_stop = segment.get_stop().0;

            let accumulator =
                (sample_offset as f64).rem(2.0 * PI * (settings.freq as f64) / (rate as f64));

            gst_debug!(
                self.cat,
                obj: element,
                "Seeked to {}-{:?} (accum: {}) for segment {:?}",
                sample_offset,
                sample_stop,
                accumulator,
                segment
            );

            *state = State {
                info: state.info.clone(),
                sample_offset: sample_offset,
                sample_stop: sample_stop,
                accumulator: accumulator,
            };

            true
        } else {
            gst_error!(
                self.cat,
                obj: element,
                "Can't seek in format {:?}",
                segment.get_format()
            );

            false
        }
    }</pre><p></p>
<p>Currently no support for reverse playback is implemented here, that is left as an exercise for the reader. So as a first step we check if the segment has a negative rate, in which case we just fail and return false.</p>
<p>Afterwards we again take a copy of the settings, keep a mutable mutex guard of our state and then start handling the actual seek.</p>
<p>If no caps are known yet, i.e. the <i>AudioInfo</i> is <i>None</i>, we assume a rate of 1 billion. That is, we just store the time in nanoseconds for now and let the <i>set_caps</i> function take care of that (which we already implemented accordingly) once the sample rate is known.</p>
<p>Then, if a <i>Time</i> seek is performed, we convert the segment start and stop position from time to sample offsets and save them. And then update the accumulator in a similar way as in the <i>set_caps</i> function. If a seek is in <i>Default</i> format (i.e. sample offsets for raw audio), we just have to store the values and update the accumulator but only do so if the sample rate is known already. A sample offset seek does not make any sense until the sample rate is known, so we just fail here to prevent unexpected surprises later.</p>
<p>Try the following pipeline for testing seeking. You should be able to seek the current time drawn over the video, and with the left/right cursor key you can seek. Also this shows that we create a quite nice sine wave.</p>
<p></p><pre class="crayon-plain-tag">gst-launch-1.0 rssinesrc ! audioconvert ! monoscope ! timeoverlay ! navseek ! glimagesink</pre><p></p>
<p>And with that all features are implemented in our sine wave raw audio source.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://arunraghavan.net/2018/02/applicative-functors-for-fun-and-parsing/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Applicative Functors for Fun and Parsing</span></a><div class="lastUpdated">2018年2月21日 15:24</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p><em>PSA: This post has a bunch of Haskell code, but I’m going to try to make it more broadly accessible. Let’s see how that goes.</em></p>

<p>I’ve been proceeding apace with my 3rd year in <a href="https://abhinavsarkar.net/">Abhinav’s</a> Haskell classes at <a href="https://nilenso.com/">Nilenso</a>, and we just got done with the section on Applicative Functors. I’m at that point when I finally “get” it, so I thought I’d document the process, and maybe capture my a-ha moment of Applicatives.</p>

<p>I should point out that the ideas and approach in this post are all based on Abhinav’s class material (and I’ve found them really effective in understanding the underlying concepts). Many thanks are due to him, and any lack of clarity you find ahead is in my own understanding.</p>

<h3>Functors and Applicatives</h3>

<p>Functors represent a type or a context on which we can meaningfully apply (map) a function. The <code>Functor</code> typeclass is pretty straightforward:</p>

<p></p><pre class="crayon-plain-tag">class Functor f where
  fmap :: (a -&gt; b) -&gt; f a -&gt; f b</pre><p></p>

<p>Easy enough. <code>fmap</code> takes a function that transforms something of type <code>a</code> to type <code>b</code> and a value of type <code>a</code> in a context <code>f</code>. It produces a value of type <code>b</code> in the same context.</p>

<p>The <code>Applicative</code> typeclass adds two things to <code>Functor</code>. Firstly, it gives us a means of <em>putting things inside a context</em> (also called lifting). The second is to <em>apply a function within a context</em>.</p>

<p></p><pre class="crayon-plain-tag">class Functor f =&gt; Applicative f where
  pure :: a -&gt; f a
  (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b</pre><p></p>

<p>We can see <code>pure</code> <em>lifts</em> a given value into a context. The apply function (<code>&lt;*&gt;</code>) intuitively looks like <code>fmap</code>, with the difference that the function is within a context. This becomes key when we remember that Haskell functions are curried (and can thus be partially applied). This would then allow us to write something like:</p>

<p></p><pre class="crayon-plain-tag">maybeAdd :: Maybe Int -&gt; Maybe Int -&gt; Maybe Int
maybeAdd ma mb = pure (+) &lt;*&gt; ma &lt;*&gt; mb</pre><p></p>

<p>This function takes two numbers in the <code>Maybe</code> context (that is, they either exist, or are <code>Nothing</code>), and adds them. The result will be the sum if both numbers exist, or <code>Nothing</code> if either or both do not.</p>

<p>Go ahead and convince yourself that it is painful to express this generically with just <code>fmap</code>.</p>

<h3>Parsers</h3>

<p>There are many ways of looking at what a parser is. Let’s work with one definition: A parser,</p>

<ul>
<li>Takes some input</li>
<li>Converts some or all of it into something else if it can</li>
<li>Returns whatever input was not used in the conversion</li>
</ul>

<p>How do we represent something that converts something to something else? It’s a <em>function</em>, of course. Let’s write that down as a type:</p>

<p></p><pre class="crayon-plain-tag">newtype Parser i o = Parser (i -&gt; (Maybe o, i))</pre><p></p>

<p>This more or less directly maps to what we just said. A <code>Parser</code> is a data type which has two type parameters — an input type and an output type. It contains a function that takes one argument of the input type, and produces a tuple of <code>Maybe</code> the output type (signifying if parsing succeeded) and the rest of the input.</p>

<p>We can name the field <code>runParser</code>, so it becomes easier to get a hold of the function inside our <code>Parser</code> type:</p>

<p></p><pre class="crayon-plain-tag">newtype Parser i o = Parser { runParser :: i -&gt; (Maybe o, i) }</pre><p></p>

<h4>Parser combinators</h4>

<p>The “rest” part is important for the reason that we would like to be able to chain small parsers together to make bigger parsers. We do this using “parser combinators” — functions that take one or more parsers and return a more complex parser formed by combining them in some way. We’ll see some of those ways as we go along.</p>

<h3>Parser instances</h3>

<p>Before we proceed, let’s define <code>Functor</code> and <code>Applicative</code> instances for our <code>Parser</code> type.</p>

<p></p><pre class="crayon-plain-tag">instance Functor (Parser i) where
  fmap f p = Parser $ \input -&gt;
    let (mo, i) = runParser p input
    in (f &lt;$&gt; mo, i)</pre><p></p>

<p>The intuition here is clear — if I have a parser that takes some input and provides some output, <code>fmap</code>ing a function on that parser translates to applying that function on the <em>output</em> of the parser.</p>

<p></p><pre class="crayon-plain-tag">instance Applicative (Parser i) where
  pure x = Parser $ \input -&gt; (Just x, input)

  pf &lt;*&gt; po = Parser $ \input -&gt;
    case runParser pf input of
         (Just f, rest) -&gt; case runParser po rest of
                                (Just o, rest') -&gt; (Just (f o), rest')
                                (Nothing, _)    -&gt; (Nothing, input)
         (Nothing, _)   -&gt; (Nothing, input)</pre><p></p>

<p>The Applicative instance is a bit more involved than Functor. What we’re doing first is “running” the first parser which gives us the function we want to apply (remember that this is a curried function, so rather than parsing out a function, we are most likely parsing out a value and creating a function with that). If we succeed, then we run the second parser to get a value to apply the function to. If this is also successful, we apply the function to the value, and return the result within the parser context (i.e. the result, and the rest of the input).</p>

<h3>Implementing some parsers</h3>

<p>Now let’s take our new data type and instances for a spin. Before we write a real parser, let’s write a helper function. A common theme while parsing a string is to match a single character on a predicate — for example, “is this character an alphabet”, or “is this character a semi-colon”. We write a function to take a predicate and return the corresponding parser:</p>

<p></p><pre class="crayon-plain-tag">satisfy :: (Char -&gt; Bool) -&gt; Parser String Char
satisfy p = Parser $ \input -&gt;
  case input of
       (c:cs) | p c -&gt; (Just c, cs)
       _            -&gt; (Nothing, input)</pre><p></p>

<p>Now let’s try to make a parser that takes a string, and if it finds a ASCII digit character, provides the corresponding integer value. We have a function from the <code>Data.Char</code> module to match ASCII digit characters — <code>isDigit</code>. We also have a function to take a digit character and give us an integer — <code>digitToInt</code>. Putting this together with <code>satisfy</code> above.</p>

<p></p><pre class="crayon-plain-tag">import Data.Char (digitToInt, isDigit)

digit :: Parser String Int
digit = digitToInt &lt;$&gt; satisfy isDigit</pre><p></p>

<p>And that’s it! Note how we used our higher-order <code>satisfy</code> function to match a ASCII digit character and the <code>Functor</code> instance to apply <code>digitToInt</code> to the <em>result</em> of that parser (reminder: <code>$&gt;</code> is just the infix form of writing <code>fmap</code> — this is the same as <code>fmap digitToInt (satisfy digit)</code>.</p>

<p>Another example — a character parser, which succeeds if the next character in the input is a specific character we choose.</p>

<p></p><pre class="crayon-plain-tag">char :: Char -&gt; Parser String Char
char x = satisfy (x ==)</pre><p></p>

<p>Once again, the <code>satisfy</code> function makes this a breeze. I must say  I’m pleased with the conciseness of this.</p>

<p>Finally, let’s combine character parsers to create a word parser — a parser that succeeds if the input is a given word.</p>

<p></p><pre class="crayon-plain-tag">word :: String -&gt; Parser String String
word ""     = Parser $ \input -&gt; (Just "", input)
word (c:cs) = (:) &lt;$&gt; char c &lt;*&gt; word cs</pre><p></p>

<p>A match on an empty word always succeeds. For anything else, we just break down the parser to a character parser of the first character and a recursive call to the word parser for the rest. Again, note the use of the Functor and Applicative instance. Let’s look at the type signature of the <code>(:)</code> (list cons) function, which prepends an element to a list:</p>

<p></p><pre class="crayon-plain-tag">(:) :: a -&gt; [a] -&gt; [a]</pre><p></p>

<p>The function takes two arguments — a single element of type <code>a</code>, and a list of elements of type <code>a</code>. If we expand the types some more, we’ll see that the first argument we give it is a <code>Parser String Char</code> and the second is a <code>Parser String [Char]</code> (String is just an alias for <code>[Char]</code>).</p>

<p>In this way we are able to take the basic list prepend function and use it to construct a list of characters <strong>within the Parser context.</strong> (a-ha!?)</p>

<h3>JSON</h3>

<p><acronym title="JavaScript Object Notation">JSON</acronym> is a relatively simple format to parse, and makes for a good example for building a parser. The <a href="http://json.org/">JSON website</a> has a couple of good depictions of the JSON language grammar front and center.</p>

<p>So that defines our parser problem  then — we want to read a string input, and convert it into some sort of in-memory representation of the JSON value. Let’s see what that would look like in Haskell.</p>

<p></p><pre class="crayon-plain-tag">data JsonValue = JsonString String
               | JsonNumber JsonNum
               | JsonObject [(String, JsonValue)]
               | JsonArray [JsonValue]
               | JsonBool Bool
               | JsonNull

-- We represent a number as an infinite precision
-- floating point number with a base 10 exponent
data JsonNum = JsonNum { negative :: Bool
                       , signif   :: Integer
                       , expo     :: Integer
                       }</pre><p></p>

<p>The JSON specification does not really tell us what type to use for numbers. We could just use a <code>Double</code>, but to make things interesting, we represent it as an arbitrary precision floating point number.</p>

<p>Note that the <code>JsonArray</code> and <code>JsonObject</code> constructors are recursive, as they should be — a JSON array is an array of JSON values, and a JSON object is a mapping from string keys to JSON values.</p>

<h3>Parsing JSON</h3>

<p>We now have the pieces we need to start parsing JSON. Let’s start with the easy bits.</p>

<h4>null</h4>

<p>To parse a <code>null</code> we literally just look for the word “null”.</p>

<p></p><pre class="crayon-plain-tag">jsonNull :: Parser String JsonValue
jsonNull = word "null" $&gt; JsonNull</pre><p></p>

<p>The <code>$&gt;</code> operator is a flipped shortcut for <code>fmap . const</code> — it evaluates the argument on the left, and then <code>fmap</code>s the argument on the right onto it. If the <code>word "null"</code> parser is successful (<code>Just "null"</code>), we’ll <code>fmap</code> the <code>JsonValue</code> representing <code>null</code> to replace the string <code>"null"</code> (i.e. we’ll get a <code>(Just JsonNull, &lt;rest of the input&gt;)</code>).</p>

<h4>true and false</h4>

<p>First a quick detour:</p>

<p></p><pre class="crayon-plain-tag">instance Alternative (Parser i) where
  empty = Parser $ \input -&gt; (Nothing, input)
  p1 &lt;|&gt; p2 = Parser $ \input -&gt;
      case runParser p1 input of
           (Nothing, _) -&gt; case runParser p2 input of
                                (Nothing, _) -&gt; (Nothing, input)
                                justValue    -&gt; justValue
           justValue    -&gt; justValue</pre><p></p>

<p>The Alternative instance is easy to follow once you understand Applicative. We define an empty parser that matches nothing. Then we define the alternative operator (<code>&lt;|&gt;</code>) as we might intuitively imagine.</p>

<p>We run the parser given as the first argument first, if it succeeds we are done. If it fails, we run the second parser on the whole input again, if it succeeds, we return that value. If both fail, we return <code>Nothing</code>.</p>

<p>Parsing <code>true</code> and <code>false</code> with this in our belt looks like:</p>

<p></p><pre class="crayon-plain-tag">jsonBool :: Parser String JsonValue
jsonBool =  (word "true" $&gt; JsonBool True)
        &lt;|&gt; (word "false" $&gt; JsonBool False)</pre><p></p>

<p>We are easily able express the idea of trying to parse for the string “true”, and if that fails, trying again for the string “false”. If either matches, we have a boolean value, if not, <code>Nothing</code>. Again, nice and concise.</p>

<h4>String</h4>

<p>This is only slightly more complex. We need a couple of helper functions first:</p>

<p></p><pre class="crayon-plain-tag">hexDigit :: Parser String Int
hexDigit = digitToInt &lt;$&gt; satisfy isHexDigit

digitsToNumber :: Int -&gt; [Int] -&gt; Integer
digitsToNumber base digits = foldl (\num d -&gt; num * fromIntegral base + fromIntegral d) 0 digits</pre><p></p>

<p><code>hexDigit</code> is easy to follow. It just matches anything from <code>0-9</code> and <code>a-f</code> or <code>A-F</code>.</p>

<p><code>digitsToNumber</code> is a pure function that takes a list of digits, and interprets it as a number in the given base. We do some jumping through hoops with <code>fromIntegral</code> to take <code>Int</code> digits (mapping to a normal word-sized integer) and produce an <code>Integer</code> (arbitrary sized integer).</p>

<p>Now follow along one line at a time:</p>

<p></p><pre class="crayon-plain-tag">jsonString :: Parser String String
jsonString = (char '"' *&gt; many jsonChar &lt;* char '"')
  where
    jsonChar =  satisfy (\c -&gt; not (c == '\"' || c == '\\' || isControl c))
            &lt;|&gt; word "\\\"" $&gt; '"'
            &lt;|&gt; word "\\\\" $&gt; '\\'
            &lt;|&gt; word "\\/"  $&gt; '/'
            &lt;|&gt; word "\\b"  $&gt; '\b'
            &lt;|&gt; word "\\f"  $&gt; '\f'
            &lt;|&gt; word "\\n"  $&gt; '\n'
            &lt;|&gt; word "\\r"  $&gt; '\r'
            &lt;|&gt; word "\\t"  $&gt; '\t'
            &lt;|&gt; chr . fromIntegral . digitsToNumber 16 &lt;$&gt; (word "\\u" *&gt; replicateM 4 hexDigit)</pre><p></p>

<p>A string is a valid JSON character, surrounded by quotes. The <code>*&gt;</code> and <code>&lt;*</code> operators allow us to chain parsers whose output we wish to discard (since the quotes are not part of the actual string itself). The <code>many</code> function comes from the Alternative typeclass. It represents zero or more instances of context. In our case, it tries to match zero or more <code>jsonChar</code> parsers.</p>

<p>So what does <code>jsonChar</code> do? Following the definition of a character in the JSON spec, first we try to match something that is not a quote (<code>"</code>), a backslash (<code>\</code>) or a control character. If that doesn’t match, we try to match the various escape characters that the specification mentions.</p>

<p>Finally, if we get a <code>\u</code> followed by 4 hexadecimal characters, we put them in a list (<code>replicateM 4 hexDigit</code> chains 4 <code>hexDigit</code> parsers and provides the output as a list), convert that list into a base 16 integer (<code>digitsToNumber</code>), and then convert that to a Unicode character (<code>chr</code>).</p>

<p>The order of chaining these parsers does matter for performance. The first parser in our <code>&lt;|&gt;</code> chain is the one that is most likely (most characters are not escaped). This follows from our definition of the Alternative instance. We <em>run</em> the first parser, then the second, and so on. We want this to succeed as early as possible so we don’t run more parsers than necessary.</p>

<h4>Arrays</h4>

<p>Arrays and objects have something in common — they have items which are separated by some value (commas for array values, commas for each key-value pair in an object, and colons separating keys and values). Let’s just factor this commonality out:</p>

<p></p><pre class="crayon-plain-tag">sepBy :: Parser i v -&gt; Parser i s -&gt; Parser i [v]
sepBy v s = (:) &lt;$&gt; v &lt;*&gt; many (s *&gt; v) 
         &lt;|&gt; pure []</pre><p></p>

<p>We take a parser for our values (<code>v</code>), and a parser for our separator (<code>s</code>). We try to parse one or more <code>v</code> separated by <code>s</code>, and or just return an empty list in the parser context if there are none.</p>

<p>Now we write our JSON array parser as:</p>

<p></p><pre class="crayon-plain-tag">jsonArray :: Parser String JsonValue
jsonArray = JsonArray &lt;$&gt; (char '[' *&gt; (json `sepBy` char ',') &lt;* char ']')</pre><p></p>

<p>Nice, that’s really succinct. But wait! What is <code>json</code>?</p>

<h4>Putting it all together</h4>

<p>We know that arrays contain JSON values. And we know how to parse some JSON values. Let’s try to put those together for our recursive definition:</p>

<p></p><pre class="crayon-plain-tag">json :: Parser String JsonValue
json =  jsonNull
    &lt;|&gt; jsonBool
    &lt;|&gt; jsonString
    &lt;|&gt; jsonArray
--  &lt;|&gt; jsonNumber
--  &lt;|&gt; jsonObject</pre><p></p>

<p>And that’s it!</p>

<p>The JSON object and number parsers follow the same pattern. So far we’ve ignored spaces in the input, but those can be consumed and ignored easily enough based on what we’ve learned.</p>

<p>You can find the complete code for this exercise <a href="https://github.com/ford-prefect/haskell-classes/blob/master/year3/json.hs">on Github</a>.</p>

<p>Some examples of what this looks like in the REPL:</p>

<p></p><pre class="crayon-plain-tag">*Json&gt; runParser json "null"
(Just null,"")

*Json&gt; runParser json "true"
(Just true,"")

*Json&gt; runParser json "[null,true,\"hello!\"]"
(Just [null, true, "hello!" ],"")</pre><p></p>

<h3>Concluding thoughts</h3>

<p>If you’ve made it this far, thank you! I realise this is long and somewhat dense, but I am very excited by how elegantly Haskell allows us to express these ideas, using fundamental aspects of its type(class) system.</p>

<p>A nice real world example of how you might use this is the <a href="https://github.com/pcapriotti/optparse-applicative">optparse-applicative</a> package which uses these ideas to greatly simplify the otherwise dreary task of parsing command line arguments.</p>

<p>I hope this post generates at least some of the excitement in you that it has in me. Feel free to leave your comments and thoughts below.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://gstreamer.freedesktop.org/news/#2018-02-16T12:00:00Z"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer 1.13.1 unstable development release</span></a><div class="lastUpdated">2018年2月16日 20:00</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>
The GStreamer team is pleased to announce the first development release in
the unstable 1.13 release series.
</p><p>
The unstable 1.13 release series adds new features on top of the current
stable 1.12 series and is part of the API and ABI-stable 1.x release series
of the GStreamer multimedia framework.
</p><p>
The unstable 1.13 release series is for testing and development purposes in
the lead-up to the stable 1.14 series which is scheduled for release in a
few weeks time. Any newly-added API can still change until that point, although
it is rare for that to happen.
</p><p>
Full release notes will be provided in the near future, highlighting all the
new features, bugfixes, performance optimizations and other important changes.
</p><p>
Packagers: please note that quite a few plugins and libraries have moved
between modules, so please take extra care and make sure inter-module
version dependencies are such that users can only upgrade all modules in
one go, instead of seeing a mix of 1.13 and 1.12 on their system.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided shortly.
</p><p>
Release tarballs can be downloaded directly here:
</p><ul>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.13.1.tar.xz">gstreamer-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.13.1.tar.xz">gst-plugins-base-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.13.1.tar.xz">gst-plugins-good-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.13.1.tar.xz">gst-plugins-ugly-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.13.1.tar.xz">gst-plugins-bad-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.13.1.tar.xz">gst-libav-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.13.1.tar.xz">gst-rtsp-server-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.13.1.tar.xz">gst-python-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.13.1.tar.xz">gst-editing-services-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.13.1.tar.xz">gst-validate-1.13.1.tar.xz</a>,</li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.13.1.tar.xz">gstreamer-vaapi-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.13.1.tar.xz">gst-omx-1.13.1.tar.xz</a></li>
</ul>
        <p></p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://wingolog.org/archives/2018/02/07/design-notes-on-inline-caches-in-guile"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">design notes on inline caches in guile</span></a><div class="lastUpdated">2018年2月7日 23:14</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div><p>Ahoy, programming-language tinkerfolk!  Today's rambling missive chews the gnarly bones of <a href="https://en.wikipedia.org/wiki/Inline_caching">"inline caches"</a>, in general but also with particular respect to the <a href="https://gnu.org/s/guile">Guile</a> implementation of Scheme.  First, a little intro.</p><p><b>inline what?</b></p><p>Inline caches are a language implementation technique used to accelerate polymorphic dispatch.  Let's dive in to that.</p><p>By <i>implementation technique</i>, I mean that the technique applies to the language compiler and runtime, rather than to the semantics of the language itself.  The effects on the language do exist though in an indirect way, in the sense that inline caches can make some operations faster and therefore more common.  Eventually inline caches can affect what users expect out of a language and what kinds of programs they write.</p><p>But I'm getting ahead of myself.  <i>Polymorphic dispatch</i> literally means "choosing based on multiple forms".  Let's say your language has immutable strings -- like Java, Python, or Javascript.  Let's say your language also has operator overloading, and that it uses <tt>+</tt> to concatenate strings.  Well at that point you have a problem -- while you can specify a terse semantics of some core set of operations on strings (win!), you can't choose one representation of strings that will work well for all cases (lose!).  If the user has a workload where they regularly build up strings by concatenating them, you will want to store strings as trees of substrings.  On the other hand if they want to access <s>characters</s>codepoints by index, then you want an array.  But if the codepoints are all below 256, maybe you should represent them as bytes to save space, whereas maybe instead as 4-byte codepoints otherwise?  Or maybe even UTF-8 with a codepoint index side table.</p><p>The right representation (form) of a string depends on the myriad ways that the string might be used.  The <tt>string-append</tt> operation is <i>polymorphic</i>, in the sense that the precise code for the operator depends on the representation of the operands -- despite the fact that the <i>meaning</i> of <tt>string-append</tt> is monomorphic!</p><p>Anyway, that's the problem.  Before inline caches came along, there were two solutions: callouts and open-coding.  Both were bad in similar ways.  A callout is where the compiler generates a call to a generic runtime routine.  The runtime routine will be able to handle all the myriad forms and combination of forms of the operands.  This works fine but can be a bit slow, as all callouts for a given operator (e.g. <tt>string-append</tt>) dispatch to a single routine for the whole program, so they don't get to optimize for any particular call site.</p><p>One tempting thing for compiler writers to do is to effectively inline the <tt>string-append</tt> operation into each of its call sites.  This is "open-coding" (in the terminology of the early Lisp implementations like MACLISP).  The advantage here is that maybe the compiler knows something about one or more of the operands, so it can eliminate some cases, effectively performing some compile-time specialization.  But this is a limited technique; one could argue that the whole point of polymorphism is to allow for generic operations on generic data, so you rarely have compile-time invariants that can allow you to specialize.  Open-coding of polymorphic operations instead leads to code bloat, as the <tt>string-append</tt> operation is just so many copies of the same thing.</p><p>Inline caches emerged to solve this problem.  They trace their lineage back to Smalltalk 80, gained in complexity and power with Self and finally reached mass consciousness through Javascript.  These languages all share the characteristic of being dynamically typed and object-oriented.  When a user evaluates a statement like <tt>x = y.z</tt>, the language implementation needs to figure out where <tt>y.z</tt> is actually located.  This location depends on the representation of <tt>y</tt>, which is rarely known at compile-time.</p><p>However for any given reference <tt>y.z</tt> in the source code, there is a finite set of concrete representations of <tt>y</tt> that will actually flow to that call site at run-time.  Inline caches allow the language implementation to specialize the <tt>y.z</tt> access for its particular call site.  For example, at some point in the evaluation of a program, <tt>y</tt> may be seen to have representation R1 or R2.  For R1, the <tt>z</tt> property may be stored at offset 3 within the object's storage, and for R2 it might be at offset 4.  The inline cache is a bit of specialized code that compares the type of the object being accessed against R1 , in that case returning the value at offset 3, otherwise R2 and offset r4, and otherwise falling back to a generic routine.  If this isn't clear to you, Vyacheslav Egorov write a <a href="http://mrale.ph/blog/2012/06/03/explaining-js-vms-in-js-inline-caches.html">fine article describing and implementing the object representation optimizations enabled by inline caches</a>.</p><p>Inline caches also serve as input data to later stages of an adaptive compiler, allowing the compiler to selectively inline (open-code) only those cases that are appropriate to values actually seen at any given call site.</p><p><b>but how?</b></p><p>The classic formulation of inline caches from Self and early V8 actually patched the code being executed.  An inline cache might be allocated at address <tt>0xcabba9e5</tt> and the code emitted for its call-site would be <tt>jmp 0xcabba9e5</tt>.  If the inline cache ended up bottoming out to the generic routine, a new inline cache would be generated that added an implementation appropriate to the newly seen "form" of the operands and the call-site.  Let's say that new IC (inline cache) would have the address <tt>0x900db334</tt>.  Early versions of V8 would actually patch the machine code at the call-site to be <tt>jmp 0x900db334</tt> instead of <tt>jmp 0xcabba6e5</tt>.</p><p>Patching machine code has a number of disadvantages, though.  It inherently target-specific: you will need different strategies to patch x86-64 and armv7 machine code.  It's also expensive: you have to flush the instruction cache after the patch, which slows you down.  That is, of course, if you are allowed to patch executable code; on many systems that's impossible.  Writable machine code is a potential vulnerability if the system may be vulnerable to remote code execution.</p><p>Perhaps worst of all, though, patching machine code is not thread-safe.  In the case of early Javascript, this perhaps wasn't so important; but as JS implementations gained parallel garbage collectors and JS-level parallelism via "service workers", this becomes less acceptable.</p><p>For all of these reasons, the modern take on inline caches is to implement them as a memory location that can be atomically modified.  The call site is just <tt>jmp *<i>loc</i></tt>, as if it were a virtual method call.  Modern CPUs have "branch target buffers" that predict the target of these indirect branches with very high accuracy so that the indirect jump does not become a pipeline stall.  (What does this mean in the face of the Spectre v2 vulnerabilities?  Sadly, God only knows at this point.  Saddest panda.)</p><p><b>cry, the beloved country</b></p><p>I am interested in ICs in the context of the Guile implementation of Scheme, but first I will make a digression.  Scheme is a very monomorphic language.  Yet, this monomorphism is entirely cultural.  It is in no way essential.  Lack of ICs in implementations has actually fed back and encouraged this monomorphism.</p><p>Let us take as an example the case of property access.  If you have a pair in Scheme and you want its first field, you do <tt>(car x)</tt>.  But if you have a vector, you do <tt>(vector-ref x 0)</tt>.</p><p>What's the reason for this nonuniformity?  You could have a generic <tt>ref</tt> procedure, which when invoked as <tt>(ref x 0)</tt> would return the field in <tt>x</tt> associated with 0.  Or <tt>(ref x 'foo)</tt> to return the <tt>foo</tt> property of <tt>x</tt>.  It would be more orthogonal in some ways, and it's completely valid Scheme.</p><p>We don't write Scheme programs this way, though.  From what I can tell, it's for two reasons: one good, and one bad.</p><p>The good reason is that saying <tt>vector-ref</tt> means more to the reader.  You know more about the complexity of the operation and what side effects it might have.  When you call <tt>ref</tt>, who knows?  Using concrete primitives allows for better program analysis and understanding.</p><p>The bad reason is that Scheme implementations, Guile included, tend to compile <tt>(car x)</tt> to much better code than <tt>(ref x 0)</tt>.  Scheme implementations in practice aren't well-equipped for polymorphic data access.  In fact it is standard Scheme practice to abuse the "macro" facility to manually inline code so that that certain performance-sensitive operations get inlined into a closed graph of monomorphic operators with no callouts.  To the extent that this is true, Scheme programmers, Scheme programs, and the Scheme language as a whole are all victims of their implementations.  JavaScript, for example, does not have this problem -- to a small extent, maybe, yes, performance tweaks and tuning are always a thing but JavaScript implementations' ability to burn away polymorphism and abstraction results in an entirely different character in JS programs versus Scheme programs.</p><p><b>it gets worse</b></p><p>On the most basic level, Scheme is the call-by-value lambda calculus.  It's well-studied, well-understood, and eminently flexible.  However the way that the syntax maps to the semantics hides a constrictive monomorphism: that the "callee" of a call refer to a lambda expression.</p><p>Concretely, in an expression like <tt>(a b)</tt>, in which <tt>a</tt> is not a macro, <tt>a</tt> must evaluate to the result of a <tt>lambda</tt> expression.  Perhaps by reference (e.g. <tt>(define a (lambda (x) x))</tt>), perhaps directly; but a lambda nonetheless.  But what if <tt>a</tt> is actually a vector?  At that point the Scheme language standard would declare that to be an error.</p><p>The semantics of Clojure, though, would allow for <tt>((vector 'a 'b 'c) 1)</tt> to evaluate to <tt>b</tt>.  Why not in Scheme?  There are the same good and bad reasons as with <tt>ref</tt>.  Usually, the concerns of the language implementation dominate, regardless of those of the users who generally want to write terse code.  Of course in some cases the implementation concerns <i>should</i> dominate, but not always.  Here, Scheme could be more flexible if it wanted to.</p><p><b>what have you done for me lately</b></p><p>Although inline caches are not a miracle cure for performance overheads of polymorphic dispatch, they are a tool in the box.  But what, precisely, can they do, both in general and for Scheme?</p><p>To my mind, they have five uses.  If you can think of more, please let me know in the comments.</p><p>Firstly, they have the classic named property access optimizations as in JavaScript.  These apply less to Scheme, as we don't have generic property access.  Perhaps this is a deficiency of Scheme, but it's not exactly low-hanging fruit.  Perhaps this would be more interesting if Guile had more generic protocols such as Racket's iteration.</p><p>Next, there are the arithmetic operators: addition, multiplication, and so on.  Scheme's arithmetic is indeed polymorphic; the addition operator <tt>+</tt> can add any number of complex numbers, with a distinction between exact and inexact values.  On a representation level, Guile has fixnums (small exact integers, no heap allocation), bignums (arbitrary-precision heap-allocated exact integers), fractions (exact ratios between integers), flonums (heap-allocated double-precision floating point numbers), and compnums (inexact complex numbers, internally a pair of doubles).  Also in Guile, arithmetic operators are a "primitive generics", meaning that they can be extended to operate on new types at runtime via GOOPS.</p><p>The usual situation though is that any particular instance of an addition operator only sees fixnums.  In that case, it makes sense to only emit code for fixnums, instead of the product of all possible numeric representations.  This is a clear application where inline caches can be interesting to Guile.</p><p>Third, there is a very specific case related to dynamic linking.  Did you know that most programs compiled for GNU/Linux and related systems have inline caches in them?  It's a bit weird but the <a href="https://www.airs.com/blog/archives/41">"Procedure Linkage Table"</a> (PLT) segment in ELF binaries on Linux systems is set up in a way that when e.g. <tt>libfoo.so</tt> is loaded, the dynamic linker usually doesn't eagerly resolve all of the external routines that <tt>libfoo.so</tt> uses.  The first time that <tt>libfoo.so</tt> calls <tt>frobulate</tt>, it ends up calling a procedure that looks up the location of the <tt>frobulate</tt> procedure, then patches the binary code in the PLT so that the next time <tt>frobulate</tt> is called, it dispatches directly.  To dynamic language people it's the weirdest thing in the world that the C/C++/everything-static universe has at its cold, cold heart a hash table and a dynamic dispatch system that it doesn't expose to any kind of user for instrumenting or introspection -- any user that's not a malware author, of course.</p><p>But I digress!  Guile can use ICs to lazily resolve runtime routines used by compiled Scheme code.  But perhaps this isn't optimal, as the set of primitive runtime calls that Guile will embed in its output is finite, and so resolving these routines eagerly would probably be sufficient.  Guile could use ICs for inter-module references as well, and these should indeed be resolved lazily; but I don't know, perhaps the current strategy of using a call-site cache for inter-module references is sufficient.</p><p>Fourthly (are you counting?), there is a general case of the former:  when you see a call <tt>(a b)</tt> and you don't know what <tt>a</tt> is.  If you put an inline cache in the call, instead of having to emit checks that <tt>a</tt> is a heap object and a procedure and then emit an indirect call to the procedure's code, you might be able to emit simply a check that <tt>a</tt> is the same as <tt>x</tt>, the only callee you ever saw at that site, and in that case you can emit a direct branch to the function's code instead of an indirect branch.</p><p>Here I think the argument is less strong.  Modern CPUs are already very good at indirect jumps and well-predicted branches.  The value of a devirtualization pass in compilers is that it makes the side effects of a virtual method call concrete, allowing for more optimizations; avoiding indirect branches is good but not necessary.  On the other hand, Guile does have polymorphic callees (<a href="https://www.gnu.org/software/guile/docs/docs-2.0/guile-ref/Methods-and-Generic-Functions.html#Methods-and-Generic-Functions">generic functions</a>), and call ICs could help there.  Ideally though we would need to extend the language to allow generic functions to feed back to their inline cache handlers.</p><p>Finally, ICs could allow for cheap tracepoints and breakpoints.  If at every breakable location you included a <tt>jmp *<i>loc</i></tt>, and the initial value of <tt>*<i>loc</i></tt> was the next instruction, then you could patch individual locations with code to run there.  The patched code would be responsible for saving and restoring machine state around the instrumentation.</p><p>Honestly I struggle a lot with the idea of debugging native code.  GDB does the least-overhead, most-generic thing, which is patching code directly; but it runs from a separate process, and in Guile we need in-process portable debugging.  The debugging use case is a clear area where you want adaptive optimization, so that you can emit debugging ceremony from the hottest code, knowing that you can fall back on some earlier tier.  Perhaps Guile should bite the bullet and go this way too.</p><p><b>implementation plan</b></p><p>In Guile, monomorphic as it is in most things, probably only arithmetic is worth the trouble of inline caches, at least in the short term.</p><p>Another question is how much to specialize the inline caches to their call site.  On the extreme side, each call site could have a custom calling convention: if the first operand is in register A and the second is in register B and they are expected to be fixnums, and the result goes in register C, and the continuation is the code at L, well then you generate an inline cache that specializes to all of that.  No need to shuffle operands or results, no need to save the continuation (return location) on the stack.</p><p>The opposite would be to call ICs as if their were normal procedures:  shuffle arguments into fixed operand registers, push a stack frame, and when the IC returns, shuffle the result into place.</p><p>Honestly I am looking mostly to the simple solution.  I am concerned about code and heap bloat if I specify to every last detail of a call site.  Also maximum speed comes with an adaptive optimizer, and in that case simple lower tiers are best.</p><p><b>sanity check</b></p><p>To compare these impressions, I took a look at V8's current source code to see where they use ICs in practice.  When I worked on V8, the compiler was entirely different -- there were two tiers, and both of them generated native code.  Inline caches were everywhere, and they were gnarly; every architecture had its own implementation.  Now in V8 there are two tiers, not the same as the old ones, and the lowest one is a bytecode interpreter.</p><p>As an adaptive optimizer, V8 doesn't need breakpoint ICs.  It can always deoptimize back to the interpreter.  In actual practice, to debug at a source location, V8 will patch the bytecode to insert a "DebugBreak" instruction, which has its own support in the interpreter.  V8 also supports optimized compilation of this operation.  So, no ICs needed here.</p><p>Likewise for generic type feedback, V8 records types as data rather than in the classic formulation of inline caches as in Self.  I think WebKit's JavaScriptCore uses a similar strategy.</p><p>V8 does use inline caches for property access (loads and stores).  Besides that there is an inline cache used in calls which is just used to record callee counts, and not used for direct call optimization.</p><p>Surprisingly, V8 doesn't even seem to use inline caches for arithmetic (any more?).  Fair enough, I guess, given that JavaScript's numbers aren't very polymorphic, and even with a system with fixnums and heap floats like V8, floating-point numbers are rare in cold code.</p><p>The dynamic linking and relocation points don't apply to V8 either, as it doesn't receive binary code from the internet; it always starts from source.</p><p><b>twilight of the inline cache</b></p><p>There was a time when inline caches were recommended to solve all your VM problems, but it would seem now that their heyday is past.</p><p>ICs are still a win if you have named property access on objects whose shape you don't know at compile-time.  But improvements in CPU branch target buffers mean that it's no longer imperative to use ICs to avoid indirect branches (modulo Spectre v2), and creating direct branches via code-patching has gotten more expensive and tricky on today's targets with concurrency and deep cache hierarchies.</p><p>Besides that, the type feedback component of inline caches seems to be taken over by explicit data-driven call-site caches, rather than executable inline caches, and the highest-throughput tiers of an adaptive optimizer burn away inline caches anyway.  The pressure on an inline cache infrastructure now is towards simplicity and ease of type and call-count profiling, leaving the speed component to those higher tiers.</p><p>In Guile the bounded polymorphism on arithmetic combined with the need for ahead-of-time compilation means that ICs are probably a code size and execution time win, but it will take some engineering to prevent the calling convention overhead from dominating cost.</p><p>Time to experiment, then -- I'll let y'all know how it goes.  Thoughts and feedback welcome from the compilerati.  Until then, happy hacking :)</p></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://wingolog.org/archives/2018/02/05/notes-from-the-fosdem-2018-networking-devroom"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">notes from the fosdem 2018 networking devroom</span></a><div class="lastUpdated">2018年2月6日 1:22</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div><p>Greetings, internet!</p><p>I am on my way back from <a href="https://fosdem.org/">FOSDEM</a> and thought I would share with yall some impressions from talks in the <a href="https://fosdem.org/2018/schedule/track/sdn_and_nfv/">Networking devroom</a>.  I didn't get to go to all that many talks -- FOSDEM's hallway track is the hottest of them all -- but I did hit a select few.  Thanks to Dave Neary at Red Hat for organizing the room.</p><p><b>Ray Kinsella -- Intel -- <a href="https://fosdem.org/2018/schedule/event/dpdk_microservices/">The path to data-plane micro-services</a></b></p><p>The day started with a drum-beating talk that was very light on technical information.</p><p>Essentially Ray was arguing for an evolution of network function virtualization -- that instead of running VNFs on bare metal as was done in the days of yore, that people started to run them in virtual machines, and now they run them in containers -- what's next?  Ray is saying that "cloud-native VNFs" are the next step.</p><p>Cloud-native VNFs to move from "greedy" VNFs that take charge of the cores that are available to them, to some kind of resource sharing.  "Maybe users value flexibility over performance", says Ray.  It's the Care Bears approach to networking: (resource) sharing is caring.</p><p>In practice he proposed two ways that VNFs can map to cores and cards.</p><p>One was in-process sharing, which if I understood him properly was actually as nodes running within a VPP process.  Basically in this case VPP or DPDK is the scheduler and multiplexes two or more network functions in one process.</p><p>The other was letting Linux schedule separate processes.  In networking, we don't usually do it this way: <a href="https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/doc/performance.md">we run network functions on dedicated cores on which nothing else runs</a>.  Ray was suggesting that perhaps network functions could be more like "normal" Linux services.  Ray doesn't know if Linux scheduling will work in practice.  Also it might mean allowing DPDK to work with 4K pages instead of the 2M hugepages it currently requires.  This obviously has the potential for more latency hazards and would need some tighter engineering, and ultimately would have fewer guarantees than the "greedy" approach.</p><p>Interesting side things I noticed:</p><ul>
<li><p>All the diagrams show Kubernetes managing CPU node allocation and interface assignment.  I guess in marketing diagrams, Kubernetes has completely replaced OpenStack.</p></li>
<li><p>One slide showed guest VNFs differentiated between "virtual network functions" and "socket-based applications", the latter ones being the legacy services that use kernel APIs.  It's a useful terminology difference.</p></li>
<li><p>The talk identifies user-space networking with DPDK (only!).</p></li>
</ul><p>Finally, I note that <a href="https://www.wingolog.org/archives/2015/11/09/embracing-conways-law">Conway's law</a> is obviously reflected in the performance overheads: because there are organizational isolations between dev teams, vendors, and users, there are big technical barriers between them too.  The least-overhead forms of resource sharing are also those with the highest technical consistency and integration (nodes in a single VPP instance).</p><p><b>Magnus Karlsson -- Intel -- <a href="https://fosdem.org/2018/schedule/event/af_xdp/">AF_XDP</a></b></p><p>This was a talk about getting good throughput from the NIC to userspace, but by using some kernel facilities.  The idea is to get the kernel to set up the NIC and virtualize the transmit and receive ring buffers, but to let the NIC's DMA'd packets go directly to userspace.</p><p>The performance goal is 40Gbps for thousand-byte packets, or 25 Gbps for traffic with only the smallest packets (64 bytes).  The fast path does "zero copy" on the packets if the hardware has the capability to steer the subset of traffic associated with the AF_XDP socket to that particular process.</p><p>The AF_XDP project builds on <a href="https://www.iovisor.org/technology/xdp">XDP</a>, a newish thing where a little kind of bytecode can run on the kernel or possibly on the NIC.  One of the bytecode commands (REDIRECT) causes packets to be forwarded to user-space instead of handled by the kernel's otherwise heavyweight networking stack.  AF_XDP is the bridge between XDP on the kernel side and an interface to user-space using sockets (as opposed to e.g. AF_INET).  The performance goal was to be within 10% or so of DPDK's raw user-space-only performance.</p><p>The benefits of AF_XDP over the current situation would be that you have just one device driver, in the kernel, rather than having to have one driver in the kernel (which you have to have anyway) and one in user-space (for speed).  Also, with the kernel involved, there is a possibility for better isolation between different processes or containers, when compared with raw PCI access from user-space..</p><p>AF_XDP is what was previously known as AF_PACKET v4, and its numbers are looking somewhat OK.  Though it's not upstream yet, it might be interesting to get a Snabb driver here.</p><p>I would note that kernel-userspace cooperation is a bit of a theme these days.  There are other points of potential cooperation or common domain sharing, storage being an obvious one.  However I heard more than once this weekend the kind of "I don't know, that area of the kernel has a different culture" sort of concern as that highlighted by Daniel Vetter in his <a href="https://lwn.net/Articles/745817/">recent LCA talk</a>.</p><p><b>François-Frédéric Ozog -- Linaro -- <a href="https://fosdem.org/2018/schedule/event/netmdev/">Userland Network I/O</a></b></p><p>This talk is hard to summarize.  Like the previous one, it's again about getting packets to userspace with some support from the kernel, but the speaker went really deep and I'm not quite sure what in the talk is new and what is known.</p><p>François-Frédéric is working on a new set of abstractions for relating the kernel and user-space.  He works on OpenDataPlane (ODP), which is kinda like DPDK in some ways.  ARM seems to be a big target for his work; that x86-64 is also a target goes without saying.</p><p>His problem statement was, how should we enable fast userland network I/O, without duplicating drivers?</p><p>François-Frédéric was a bit negative on AF_XDP because (he says) it is so focused on packets that it neglects other kinds of devices with similar needs, such as crypto accelerators.  Apparently the challenge here is accelerating a single large IPsec tunnel -- because the cryptographic operations are serialized, you need good single-core performance, and making use of hardware accelerators seems necessary right now for even a single 10Gbps stream.  (If you had many tunnels, you could parallelize, but that's not the case here.)</p><p>He was also a bit skeptical about standardizing on the "packet array I/O model" which AF_XDP and most NICS use.  What he means here is that most current NICs move packets to and from main memory with the help of a "descriptor array" ring buffer that holds pointers to packets.  A transmit array stores packets ready to transmit; a receive array stores maximum-sized packet buffers ready to be filled by the NIC.  The packet data itself is somewhere else in memory; the descriptor only points to it.  When a new packet is received, the NIC fills the corresponding packet buffer and then updates the "descriptor array" to point to the newly available packet.  This requires at least two memory writes from the NIC to memory: at least one to write the packet data (one per 64 bytes of packet data), and one to update the DMA descriptor with the packet length and possible other metadata.</p><p>Although these writes go directly to cache, there's a limit to the number of DMA operations that can happen per second, and with 100Gbps cards, we can't afford to make one such transaction per packet.</p><p>François-Frédéric promoted an alternative I/O model for high-throughput use cases: the "tape I/O model", where packets are just written back-to-back in a uniform array of memory.  Every so often a block of memory containing some number of packets is made available to user-space.  This has the advantage of packing in more packets per memory block, as there's no wasted space between packets.  This increases cache density and decreases DMA transaction count for transferring packet data, as we can use each 64-byte DMA write to its fullest.  Additionally there's no side table of descriptors to update, saving a DMA write there.</p><p>Apparently the only cards currently capable of 100 Gbps traffic, the Chelsio and Netcope cards, use the "tape I/O model".</p><p>Incidentally, the DMA transfer limit isn't the only constraint.  Something I hadn't fully appreciated before was memory write bandwidth.  Before, I had thought that because the NIC would transfer in packet data directly to cache, that this wouldn't necessarily cause any write traffic to RAM.  Apparently that's not the case.  Later over drinks (thanks to Red Hat's networking group for organizing), François-Frédéric asserted that the DMA transfers would eventually use up DDR4 bandwidth as well.</p><p>A NIC-to-RAM DMA transaction will write one cache line (usually 64 bytes) to the socket's last-level cache.  This write will evict whatever was there before.  As far as I can tell, there are three cases of interest here.  The best case is where the evicted cache line is from a previous DMA transfer to the same address.  In that case it's modified in the cache and not yet flushed to main memory, and we can just update the cache instead of flushing to RAM.  (Do I misunderstand the way caches work here?  Do let me know.)</p><p>However if the evicted cache line is from some other address, we might have to flush to RAM if the cache line is dirty.  That causes a memory write traffic.  But if the cache line is clean, that means it was probably loaded as part of a memory read operation, and then that means we're evicting part of the network function's working set, which will later cause memory read traffic as the data gets loaded in again, and write traffic to flush out the DMA'd packet data cache line.</p><p>François-Frédéric simplified the whole thing to equate packet bandwidth with memory write bandwidth, that yes, the packet goes directly to cache but it is also written to RAM.  I can't convince myself that that's the case for all packets, but I need to look more into this.</p><p>Of course the cache pressure and the memory traffic is worse if the packet data is less compact in memory; and worse still if there is any need to copy data.  Ultimately, processing small packets at 100Gbps is still a huge challenge for user-space networking, and it's no wonder that there are only a couple devices on the market that can do it reliably, not that I've seen either of them operate first-hand :)</p><p>Talking with Snabb's Luke Gorrie later on, he thought that it could be that we can still stretch the packet array I/O model for a while, given that PCIe gen4 is coming soon, which will increase the DMA transaction rate.  So that's a possibility to keep in mind.</p><p>At the same time, apparently there are some <a href="https://www.ccixconsortium.com/">"coherent interconnects"</a> coming too which will allow the NIC's memory to be mapped into the "normal" address space available to the CPU.  In this model, instead of having the NIC transfer packets to the CPU, the NIC's memory will be directly addressable from the CPU, as if it were part of RAM.  The latency to pull data in from the NIC to cache is expected to be slightly longer than a RAM access; for comparison, RAM access takes about 70 nanoseconds.</p><p>For a user-space networking workload, coherent interconnects don't change much.  You still need to get the packet data into cache.  True, you do avoid the writeback to main memory, as the packet is already in addressable memory before it's in cache.  But, if it's possible to keep the packet on the NIC -- like maybe you are able to add some kind of inline classifier on the NIC that could directly shunt a packet towards an on-board IPSec accelerator -- in that case you could avoid a lot of memory transfer.  That appears to be the driving factor for coherent interconnects.</p><p>At some point in François-Frédéric's talk, my brain just died.  I didn't quite understand all the complexities that he was taking into account.  Later, after he kindly took the time to dispell some more of my ignorance, I understand more of it, though not yet all :)  The concrete "deliverable" of the talk was a model for kernel modules and user-space drivers that uses the paradigms he was promoting.  It's a work in progress from Linaro's networking group, with some support from NIC vendors and CPU manufacturers.</p><p><b>Luke Gorrie and Asumu Takikawa -- SnabbCo and Igalia -- <a href="https://fosdem.org/2018/schedule/event/lua_snabb/">How to write your own NIC driver, and why</a></b></p><p>This talk had the most magnificent beginning: a sort of "repent now ye sinners" sermon from Luke Gorrie, a seasoned veteran of software networking.  Luke started by describing the path of righteousness leading to "driver heaven", a world in which all vendors have publically accessible datasheets which parsimoniously describe what you need to get packets flowing.  In this blessed land it's easy to write drivers, and for that reason there are many of them.  Developers choose a driver based on their needs, or they write one themselves if their needs are quite specific.</p><p>But there is another path, says Luke, that of "driver hell": a world of wickedness and proprietary datasheets, where even when you buy the hardware, you can't program it unless you're buying a hundred thousand units, and even then you are smitten with the cursed non-disclosure agreements.  In this inferno, only a vendor is practically empowered to write drivers, but their poor driver developers are only incentivized to get the driver out the door deployed on all nine architectural circles of driver hell.  So they include some kind of circle-of-hell abstraction layer, resulting in a hundred thousand lines of code like a tangled frozen beard.  We all saw the abyss and repented.</p><p>Luke described the process that led to Mellanox releasing the specification for its ConnectX line of cards, something that was warmly appreciated by the entire audience, users and driver developers included.  Wonderful stuff.</p><p>My Igalia colleague Asumu Takikawa took the last half of the presentation, showing some code for the driver for the Intel i210, i350, and 82599 cards.  For more on that, I recommend his recent <a href="https://www.asumu.xyz/blog/2018/01/15/supporting-both-vmdq-and-rss-in-snabb">blog post on user-space driver development</a>.  It was truly a ray of sunshine in dark, dark Brussels.</p><p><b>Ole Trøan -- Cisco -- <a href="https://fosdem.org/2018/schedule/event/vnf_vpp/">Fast dataplanes with VPP</a></b></p><p>This talk was a delightful introduction to <a href="https://wiki.fd.io/view/VPP">VPP</a>, but without all of the marketing; the sort of talk that makes FOSDEM worthwhile.  Usually at more commercial, vendory events, you can't really get close to the technical people unless you have a vendor relationship: they are surrounded by a phalanx of salesfolk.  But in FOSDEM it is clear that we are all comrades out on the open source networking front.</p><p>The speaker expressed great personal pleasure on having being able to work on open source software; his relief was palpable.  A nice moment.</p><p>He also had some kind words about Snabb, too, saying at one point that "of course you can do it on snabb as well -- Snabb and VPP are quite similar in their approach to life".  He trolled the horrible complexity diagrams of many "NFV" stacks whose components reflect the org charts that produce them more than the needs of the network functions in question (service chaining anyone?).</p><p>He did get to drop some numbers as well, which I found interesting.  One is that recently they have been working on carrier-grade NAT, aiming for 6 terabits per second.  Those are pretty big boxes and I hope they are getting paid appropriately for that :)  For context he said that for a 4-unit server, these days you can build one that does a little less than a terabit per second.  I assume that's with ten dual-port 40Gbps cards, and I would guess to power that you'd need around 40 cores or so, split between two sockets.</p><p>Finally, he finished with a long example on lightweight 4-over-6.  Incidentally this is the same network function my group at Igalia has been building in Snabb over the last couple years, so it was interesting to see the comparison.  I enjoyed his commentary that although all of these technologies (carrier-grade NAT, MAP, lightweight 4-over-6) have the ostensible goal of keeping IPv4 running, in reality "we're day by day making IPv4 work worse", mainly by breaking the assumption that just because you get traffic from port P on IP M, doesn't mean you can send traffic to M from another port or another protocol and have it reach the target.</p><p>All of these technologies also have problems with IPv4 fragmentation.  Getting it right is possible but expensive.  Instead, Ole mentions that he and a cross-vendor cabal of dataplane people have a "dark RFC" in the works to deprecate IPv4 fragmentation entirely :)</p><p>OK that's it.  If I get around to writing up the couple of interesting Java talks I went to (I know right?) I'll let yall know.  Happy hacking!</p></div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">GStreamer has grown a WebRTC implementation</span></a><div class="lastUpdated">2018年2月5日 22:28</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div dir="ltr"><span><i>In other news, GStreamer is now almost buzzword-compliant! The next blog post on our list: blockchains and smart contracts in GStreamer.</i></span><br /><br />Late last year, <a href="https://twitter.com/centricular/status/921727092810592256" target="_top">we at Centricular announced</a> a new <a href="http://webrtcbydralex.com/index.php/2017/10/21/a-new-webrtc-implementation-is-out/" target="_top">implementation of WebRTC</a> in GStreamer.  Today we're happy to announce that after  community review, that work has been <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/commit/?id=1894293d6378c69548d974d2965e9decc1527654" target="_top">merged into GStreamer</a> itself! The plugin is called <tt>webrtcbin</tt>, and the library is, naturally, called <tt>gstwebrtc</tt>.<br /><br />The implementation has all the basic features, is transparently compatible with other WebRTC stacks (particularly in browsers), and has been  well-tested with both Firefox and Chrome.<br /><br />Some of the more advanced features such as FEC  are already a <a href="https://bugzilla.gnome.org/show_bug.cgi?id=792696" target="_top">work in progress</a>, and others will be too—if you want them to be! Hop onto IRC on #gstreamer @ Freenode.net or join <a href="https://lists.freedesktop.org/mailman/listinfo/gstreamer-devel" target="_top">the mailing list</a>.<br /><br /><h3>How do I use it?</h3><br />Currently, the easiest way to use <tt>webrtcbin</tt> is to build GStreamer using either <a href="https://arunraghavan.net/2014/07/quick-start-guide-to-gst-uninstalled-1-x/">gst-uninstalled</a> (Linux and macOS) or <a href="https://gstreamer.freedesktop.org/documentation/installing/building-from-source-using-cerbero.html">Cerbero</a> (Windows, iOS, Android). If you're a patient person, you can follow <a href="https://twitter.com/gstreamer">@gstreamer</a> and wait for GStreamer 1.14 to be released which will include <a href="https://gstreamer.freedesktop.org/download/">Windows, macOS, iOS, and Android binaries</a>.<br /><br />The API currently lacks documentation, so the best way to learn it is to dive into the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/tests/examples/webrtc">source-tree examples</a>. Help on this will be most appreciated! To see how to use GStreamer to do WebRTC with a browser, checkout the <a href="https://github.com/centricular/gstwebrtc-demos/">bidirectional audio-video demos</a> that I wrote.<br /><br /><a name="show-code"></a><h3>Show me the code! <span><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#no-code-pls">[skip]</a></span></h3><br />Here's a quick highlight of the important bits that should get you started if you already know how GStreamer works. This example is in C, but GStreamer also has bindings for <a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/">Rust</a>, <a href="https://cgit.freedesktop.org/gstreamer/gst-python/">Python</a>, <a href="https://github.com/gstreamer-java/gst1-java-core">Java</a>, <a href="https://cgit.freedesktop.org/gstreamer/gstreamer-sharp/">C#</a>, Vala, and so on.<br /><br />Let's say you want to capture video from <a href="https://en.wikipedia.org/wiki/Video4Linux">V4L2</a>, stream it to a webrtc peer, and receive video back from it. The first step is the streaming pipeline, which will look something like this:<br /><br /><div><pre><span>v4l2src</span> <span>!</span> <span>queue</span> <span>!</span> <span>vp8enc</span> <span>!</span> <span>rtpvp8pay</span> <span>!</span><br />    <span>application</span><span>/</span><span>x</span><span>-</span><span>rtp,media</span><span>=</span><span>video,encoding</span><span>-</span><span>name</span><span>=</span><span>VP8</span><span>,payload</span><span>=</span><span>96</span> <span>!</span> <br />    <span>webrtcbin</span> <span>name</span><span>=</span><span>sendrecv</span><br /></pre><table><tbody><tr></tr></tbody></table></div></div><br />As a short-cut, let's parse the string description to create the pipeline.<br /><br /><div><table><tbody><tr><td><pre>1<br />2<br />3<br />4<br />5</pre></td><td><pre><span>GstElement</span> <span>*</span><span>pipe;</span><br /><br /><span>pipe</span> <span>=</span> <span>gst_parse_launch</span> <span>(</span><span>"v4l2src ! queue ! vp8enc ! rtpvp8pay ! "</span><br />    <span>"application/x-rtp,media=video,encoding-name=VP8,payload=96 !"</span><br />    <span>" webrtcbin name=sendrecv"</span><span>,</span> <span>NULL</span><span>);</span><br /></pre></td></tr></tbody></table></div><br />Next, we get a reference to the <tt>webrtcbin</tt> element and attach some callbacks to it.<br /><br /><div><table><tbody><tr><td><pre> 1<br /> 2<br /> 3<br /> 4<br /> 5<br /> 6<br /> 7<br /> 8<br /> 9<br />10<br />11<br />12<br />13<br />14<br />15<br />16<br />17<br />18<br />19</pre></td><td><pre><span>GstElement</span> <span>*</span><span>webrtc;</span><br /><br /><span>webrtc</span> <span>=</span> <span>gst_bin_get_by_name</span> <span>(</span><span>GST_BIN</span> <span>(pipe),</span> <span>"sendrecv"</span><span>);</span><br /><span>g_assert</span> <span>(webrtc</span> <span>!=</span> <span>NULL</span><span>);</span><br /><br /><span>/* This is the gstwebrtc entry point where we create the offer.</span><br /><span> * It will be called when the pipeline goes to PLAYING. */</span><br /><span>g_signal_connect</span> <span>(webrtc,</span> <span>"on-negotiation-needed"</span><span>,</span><br />    <span>G_CALLBACK</span> <span>(on_negotiation_needed),</span> <span>NULL</span><span>);</span><br /><span>/* We will transmit this ICE candidate to the remote using some</span><br /><span> * signalling. Incoming ICE candidates from the remote need to be</span><br /><span> * added by us too. */</span><br /><span>g_signal_connect</span> <span>(webrtc,</span> <span>"on-ice-candidate"</span><span>,</span><br />    <span>G_CALLBACK</span> <span>(send_ice_candidate_message),</span> <span>NULL</span><span>);</span><br /><span>/* Incoming streams will be exposed via this signal */</span><br /><span>g_signal_connect</span> <span>(webrtc,</span> <span>"pad-added"</span><span>,</span><br />    <span>G_CALLBACK</span> <span>(on_incoming_stream),</span> <span>pipe);</span><br /><span>/* Lifetime is the same as the pipeline itself */</span><br /><span>gst_object_unref</span> <span>(webrtc);</span><br /></pre></td></tr></tbody></table></div><br />When the pipeline goes to PLAYING, the <tt>on_negotiation_needed()</tt> callback will be called, and we will ask <tt>webrtcbin</tt> to create an offer which will match the pipeline above.<br /><br /><div><table><tbody><tr><td><pre> 1<br /> 2<br /> 3<br /> 4<br /> 5<br /> 6<br /> 7<br /> 8<br /> 9<br />10</pre></td><td><pre><span>static</span> <span>void</span><br /><span>on_negotiation_needed</span> <span>(GstElement</span> <span>*</span> <span>webrtc,</span> <span>gpointer</span> <span>user_data)</span><br /><span>{</span><br />  <span>GstPromise</span> <span>*</span><span>promise;</span><br /><br />  <span>promise</span> <span>=</span> <span>gst_promise_new_with_change_func</span> <span>(on_offer_created,</span><br />      <span>user_data,</span> <span>NULL</span><span>);</span><br />  <span>g_signal_emit_by_name</span> <span>(webrtc,</span> <span>"create-offer"</span><span>,</span> <span>NULL</span><span>,</span><br />      <span>promise);</span><br /><span>}</span><br /></pre></td></tr></tbody></table></div><br />When webrtcbin has created the offer, it will call <tt>on_offer_created()</tt><br /><br /><div><table><tbody><tr><td><pre> 1<br /> 2<br /> 3<br /> 4<br /> 5<br /> 6<br /> 7<br /> 8<br /> 9<br />10<br />11<br />12<br />13<br />14<br />15<br />16<br />17<br />18<br />19<br />20<br />21</pre></td><td><pre><span>static</span> <span>void</span><br /><span>on_offer_created</span> <span>(</span><span>GstPromise</span> <span>*</span> <span>promise,</span> <span>GstElement</span> <span>*</span> <span>webrtc)</span><br /><span>{</span><br />  <span>GstWebRTCSessionDescription</span> <span>*</span><span>offer</span> <span>=</span> <span>NULL</span><span>;</span><br />  <span>const</span> <span>GstStructure</span> <span>*</span><span>reply;</span><br />  <span>gchar</span> <span>*</span><span>desc;</span><br /><br />  <span>reply</span> <span>=</span> <span>gst_promise_get_reply</span> <span>(promise);</span><br />  <span>gst_structure_get</span> <span>(reply,</span> <span>"offer"</span><span>,</span><br />      <span>GST_TYPE_WEBRTC_SESSION_DESCRIPTION,</span> <br />      <span>&amp;</span><span>offer,</span> <span>NULL</span><span>);</span><br />  <span>gst_promise_unref</span> <span>(promise);</span><br /><br />  <span>/* We can edit this offer before setting and sending */</span><br />  <span>g_signal_emit_by_name</span> <span>(webrtc,</span><br />      <span>"set-local-description"</span><span>,</span> <span>offer,</span> <span>NULL</span><span>);</span><br /><br />  <span>/* Implement this and send offer to peer using signalling */</span><br />  <span>send_sdp_offer</span> <span>(offer);</span><br />  <span>gst_webrtc_session_description_free</span> <span>(offer);</span><br /><span>}</span><br /></pre></td></tr></tbody></table></div><br />Similarly, when we have the SDP <tt>answer</tt> from the remote, we must call <tt>"set-remote-description"</tt> on <tt>webrtcbin</tt>. <br /><br /><div><table><tbody><tr><td><pre>1<br />2<br />3<br />4<br />5<br />6<br />7</pre></td><td><pre><span>answer</span> <span>=</span> <span>gst_webrtc_session_description_new</span> <span>(</span><br />    <span>GST_WEBRTC_SDP_TYPE_ANSWER,</span> <span>sdp);</span><br /><span>g_assert</span> <span>(answer);</span><br /><br /><span>/* Set remote description on our pipeline */</span><br /><span>g_signal_emit_by_name</span> <span>(webrtc,</span> <span>"set-remote-description"</span><span>,</span><br />    <span>answer,</span> <span>NULL</span><span>);</span><br /></pre></td></tr></tbody></table></div><br />ICE handling is very similar; when the <tt>"on-ice-candidate"</tt> signal is emitted, we get a local ICE candidate which we must <a href="https://github.com/centricular/gstwebrtc-demos/blob/master/sendrecv/gst/webrtc-sendrecv.c#L167">send to the remote</a>. When we have an ICE candidate from the remote, we must <a href="https://github.com/centricular/gstwebrtc-demos/blob/master/sendrecv/gst/webrtc-sendrecv.c#L500">call</a> <tt>"add-ice-candidate"</tt> on <tt>webrtcbin</tt>.<br /><br />There's just one piece left now; handling incoming streams that are sent by the remote. For that, we have <tt>on_incoming_stream()</tt> attached to the <tt>"pad-added"</tt> signal on <tt>webrtcbin</tt>.<br /><br /><div><table><tbody><tr><td><pre> 1<br /> 2<br /> 3<br /> 4<br /> 5<br /> 6<br /> 7<br /> 8<br /> 9<br />10<br />11<br />12<br />13<br />14<br />15</pre></td><td><pre><span>static</span> <span>void</span><br /><span>on_incoming_stream</span> <span>(</span><span>GstElement</span> <span>*</span> <span>webrtc,</span> <span>GstPad</span> <span>*</span> <span>pad,</span><br />    <span>GstElement</span> <span>*</span> <span>pipe)</span><br /><span>{</span><br />  <span>GstElement</span> <span>*</span><span>play;</span><br /><br />  <span>play</span> <span>=</span> <span>gst_parse_bin_from_description</span> <span>(</span><br />      <span>"queue ! vp8dec ! videoconvert ! autovideosink"</span><span>,</span><br />      <span>TRUE</span><span>,</span> <span>NULL</span><span>);</span><br />  <span>gst_bin_add</span> <span>(</span><span>GST_BIN</span> <span>(pipe),</span> <span>play);</span><br /><br />  <span>/* Start displaying video */</span><br />  <span>gst_element_sync_state_with_parent</span> <span>(play);</span><br />  <span>gst_element_link</span> <span>(webrtc,</span> <span>play);</span><br /><span>}</span><br /></pre></td></tr></tbody></table></div><br />That's it! This is what a basic webrtc workflow looks like. Those of you that have used the <tt>PeerConnection</tt> API before will be happy to see that this maps to that quite closely.<br /><br /><a name="no-code-pls"></a>The <a href="https://github.com/centricular/gstwebrtc-demos/">aforementioned demos</a> also include a Websocket signalling server and JS browser components, and I will be doing an in-depth application newbie developer's guide at a later time, so you can <a href="https://twitter.com/nirbheek" target="_top">follow me @nirbheek</a> to hear when it comes out!<br /><br /><h3>Tell me more!</h3><br />The code is already being used in production in a number of places, such as <a href="http://www.easymile.com/" target="_top">EasyMile</a>'s autonomous vehicles, and we're excited to see where else the community can take it.<br /><div name="why-gst"><br /></div>If you're wondering why we decided a new implementation was needed, read on! For a more detailed discussion into that, you should watch <a href="https://gstconf.ubicast.tv/videos/gstreamer-webrtc/">Matthew Waters' talk</a> from the <a href="https://gstreamer.freedesktop.org/conference/2017/">GStreamer conference last year</a>. It's a great companion for this article!<br /><br />But before we can dig into  details, we need to lay some foundations first.  <br /><br /><h3 name="what-is">What is GStreamer, and what is WebRTC? <span><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#why-build">[skip]</a></span></h3><div><br /><b>GStreamer</b> is a cross-platform <a href="https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html">open-source multimedia framework</a> that is, in my opinion, the easiest and most flexible way to implement any application that needs to play, record, or transform media-like data across an extremely versatile scale of devices and products. Embedded (IoT, IVI, phones, TVs, …), desktop (video/music players, video recording, non-linear editing, videoconferencing and <a href="https://en.wikipedia.org/wiki/Voice_over_IP">VoIP</a> clients, browsers …), to servers (encode/transcode farms, video/voice conferencing servers, …) and <a href="https://wiki.ligo.org/DASWG/GstLAL">more</a>.</div><div><br /></div><div>But what I like the most about GStreamer is the pipeline-based model which solves one of the hardest problems in API design: catering to applications of varying complexity; from the simplest one-liners and quick solutions to those that need several hundreds of thousands of lines of code to implement their full featureset. </div><div><br /></div><div>If you want to learn more about GStreamer, <a href="https://www.youtube.com/watch?v=ZphadMGufY8">Jan Schmidt's tutorial</a> from <a href="http://lca2018.linux.org.au/">Linux.conf.au</a> is a good start.</div><div><br /></div><div><b>WebRTC</b> is a set of draft specifications that build upon existing <a href="https://en.wikipedia.org/wiki/Real-time_Transport_Protocol">RTP</a>, <a href="https://en.wikipedia.org/wiki/RTP_Control_Protocol">RTCP</a>, <a href="https://en.wikipedia.org/wiki/Session_Description_Protocol">SDP</a>, <a href="https://en.wikipedia.org/wiki/Datagram_Transport_Layer_Security">DTLS</a>, <a href="https://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment">ICE</a> (and many other) real-time communication specifications and defines an API for making <abbr title="Real-time Communication">RTC</abbr> accessible using browser JS APIs.</div><div><br /></div><div>People have been doing real-time communication over <a href="https://en.wikipedia.org/wiki/Internet_Protocol">IP</a> for <a href="https://en.wikipedia.org/wiki/Session_Initiation_Protocol">decades</a> with the previously-listed protocols that WebRTC builds upon. The real innovation of WebRTC was creating a bridge between native  applications and webapps by defining a standard, yet flexible, API that browsers can expose to untrusted JavaScript code.</div><div><br /></div><div>These specifications are <a href="https://datatracker.ietf.org/wg/rtcweb/documents/">constantly</a> being <a href="https://datatracker.ietf.org/wg/rmcat/documents/">improved upon</a>, which combined with the ubiquitous nature of browsers means WebRTC is fast becoming the standard choice for  videoconferencing on all platforms and for most applications.</div><br /><a name="why-build"></a><h3>Everything is great, let's build amazing apps! <span><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#why-gst">[skip]</a></span></h3><div><br />Not so fast, there's more to the story! For WebApps, the <a href="https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection">PeerConnection API</a> is <a href="https://caniuse.com/#feat=rtcpeerconnection">everywhere</a>. There are some browser-specific quirks as usual, and the API itself keeps changing, but  the <a href="https://github.com/webrtc/adapter">WebRTC JS adapter</a> handles most of that. Overall the WebApp experience is mostly 👍.</div><div><br /></div><div>Sadly, for native code or applications that need more flexibility than a sandboxed JS app can achieve, there <i>haven't</i> been a lot of great options.</div><div><br /></div><div><a href="http://webrtc.org/" target="_top">libwebrtc</a> (Chrome's implementation), <a href="https://janus.conf.meetecho.com/" target="_top">Janus</a>, <a href="https://www.kurento.org/kurento-architecture" target="_top">Kurento</a>, and <a href="https://en.wikipedia.org/wiki/OpenWebRTC" target="_top">OpenWebRTC</a> have traditionally been the main contenders, but after having worked with all of these, we found that each implementation has its own inflexibilities, shortcomings, and constraints.</div><div><br /></div><div><b>libwebrtc</b> is still the most mature implementation, but it is also the most difficult to work with. Since it's embedded inside Chrome, it's a moving target, the API can be hard to work with, and the project <a href="https://webrtchacks.com/building-webrtc-from-source/" target="_top">is quite difficult to build and integrate</a>, all of which are obstacles in the way of native or server app developers trying to quickly prototype and try out things.</div><div><br /></div><div>It was also not built for multimedia use-cases, so while the webrtc bits are great, the lower layers get in the way of non-browser use-cases and applications. It is quite painful to do anything other than the default "set raw media, transmit" and "receive from remote, get raw media". This means that if you want to use your own filters, or hardware-specific codecs or sinks/sources, you end up having to fork libwebrtc.</div><br /><div>In contrast, <a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#show-code">as shown above</a>, our implementation gives you full control over this as with any other <a href="https://gstreamer.freedesktop.org/documentation/application-development/introduction/basics.html" target="_top">GStreamer pipeline</a>.</div><div><br /></div><div><b>OpenWebRTC</b> by Ericsson was the first attempt to rectify this situation, and it was built on top of GStreamer. The target audience was app developers, and it fit the bill quite well as a proof-of-concept—even though it used a custom API and some of the architectural decisions made it quite inflexible for most other use-cases.</div><div><br /></div><div>However, after an initial flurry of activity around the project, momentum petered out, the project failed to gather a community around itself, and is now <a href="https://www.youtube.com/watch?v=npjOSLCR2hE" target="_top">effectively dead</a>.</div><div><br /></div><div><i>Full disclosure: <a href="https://centricular.com/">we</a> worked with Ericsson to polish some of the rough edges around the project immediately prior to its public release.</i><br /></div><br /><a name="why-gst"></a><h3>WebRTC in GStreamer — webrtcbin and gstwebrtc</h3><div><br />Remember how I said the WebRTC standards build upon existing standards and protocols? As it so happens, GStreamer has supported almost all of them for a while now because they were being used for real-time communication, live streaming, and in many other <a href="https://en.wikipedia.org/wiki/Internet_Protocol">IP-based</a> applications. Indeed, that's partly why Ericsson chose it as the base for <abbr title="OpenWebRTC">OWRTC</abbr>.</div><div name="why-gst"><br /></div><div>This combined with the SRTP and DTLS plugins that were written during OWRTC's development meant that<i> </i>our implementation is built upon a solid and well-tested base, and that implementing WebRTC features is not as difficult as one might presume. However, WebRTC is a large collection of standards, and reaching feature-parity with libwebrtc is an ongoing task.<br /><br />Lucky for us, <a href="https://github.com/ystreet/">Matthew</a> made some excellent decisions while architecting the internals of webrtcbin, and we follow the PeerConnection specification quite closely, so almost all the missing features involve writing code that would plug into clearly-defined sockets.</div><div name="why-gst"><br /></div><div>We believe what we've been building here is the most flexible, versatile, and easy to use WebRTC implementation out there, and it can only get better as time goes by. Bringing the power of pipeline-based multimedia manipulation to WebRTC opens new doors for interesting, unique, and highly efficient applications.<br /><br />To demonstrate this, in the near future we will be publishing articles that dive into how to use the PeerConnection-inspired API exposed by webrtcbin to build various kinds of applications—starting with a CPU-efficient multi-party bidirectional conferencing solution with a mesh topology that can work with any webrtc stack.<br /><br />Until next time!</div></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://blogs.gnome.org/uraeus/2018/01/26/an-update-on-pipewire-the-multimedia-revolution-an-update/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">An update on Pipewire – the multimedia revolution</span></a><div class="lastUpdated">2018年1月26日 22:22</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>We launched <a href="https://pipewire.org/">PipeWire</a> last September with <a href="https://blogs.gnome.org/uraeus/2017/09/19/launching-pipewire/">this blog entry</a>. I thought it would be interesting for people to hear about the latest progress on what I believe is going to be a gigantic step forward for the Linux desktop. So I caught up with Pipewire creator <a href="https://twitter.com/wtaymans">Wim Taymans</a> during <a href="https://devconf.cz/cz/2018">DevConf 2018 in Brno</a> where Wim is doing a talk about Pipewire and we discussed the current state of the code and Wim demonstrated a few of the things that PipeWire now can do.<br />
</p><div id="attachment_2796" class="wp-caption aligncenter"><a href="https://blogs.gnome.org/uraeus/files/2018/01/wimchristianpipewire.jpg"><img src="wimchristianpipewire-300x170.jpg" alt="Christian Schaller and Wim Taymans testing PipeWire with Cheese" class="size-medium wp-image-2796" width="300" height="170" /></a><p class="wp-caption-text">Christian Schaller and Wim Taymans testing PipeWire with Cheese</p></div><p></p>
<h2>Priority number 1: video handling</h2>
<p>So as we said when we launched the top priority for PipeWire is to address our needs on the video side of multimedia. This is critical due to the more secure nature of Wayland, which makes the old methods for screen sharing not work anymore and the emergence of desktop containers in the form of <a href="http://www.flatpak.org/">Flatpak</a>. Thus we need PipeWire to help us provide appliation and desktop developers with a new method for doing screen sharing and also to provide a secure way for applications inside a container to access audio and video devices on the system.</p>
<p>There are 3 major challenges PipeWire wants to solve for video. One is device sharing, meaning that multiple applications can share the same video hardware device, second it wants to be able to do so in a secure manner, ensuring your video streams are not highjacked by a rogue process and finally it wants to provide an efficient method for sharing of multimedia between applications, like for instance fullscreen capture from your compositor (like GNOME Shell) to your video conferencing application running in your browser like Google Hangouts, Blue Jeans or Pexip.</p>
<p>So the first thing Wim showed me in action was the device sharing. We launched the GNOME photoboot application <a href="https://wiki.gnome.org/action/show/Cheese?action=show&amp;redirect=Apps%2FCheese">Cheese</a> which gets PipeWire support for free thanks to the PipeWire GStreamer plugin. And this is an important thing to remember, thanks to so many Linux applications using GStreamer these days we don’t need to port each one of them to PipeWire, instead the PipeWire GStreamer plugin does the ‘porting’ for us. We then launched a gst-launch command line pipeline in a terminal. The result is two applications sharing the same webcam input without one of them blocking access for the other.</p>
<p><a href="https://blogs.gnome.org/uraeus/files/2018/01/pipewire-cheese.png"><img src="pipewire-cheese-300x169.png" alt="Cheese and GStreamer pipeline running on Pipewiere" class="aligncenter size-medium wp-image-2795" width="300" height="169" /></a></p>
<p>As you can see from the screenshot above it worked fine, and this was actually done on my Fedora Workstation 27 system and the only thing we had to do was to start the ‘pipewire’ process in a termal before starting Cheese and the gst-launch pipeline. GStreamer autoplugging took care of the rest. So feel free to try this out yourself if you are interested, but be aware that you will find bugs quickly if you try things like on the fly resolution changes or switching video devices. This is still tech preview level software in Fedora 27.</p>
<p>The plan is for Wim Taymans to sit down with the web browser maintainers at Red Hat early next week and see if we can make progress on supporting PipeWire in Firefox and Chrome, so that conferencing software like the ones mentioned above can start working fully under Wayland.</p>
<p>Since security was one of the drivers for the move to Wayland from X Windows we of course also put a lot of emphasis of not recreating the security holes of X in the compositor. So the way PipeWire now works is that if an application wants to do full screen capture it will check with the compositor through a dbus-api, or a portal in Flatpak and Wayland terminology, and only allows the permited application to do the screen capture, so the stream can’t be highjacked by a random rougue application or process on your computer. This also works from within a sandboxed setting like Flatpaks.</p>
<h2>Jack Support</h2>
<p>Another important goal of PipeWire was to bring all Linux audio and video together, which means PipeWire needed to be as good or better replacement for <a href="http://www.jackaudio.org/">Jack</a> for the Pro-Audio usecase. This is a tough usecase to satisfy so while getting the video part has been the top development priority Wim has also worked on verifying that the design allows for the low latency and control needed for Pro-Audio. To do this Wim has implemented the Jack protocol on top of PipeWire.<br />
</p><div id="attachment_2800" class="wp-caption aligncenter"><a href="https://blogs.gnome.org/uraeus/files/2018/01/carla-pipewire.png"><img src="carla-pipewire-300x210.png" alt="" class="size-medium wp-image-2800" width="300" height="210" /></a><p class="wp-caption-text">Carla, a Jack application running on top of PipeWire.</p></div><br />
Through that work he has now verified that he is able to achieve the low latency needed for pro-audio with PipeWire and that he will be able to run Jack applications without changes on top of PipeWire. So above you see a screenshot of Carla, a Jack-based application running on top of PipeWire with no Jack server running on the system.<p></p>
<h2>ALSA/Legacy applications</h2>
<p>Another item Wim has written the first code for and verfied will work well is the Alsa emulation. The goal of this piece of code is to allow applications using the ALSA userspace API to output to Pipewire without needing special porting or application developer effort. At Red Hat we have many customers with older bespoke applications using this API so it has been of special interest for us to ensure this works just as well as the native ALSA output. It is also worth nothing that Pipewire also does mixing so that sound being routed through ALSA will get seamlessly mixed with audio coming through the Jack layer.</p>
<h2>Bluetooth support</h2>
<p>The last item Wim has spent some time on since last September is working on making sure Bluetooth output works and he demonstrated this to me while we where talking together during DevConf. The Pipewire bluetooth module plugs directly into the Bluez Bluetooth framework, meaning that things like the GNOME Bluetooth control panel just works with it without any porting work needed. And while the code is still quite young, Wim demonstrated pairing and playing music over bluetooth using it to me.</p>
<h2>What about PulseAudio?</h2>
<p>So as you probably noticed one thing we didn’t mention above is how to deal with PulseAudio applications. Handling this usecase is still on the todo list and the plan is to at least initially just keep PulseAudio running on the system outputing its sound through PipeWire. That said we are a bit unsure how many appliations would actually be using this path because as mentioned above all GStreamer applications for instance would be PipeWire native automatically through the PipeWire GStreamer plugins. And for legacy applications the PipeWire ALSA layer would replace the current PulseAudio ALSA layer as the default ALSA output, meaning that the only applications left are those outputing to PulseAudio directly themselves. The plan would also be to keep the PulseAudio ALSA device around so if people want to use things like the PulseAudio networked audio functionality they can choose the PA ALSA device manually to be able to keep doing so.<br />
Over time the goal would of course be to not have to keep the PulseAudio daemon around, but dropping it completely is likely to be a multiyear process with current plans, so it is kinda like XWayland on top of Wayland.</p>
<h2>Summary</h2>
<p>So you might read this and think, hey if all this work we are almost done right? Well unfortunately no, the components mentioned here are good enough for us to verify the design and features, but they still need a lot of maturing and testing before they will be in a state where we can consider switching Fedora Workstation over to using them by default. So there are many warts that needs to be cleaned up still, but a lot of things have become a lot more tangible now than when we last spoke about PipeWire in September. The video handling we hope to enable in Fedora Workstation 28 as mentioned, while the other pieces we will work towards enabling in later releases as the components mature.<br />
Of course the more people interesting in joining the PipeWire community to help us out, the quicker we can mature these different pieces. So if you are interested please join us in #pipewire on irc.freenode.net or just clone the code of github and start hacking. You find the details <a href="https://pipewire.org/#get-involved">for irc and git here</a>.</p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="https://coaxion.net/blog/2018/01/speeding-up-rgb-to-grayscale-conversion-in-rust-by-a-factor-of-2-2-and-various-other-multimedia-related-processing-loops/"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">Speeding up RGB to grayscale conversion in Rust by a factor of 2.2 – and various other multimedia related processing loops</span></a><div class="lastUpdated">2018年1月21日 21:48</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><p>In the <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/" rel="noopener" target="_top">previous blog post</a> I wrote about how to write a RGB to grayscale conversion filter for <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> in <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a>. In this blog post I’m going to write about how to optimize the processing loop of that filter, without resorting to <em>unsafe</em> code or <a href="https://en.wikipedia.org/wiki/SIMD" rel="noopener" target="_top">SIMD</a> instructions by staying with plain, safe Rust code.</p>
<p>I also tried to implement the processing loop with <a href="https://github.com/AdamNiederer/faster" rel="noopener" target="_top">faster</a>, a Rust crate for writing safe SIMD code. It looks very promising, but unless I missed something in the documentation it currently is missing some features to be able to express this specific algorithm in a meaningful way. Once it works on stable Rust (waiting for SIMD to be stabilized) and includes runtime CPU feature detection, this could very well be a good replacement for the <a href="https://cgit.freedesktop.org/gstreamer/orc" rel="noopener" target="_top">ORC</a> library used for the same purpose in GStreamer in various places. ORC works by JIT-compiling a minimal “array operation language” to SIMD assembly for your specific CPU (and has support for x86 MMX/SSE, PPC Altivec, ARM NEON, etc.).</p>
<p>If someone wants to prove me wrong and implement this with faster, feel free to do so and I’ll link to your solution and include it in the benchmark results below.</p>
<p>All code below can be found in this <a href="https://github.com/sdroege/bgrx-to-grayscale-benchmark.rs" rel="noopener" target="_top">GIT repository</a>.</p>
<h3 id="toc">Table of Contents</h3>
<ol>
<li><a href="https://coaxion.net/blog/feed/#baseline">Baseline Implementation</a></li>
<li><a href="https://coaxion.net/blog/feed/#assertions">First Optimization – Assertions</a></li>
<li><a href="https://coaxion.net/blog/feed/#assertions-2">First Optimization – Assertions Try 2</a>
</li><li><a href="https://coaxion.net/blog/feed/#iterator">Second Optimization – Iterate a bit more</a></li>
<li><a href="https://coaxion.net/blog/feed/#no-more-bounds">Third Optimization – Getting rid of the bounds check finally</a></li>
<li><a href="https://coaxion.net/blog/feed/#summary">Summary</a></li>
<li><a href="https://coaxion.net/blog/feed/#split-at">Addendum: <em>slice::split_at</em></a><a></a></li>
<li><a href="https://coaxion.net/blog/feed/#faster">Addendum 2: SIMD with faster</a><a></a></li>
</ol>
<h3 id="baseline">Baseline Implementation</h3>
<p>This is how the baseline implementation looks like.</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_chunks_no_asserts(
    in_data: &amp;[u8],
    out_data: &amp;mut [u8],
    in_stride: usize,
    out_stride: usize,
    width: usize,
) {
    let in_line_bytes = width * 4;
    let out_line_bytes = width * 4;

    for (in_line, out_line) in in_data
        .chunks(in_stride)
        .zip(out_data.chunks_mut(out_stride))
    {
        for (in_p, out_p) in in_line[..in_line_bytes]
            .chunks(4)
            .zip(out_line[..out_line_bytes].chunks_mut(4))
        {
            let b = u32::from(in_p[0]);
            let g = u32::from(in_p[1]);
            let r = u32::from(in_p[2]);
            let x = u32::from(in_p[3]);

            let grey = ((r * RGB_Y[0]) + (g * RGB_Y[1]) + (b * RGB_Y[2]) + (x * RGB_Y[3])) / 65536;
            let grey = grey as u8;
            out_p[0] = grey;
            out_p[1] = grey;
            out_p[2] = grey;
            out_p[3] = grey;
        }
    }
}</pre><p> </p>
<p>This basically iterates over each line of the input and output frame (outer loop), and then for each BGRx chunk of 4 bytes in each line it converts the values to <em>u32</em>, multiplies with a constant array, converts back to <em>u8</em> and stores the same value in the whole output BGRx chunk.</p>
<p><strong>Note:</strong> This is only doing the actual conversion from linear RGB to grayscale (and in <a href="https://en.wikipedia.org/wiki/YUV#SDTV_with_BT.601" rel="noopener" target="_top">BT.601</a> colorspace). To do this conversion correctly you need to know your colorspaces and use the correct coefficients for conversion, and also do <a href="https://en.wikipedia.org/wiki/Gamma_correction" rel="noopener" target="_top">gamma correction</a>. See <a href="https://web.archive.org/web/20161024090830/http://www.4p8.com/eric.brasseur/gamma.html" rel="noopener" target="_top">this</a> about why it is important.</p>
<p>So what can be improved on this? For starters, let’s write a small benchmark for this so that we know whether any of our changes actually improve something. This is using the (unfortunately <a href="https://github.com/rust-lang/rfcs/pull/2287" rel="noopener" target="_top">still</a>) unstable benchmark feature of Cargo.</p>
<p></p><pre class="crayon-plain-tag">#![feature(test)]
#![feature(exact_chunks)]

extern crate test;

pub fn bgrx_to_gray_chunks_no_asserts(...)
    [...]
}

#[cfg(test)]
mod tests {
    use super::*;
    use test::Bencher;
    use std::iter;

    fn create_vec(w: usize, h: usize) -&gt; Vec&lt;u8&gt; {
        iter::repeat(0).take(w * h * 4).collect::&lt;_&gt;()
    }

    #[bench]
    fn bench_chunks_1920x1080_no_asserts(b: &amp;mut Bencher) {
        let i = test::black_box(create_vec(1920, 1080));
        let mut o = test::black_box(create_vec(1920, 1080));

        b.iter(|| bgrx_to_gray_chunks_no_asserts(&amp;i, &amp;mut o, 1920 * 4, 1920 * 4, 1920));
    }
}</pre><p> </p>
<p>This can be run with <em>cargo bench</em> and then prints the amount of nanoseconds each iterator of the closure was taking. To only really measure the processing itself, allocations and initializations of the input/output frame are happening outside of the closure. We’re not interested in times for that.</p>
<h3 id="assertions">First Optimization – Assertions</h3>
<p>To actually start optimizing this function, let’s take a look at the assembly that the compiler is outputting. The easiest way of doing that is via the <a href="https://godbolt.org/" rel="noopener" target="_top">Godbolt Compiler Explorer</a> website. Select “rustc nightly” and use <em>“-C opt-level=3”</em> for the compiler flags, and then copy &amp; paste your code in there. Once it compiles, to find the assembly that corresponds to a line, simply right-click on the line and “Scroll to assembly”.</p>
<p>Alternatively you can use <em>cargo rustc –release — -C opt-level=3 –emit asm</em> and check the assembly file that is output in the <em>target/release/deps</em> directory.</p>
<p>What we see then for our inner loop is something like the following</p>
<p></p><pre class="crayon-plain-tag">.LBB4_19:
  cmp r15, r11
  mov r13, r11
  cmova r13, r15
  mov rdx, r8
  sub rdx, r13
  je .LBB4_34
  cmp rdx, 3
  jb .LBB4_35
  inc r9
  movzx edx, byte ptr [rbx - 1]
  movzx ecx, byte ptr [rbx - 2]
  movzx esi, byte ptr [rbx]
  imul esi, esi, 19595
  imul edx, edx, 38470
  imul ecx, ecx, 7471
  add ecx, edx
  add ecx, esi
  shr ecx, 16
  mov byte ptr [r10 - 3], cl
  mov byte ptr [r10 - 2], cl
  mov byte ptr [r10 - 1], cl
  mov byte ptr [r10], cl
  add r10, 4
  add r8, -4
  add r15, -4
  add rbx, 4
  cmp r9, r14
  jb .LBB4_19</pre><p> </p>
<p>This is already quite optimized. For each loop iteration the first few instructions are doing some bounds checking and if they fail jump to the <em>.LBB4_34</em> or <em>.LBB4_35</em> labels. How to understand that this is bounds checking? Scroll down in the assembly to where these labels are defined and you’ll see something like the following</p>
<p></p><pre class="crayon-plain-tag">.LBB4_34:
  lea rdi, [rip + .Lpanic_bounds_check_loc.D]
  xor esi, esi
  xor edx, edx
  call core::panicking::panic_bounds_check@PLT
  ud2
.LBB4_35:
  cmp r15, r11
  cmova r11, r15
  sub r8, r11
  lea rdi, [rip + .Lpanic_bounds_check_loc.F]
  mov esi, 2
  mov rdx, r8
  call core::panicking::panic_bounds_check@PLT
  ud2</pre><p> </p>
<p>Also if you check (with the colors, or the “scroll to source” feature) which Rust code these correspond to, you’ll see that it’s the first and third access to the 4-byte slice that contains our BGRx values.</p>
<p>Afterwards in the assembly, the following steps are happening: 0) incrementing of the “loop counter” representing the number of iterations we’re going to do (<em>r9</em>), 1) actual reading of the B, G and R value and conversion to <em>u32</em> (the 3 <em>movzx</em>, note that the reading of the <em>x</em> value is optimized away as the compiler sees that it is always multiplied by 0 later), 2) the multiplications with the array elements (the 3 <em>imul</em>), 3) combining of the results and division (i.e. shift) (the 2 <em>add</em> and the <em>shr</em>), 4) storing of the result in the output (the 4 <em>mov</em>). Afterwards the slice pointers are increased by 4 (<em>rbx</em> and <em>r10</em>) and the lengths (used for bounds checking) are decreased by 4 (<em>r8</em> and <em>r15</em>). Finally there’s a check (<em>cmp</em>) to see if <em>r9</em> (our loop) counter is at the end of the slice, and if not we jump back to the beginning and operate on the next BGRx chunk.</p>
<p>Generally what we want to do for optimizations is to get rid of unnecessary checks (bounds checking), memory accesses, conditions (<em>cmp</em>, <em>cmov</em>) and jumps (the instructions starting with <em>j</em>). These are all things that are slowing down our code.</p>
<p>So the first thing that seems useful to optimize here is the bounds checking at the beginning. It definitely seems not useful to do two checks instead of one for the two slices (the checks are for the both slices at once but Godbolt does not detect that and believes it’s only the input slice). And ideally we could teach the compiler that no bounds checking is needed at all.</p>
<p>As I wrote in the previous blog post, often this knowledge can be given to the compiler by inserting assertions.</p>
<p>To prevent two checks and just have a single check, you can insert a <em>assert_eq!(in_p.len(), 4)</em> at the beginning of the inner loop and the same for the output slice. Now we only have a single bounds check left per iteration.</p>
<p>As a next step we might want to try to move this knowledge outside the inner loop so that there is no bounds checking at all in there anymore. We might want to add assertions like the following outside the outer loop then to give all knowledge we have to the compiler</p>
<p></p><pre class="crayon-plain-tag">assert_eq!(in_data.len() % 4, 0);
assert_eq!(out_data.len() % 4, 0);
assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

assert!(in_line_bytes &lt;= in_stride);
assert!(out_line_bytes &lt;= out_stride);</pre><p></p>
<p>Unfortunately adding those has no effect at all on the inner loop, but having them outside the outer loop for good measure is not the worst idea so let’s just keep them. At least it can be used as some kind of documentation of the invariants of this code for future readers.</p>
<p>So let’s benchmark these two implementations now. The results on my machine are the following</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_no_asserts ... bench:   4,420,145 ns/iter (+/- 139,051)
test tests::bench_chunks_1920x1080_asserts    ... bench:   4,897,046 ns/iter (+/- 166,555)</pre><p></p>
<p>This is surprising, our version without the assertions is actually faster by a factor of ~1.1 although it had fewer conditions. So let’s take a closer look at the assembly at the top of the loop again, where the bounds checking happens, in the version with assertions</p>
<p></p><pre class="crayon-plain-tag">.LBB4_19:
  cmp rbx, r11
  mov r9, r11
  cmova r9, rbx
  mov r14, r12
  sub r14, r9
  lea rax, [r14 - 1]
  mov qword ptr [rbp - 120], rax
  mov qword ptr [rbp - 128], r13
  mov qword ptr [rbp - 136], r10
  cmp r14, 5
  jne .LBB4_33
  inc rcx
  [...]</pre><p> </p>
<p>While this indeed has only one jump as expected for the bounds checking, the number of comparisons is the same and even worse: 3 memory writes to the stack are happening right before the jump. If we follow to the <em>.LBB4_33</em> label we will see that the <em>assert_eq!</em> macro is going to do something with <em>core::fmt::Debug</em>. This is setting up the information needed for printing the assertion failure, the “expected X equals to Y” output. This is certainly not good and the reason why everything is slower now.</p>
<h3 id="assertions-2">First Optimization – Assertions Try 2</h3>
<p>All the additional instructions and memory writes were happening because the <em>assert_eq!</em> macro is outputting something user friendly that actually contains the values of both sides. Let’s try again with the <em>assert!</em> macro instead</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_no_asserts ... bench:   4,420,145 ns/iter (+/- 139,051)
test tests::bench_chunks_1920x1080_asserts    ... bench:   4,897,046 ns/iter (+/- 166,555)
test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)</pre><p></p>
<p>This already looks more promising. Compared to our baseline version this gives us a speedup of a factor of 1.12, and compared to the version with <em>assert_eq!</em> 1.23. If we look at the assembly for the bounds checks (everything else stays the same), it also looks more like what we would’ve expected</p>
<p></p><pre class="crayon-plain-tag">.LBB4_19:
  cmp rbx, r12
  mov r13, r12
  cmova r13, rbx
  add r13, r14
  jne .LBB4_33
  inc r9
  [...]</pre><p> </p>
<p>One <em>cmp</em> less, only one jump left. And no memory writes anymore!</p>
<p>So keep in mind that <em>assert_eq!</em> is more user-friendly but quite a bit more expensive even in the “good case” compared to <em>assert!</em>.</p>
<h3 id="iterator">Second Optimization – Iterate a bit more</h3>
<p>This is still not very satisfying though. No bounds checking should be needed at all as each chunk is going to be exactly 4 bytes. We’re just not able to convince the compiler that this is the case. While it may be possible (let me know if you find a way!), let’s try something different. The <em>zip</em> iterator is done when the shortest iterator of both is done, and there are optimizations specifically for zipped slice iterators implemented. Let’s try that and replace the grayscale value calculation with</p>
<p></p><pre class="crayon-plain-tag">let grey = in_p.iter()
    .zip(RGB_Y.iter())
    .map(|(i, c)| u32::from(*i) * c)
    .sum::&lt;u32&gt;() / 65536;</pre><p> </p>
<p>If we run that through our benchmark after removing the <em>assert!(in_p.len() == 4)</em> (and the same for the output slice), these are the results</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)
test tests::bench_chunks_1920x1080_iter_sum   ... bench:  11,393,600 ns/iter (+/- 347,958)</pre><p></p>
<p>We’re actually 2.9 times slower! Even when adding back the <em>assert!(in_p.len() == 4)</em> assertion (and the same for the output slice) we’re still slower</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)
test tests::bench_chunks_1920x1080_iter_sum   ... bench:  11,393,600 ns/iter (+/- 347,958)
test tests::bench_chunks_1920x1080_iter_sum_2 ... bench:  10,420,442 ns/iter (+/- 242,379)</pre><p></p>
<p>If we look at the assembly of the assertion-less variant, it’s a complete mess now</p>
<p></p><pre class="crayon-plain-tag">.LBB0_19:
  cmp rbx, r13
  mov rcx, r13
  cmova rcx, rbx
  mov rdx, r8
  sub rdx, rcx
  cmp rdx, 4
  mov r11d, 4
  cmovb r11, rdx
  test r11, r11
  je .LBB0_20
  movzx ecx, byte ptr [r15 - 2]
  imul ecx, ecx, 19595
  cmp r11, 1
  jbe .LBB0_22
  movzx esi, byte ptr [r15 - 1]
  imul esi, esi, 38470
  add esi, ecx
  movzx ecx, byte ptr [r15]
  imul ecx, ecx, 7471
  add ecx, esi
  test rdx, rdx
  jne .LBB0_23
  jmp .LBB0_35
.LBB0_20:
  xor ecx, ecx
.LBB0_22:
  test rdx, rdx
  je .LBB0_35
.LBB0_23:
  shr ecx, 16
  mov byte ptr [r10 - 3], cl
  mov byte ptr [r10 - 2], cl
  cmp rdx, 3
  jb .LBB0_36
  inc r9
  mov byte ptr [r10 - 1], cl
  mov byte ptr [r10], cl
  add r10, 4
  add r8, -4
  add rbx, -4
  add r15, 4
  cmp r9, r14
  jb .LBB0_19</pre><p> </p>
<p>In short, there are now various new conditions and jumps for short-circuiting the zip iterator in the various cases. And because of all the noise added, the compiler was not even able to optimize the bounds check for the output slice away anymore (<em>.LBB0_35</em> cases). While it was able to unroll the iterator (note that the 3 <em>imul</em> multiplications are not interleaved with jumps and are actually 3 multiplications instead of yet another loop), which is quite impressive, it couldn’t do anything meaningful with that information it somehow got (it must’ve understood that each chunk has 4 bytes!). This looks like something going wrong somewhere in the optimizer to me.</p>
<p>If we take a look at the variant with the assertions, things look much better</p>
<p></p><pre class="crayon-plain-tag">.LBB3_19:
  cmp r11, r12
  mov r13, r12
  cmova r13, r11
  add r13, r14
  jne .LBB3_33
  inc r9
  movzx ecx, byte ptr [rdx - 2]
  imul r13d, ecx, 19595
  movzx ecx, byte ptr [rdx - 1]
  imul ecx, ecx, 38470
  add ecx, r13d
  movzx ebx, byte ptr [rdx]
  imul ebx, ebx, 7471
  add ebx, ecx
  shr ebx, 16
  mov byte ptr [r10 - 3], bl
  mov byte ptr [r10 - 2], bl
  mov byte ptr [r10 - 1], bl
  mov byte ptr [r10], bl
  add r10, 4
  add r11, -4
  add r14, 4
  add rdx, 4
  cmp r9, r15
  jb .LBB3_19</pre><p> </p>
<p>This is literally the same as the assertion version we had before, except that the reading of the input slice, the multiplications and the additions are happening in iterator order instead of being batched all together. It’s quite impressive that the compiler was able to completely optimize away the zip iterator here, but unfortunately it’s still many times slower than the original version. The reason must be the instruction-reordering. The previous version had all memory reads batched and then the operations batched, which is apparently much better for the internal pipelining of the CPU (it is going to perform the next instructions without dependencies on the previous ones already while waiting for the pending instructions to finish).</p>
<p>It’s also not clear to me why the LLVM optimizer is not able to schedule the instructions the same way here. It apparently has all information it needs for that if no iterator is involved, and both versions are leading to exactly the same assembly except for the order of instructions. This also seems like something fishy.</p>
<p>Nonetheless, we still have our manual bounds check (the assertion) left here and we should really try to get rid of that. No progress so far.</p>
<h3 id="no-more-bounds">Third Optimization – Getting rid of the bounds check finally</h3>
<p>Let’s tackle this from a different angle now. Our problem is apparently that the compiler is not able to understand that each chunk is exactly 4 bytes.</p>
<p>So why don’t we write a new chunks iterator that has always exactly the requested amount of items, instead of potentially less for the very last iteration. And instead of panicking if there are leftover elements, it seems useful to just ignore them. That way we have API that is functionally different from the existing chunks iterator and provides behaviour that is useful in various cases. It’s basically the slice equivalent of the <a href="https://docs.rs/ndarray/0.11.0/ndarray/struct.ArrayBase.html#method.exact_chunks" rel="noopener" target="_top">exact_chunks</a> iterator of the <em>ndarray</em> crate.</p>
<p>By having it functionally different from the existing one, and not just an optimization, I also <a href="https://github.com/rust-lang/rust/pull/47126" rel="noopener" target="_top">submitted</a> it for inclusion in Rust’s standard library and it’s nowadays available as an unstable feature in nightly. Like all newly added API. Nonetheless, the same can also be implemented inside your code with basically the same effect, there are no dependencies on standard library internals.</p>
<p>So, let’s use our new <a href="https://doc.rust-lang.org/nightly/std/primitive.slice.html#method.exact_chunks" rel="noopener" target="_top"><em>exact_chunks</em></a> iterator that is guaranteed (by API) to always give us exactly 4 bytes. In our case this is exactly equivalent to the normal chunks as by construction our slices always have a length that is a multiple of 4, but the compiler can’t infer that information. The resulting code looks as follows</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_exact_chunks(
    in_data: &amp;[u8],
    out_data: &amp;mut [u8],
    in_stride: usize,
    out_stride: usize,
    width: usize,
) {
    assert_eq!(in_data.len() % 4, 0);
    assert_eq!(out_data.len() % 4, 0);
    assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

    let in_line_bytes = width * 4;
    let out_line_bytes = width * 4;

    assert!(in_line_bytes &lt;= in_stride);
    assert!(out_line_bytes &lt;= out_stride);

    for (in_line, out_line) in in_data
        .exact_chunks(in_stride)
        .zip(out_data.exact_chunks_mut(out_stride))
    {
        for (in_p, out_p) in in_line[..in_line_bytes]
            .exact_chunks(4)
            .zip(out_line[..out_line_bytes].exact_chunks_mut(4))
        {
            assert!(in_p.len() == 4);
            assert!(out_p.len() == 4);

            let b = u32::from(in_p[0]);
            let g = u32::from(in_p[1]);
            let r = u32::from(in_p[2]);
            let x = u32::from(in_p[3]);

            let grey = ((r * RGB_Y[0]) + (g * RGB_Y[1]) + (b * RGB_Y[2]) + (x * RGB_Y[3])) / 65536;
            let grey = grey as u8;
            out_p[0] = grey;
            out_p[1] = grey;
            out_p[2] = grey;
            out_p[3] = grey;
        }
    }
}</pre><p> </p>
<p>It’s exactly the same as the previous version with assertions, except for using <em>exact_chunks</em> instead of <em>chunks</em> and the same for the mutable iterator. The resulting benchmark of all our variants now looks as follow</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_no_asserts ... bench:   4,420,145 ns/iter (+/- 139,051)
test tests::bench_chunks_1920x1080_asserts    ... bench:   4,897,046 ns/iter (+/- 166,555)
test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)
test tests::bench_chunks_1920x1080_iter_sum   ... bench:  11,393,600 ns/iter (+/- 347,958)
test tests::bench_chunks_1920x1080_iter_sum_2 ... bench:  10,420,442 ns/iter (+/- 242,379)
test tests::bench_exact_chunks_1920x1080      ... bench:   2,007,459 ns/iter (+/- 112,287)</pre><p></p>
<p>Compared to our initial version this is a speedup of a factor of 2.2, compared to our version with assertions a factor of 1.98. This seems like a worthwhile improvement, and if we look at the resulting assembly there are no bounds checks at all anymore</p>
<p></p><pre class="crayon-plain-tag">.LBB0_10:
  movzx edx, byte ptr [rsi - 2]
  movzx r15d, byte ptr [rsi - 1]
  movzx r12d, byte ptr [rsi]
  imul r13d, edx, 19595
  imul edx, r15d, 38470
  add edx, r13d
  imul ebx, r12d, 7471
  add ebx, edx
  shr ebx, 16
  mov byte ptr [rcx - 3], bl
  mov byte ptr [rcx - 2], bl
  mov byte ptr [rcx - 1], bl
  mov byte ptr [rcx], bl
  add rcx, 4
  add rsi, 4
  dec r10
  jne .LBB0_10</pre><p> </p>
<p>Also due to this the compiler is able to apply some more optimizations and we only have one loop counter for the number of iterations <em>r10</em> and the two pointers <em>rcx</em> and <em>rsi</em> that are increased/decreased in each iteration. There is no tracking of the remaining slice lengths anymore, as in the assembly of the original version (and the versions with assertions).</p>
<h3 id="summary">Summary</h3>
<p>So overall we got a speedup of a factor of 2.2 while still writing very high-level Rust code with iterators and not falling back to unsafe code or using SIMD. The optimizations the Rust compiler is applying are quite impressive and the Rust marketing line of <em>zero-cost abstractions</em> is really visible in reality here.</p>
<p>The same approach should also work for many similar algorithms, and thus many similar multimedia related algorithms where you iterate over slices and operate on fixed-size chunks.</p>
<p>Also the above shows that as a first step it’s better to write clean and understandable high-level Rust code without worrying too much about performance (assume the compiler can optimize well), and only afterwards take a look at the generated assembly and check which instructions should really go away (like bounds checking). In many cases this can be achieved by adding assertions in strategic places, or like in this case by switching to a slightly different abstraction that is closer to the actual requirements (however I believe the compiler should be able to produce the same code with the help of assertions with the normal chunks iterator, but making that possible requires improvements to the LLVM optimizer probably).</p>
<p>And if all does not help, there’s still the escape hatch of <em>unsafe</em> (for using functions like <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.get_unchecked" rel="noopener" target="_top"><em>slice::get_unchecked()</em></a> or going down to raw pointers) and the possibility of using SIMD instructions (by using <a href="https://github.com/AdamNiederer/faster" rel="noopener" target="_top"><em>faster</em></a> or <a href="https://crates.io/crates/stdsimd" rel="noopener" target="_top"><em>stdsimd</em></a> directly). But in the end this should be a last resort for those little parts of your code where optimizations are needed and the compiler can’t be easily convinced to do it for you.</p>
<h3 id="split-at">Addendum: <em>slice::split_at</em></h3>
<p>User <em>newpavlov</em> <a href="https://www.reddit.com/r/rust/comments/7rxrka/speeding_up_rgb_to_grayscale_conversion_in_rust/dt0ejao/" rel="noopener" target="_top">suggested on Reddit</a> to use repeated <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.split_at" rel="noopener" target="_top"><em>slice::split_at</em></a> in a while loop for similar performance.</p>
<p>This would for example like</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_split_at(
    in_data: &amp;[u8],
    out_data: &amp;mut [u8],
    in_stride: usize,
    out_stride: usize,
    width: usize,
) {
    assert_eq!(in_data.len() % 4, 0);
    assert_eq!(out_data.len() % 4, 0);
    assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

    let in_line_bytes = width * 4;
    let out_line_bytes = width * 4;

    assert!(in_line_bytes &lt;= in_stride);
    assert!(out_line_bytes &lt;= out_stride);

    for (in_line, out_line) in in_data
        .exact_chunks(in_stride)
        .zip(out_data.exact_chunks_mut(out_stride))
    {
        let mut in_pp: &amp;[u8] = in_line[..in_line_bytes].as_ref();
        let mut out_pp: &amp;mut [u8] = out_line[..out_line_bytes].as_mut();
        assert!(in_pp.len() == out_pp.len());

        while in_pp.len() &gt;= 4 {
            let (in_p, in_tmp) = in_pp.split_at(4);
            let (out_p, out_tmp) = { out_pp }.split_at_mut(4);
            in_pp = in_tmp;
            out_pp = out_tmp;

            let b = u32::from(in_p[0]);
            let g = u32::from(in_p[1]);
            let r = u32::from(in_p[2]);
            let x = u32::from(in_p[3]);

            let grey = ((r * RGB_Y[0]) + (g * RGB_Y[1]) + (b * RGB_Y[2]) + (x * RGB_Y[3])) / 65536;
            let grey = grey as u8;
            out_p[0] = grey;
            out_p[1] = grey;
            out_p[2] = grey;
            out_p[3] = grey;
        }
    }
}</pre><p> </p>
<p>Performance-wise this brings us very close to the <em>exact_chunks</em> version</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_exact_chunks_1920x1080      ... bench: 1,965,631 ns/iter (+/- 58,832)
test tests::bench_split_at_1920x1080          ... bench: 2,046,834 ns/iter (+/- 35,990)</pre><p></p>
<p>and the assembly is also very similar</p>
<p></p><pre class="crayon-plain-tag">.LBB0_10:
  add rbx, -4
  movzx r15d, byte ptr [rsi]
  movzx r12d, byte ptr [rsi + 1]
  movzx edx, byte ptr [rsi + 2]
  imul r13d, edx, 19595
  imul r12d, r12d, 38470
  imul edx, r15d, 7471
  add edx, r12d
  add edx, r13d
  shr edx, 16
  movzx edx, dl
  imul edx, edx, 16843009
  mov dword ptr [rcx], edx
  lea rcx, [rcx + 4]
  add rsi, 4
  cmp rbx, 3
  ja .LBB0_10</pre><p> </p>
<p>Here the compiler even optimizes the storing of the value into a single write operation of 4 bytes, at the cost of an additional multiplication and zero-extend register move.</p>
<p>Overall this code performs very well too, but in my opinion it looks rather ugly compared to the versions using the different chunks iterators. Also this is basically what the <em>exact_chunks</em> iterator does internally: repeatedly calling <em>slice::split_at</em>. In theory both versions could lead to the very same assembly, but the LLVM optimizer is currently handling both slightly different.</p>
<h3 id="faster">Addendum 2: SIMD with faster</h3>
<p><a href="https://github.com/AdamNiederer" rel="noopener" target="_top">Adam Niederer</a>, author of <a href="https://github.com/AdamNiederer/faster" rel="noopener" target="_top">faster</a>, <a href="https://github.com/sdroege/bgrx-to-grayscale-benchmark.rs/pull/1" rel="noopener" target="_top">provided a PR</a> that implements the same algorithm with faster to make explicit use of SIMD instructions if available.</p>
<p>Due to some codegen issues, this currently has to be compiled with the CPU being selected as <em>nehalem</em>, i.e. by running <em>RUSTFLAGS=”-C target-cpu=nehalem” cargo +nightly bench</em>, but it provides yet another speedup by a factor of up to 1.27x compared to the fastest previous solution and 2.7x compared to the initial solution:</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_asserts                     ... bench:   4,539,286 ns/iter (+/- 106,265)
test tests::bench_chunks_1920x1080_asserts_2                   ... bench:   3,550,683 ns/iter (+/- 96,917)
test tests::bench_chunks_1920x1080_iter_sum                    ... bench:   5,233,238 ns/iter (+/- 114,671)
test tests::bench_chunks_1920x1080_iter_sum_2                  ... bench:   3,532,059 ns/iter (+/- 94,964)
test tests::bench_chunks_1920x1080_no_asserts                  ... bench:   4,468,269 ns/iter (+/- 89,329)
test tests::bench_chunks_1920x1080_no_asserts_faster           ... bench:   2,476,077 ns/iter (+/- 54,877)
test tests::bench_chunks_1920x1080_no_asserts_faster_unstrided ... bench:   1,642,980 ns/iter (+/- 108,034)
test tests::bench_exact_chunks_1920x1080                       ... bench:   2,078,950 ns/iter (+/- 64,536)
test tests::bench_split_at_1920x1080                           ... bench:   2,096,603 ns/iter (+/- 107,420)</pre><p></p>
<p>The code in question is very similar to what you would’ve written with <a href="https://cgit.freedesktop.org/gstreamer/orc" rel="noopener" target="_top">ORC</a>, especially the unstrided version. You basically operate on multiple elements at once, doing the same operation on each, but both versions do this in a slightly different way.</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_chunks_no_asserts_faster_unstrided(in_data: &amp;[u8], out_data: &amp;mut [u8]) {
    // Relies on vector width which is a multiple of 4
    assert!(u8s::WIDTH % 4 == 0 &amp;&amp; u32s::WIDTH % 4 == 0);

    const RGB_Y: [u32; 16] = [19595, 38470, 7471, 0, 19595, 38470, 7471, 0, 19595, 38470, 7471, 0, 19595, 38470, 7471, 0];
    let rgbvec = u32s::load(&amp;RGB_Y, 0);
    in_data.simd_iter(u8s(0)).simd_map(|v| {
        let (a, b) = v.upcast();
        let (a32, b32) = a.upcast();
        let (c32, d32) = b.upcast();

        let grey32a = a32 * rgbvec / u32s(65536);
        let grey32b = b32 * rgbvec / u32s(65536);
        let grey32c = c32 * rgbvec / u32s(65536);
        let grey32d = d32 * rgbvec / u32s(65536);

        let grey16a = grey32a.saturating_downcast(grey32b);
        let grey16b = grey32c.saturating_downcast(grey32d);

        let grey = grey16a.saturating_downcast(grey16b);
        grey
    }).scalar_fill(out_data);
}

pub fn bgrx_to_gray_chunks_no_asserts_faster(in_data: &amp;[u8], out_data: &amp;mut [u8]) {
    // Sane, but slowed down by faster's current striding implementation.
    in_data.stride_four(tuplify!(4, u8s(0))).zip().simd_map(|(r, g, b, _)| {
        let (r16a, r16b) = r.upcast();
        let (r32a, r32b) = r16a.upcast();
        let (r32c, r32d) = r16b.upcast();

        let (g16a, g16b) = g.upcast();
        let (g32a, g32b) = g16a.upcast();
        let (g32c, g32d) = g16b.upcast();

        let (b16a, b16b) = b.upcast();
        let (b32a, b32b) = b16a.upcast();
        let (b32c, b32d) = b16b.upcast();

        let grey32a = (r32a * u32s(19595) + g32a * u32s(38470) + b32a * u32s(7471)) / u32s(65536);
        let grey32b = (r32b * u32s(19595) + g32b * u32s(38470) + b32b * u32s(7471)) / u32s(65536);
        let grey32c = (r32c * u32s(19595) + g32c * u32s(38470) + b32c * u32s(7471)) / u32s(65536);
        let grey32d = (r32d * u32s(19595) + g32d * u32s(38470) + b32d * u32s(7471)) / u32s(65536);

        let grey16a = grey32a.saturating_downcast(grey32b);
        let grey16b = grey32c.saturating_downcast(grey32d);

        let grey = grey16a.saturating_downcast(grey16b);
        grey
    }).scalar_fill(out_data);
}</pre><p> </p></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://wingolog.org/archives/2018/01/17/instruction-explosion-in-guile"><span xml:base="https://gstreamer.freedesktop.org/planet/atom.xml">instruction explosion in guile</span></a><div class="lastUpdated">2018年1月17日 18:30</div></h3><div xml:base="https://gstreamer.freedesktop.org/planet/atom.xml" class="feedEntryContent"><div><p>Greetings, fellow Schemers and compiler nerds: I bring fresh nargery!</p><p><b>instruction explosion</b></p><p>A couple years ago I made a list of <a href="https://wingolog.org/archives/2016/02/04/guile-compiler-tasks">compiler tasks for Guile</a>.  Most of these are still open, but I've been chipping away at the one labeled "instruction explosion":</p><blockquote><p> Now we get more to the compiler side of things. Currently in Guile's VM there are instructions like <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Inlined-Scheme-Instructions.html#Inlined-Scheme-Instructions">vector-ref</a>.  This is a little silly: there are also instructions to branch on the type of an object (<a href="https://www.gnu.org/software/guile/docs/master/guile.html/Branch-Instructions.html#Branch-Instructions">br-if-tc7</a> in this case), to get the vector's length, and to do a branching integer comparison. Really we should replace vector-ref with a combination of these test-and-branches, with real control flow in the function, and then the actual ref should use some more primitive unchecked memory reference instruction. Optimization could end up hoisting everything but the primitive unchecked memory reference, while preserving safety, which would be a win. But probably in most cases optimization wouldn't manage to do this, which would be a lose overall because you have more instruction dispatch.</p><p>Well, this transformation is something we need for native compilation anyway. I would accept a patch to do this kind of transformation on the master branch, after version 2.2.0 has forked. In theory this would remove most all high level instructions from the VM, making the bytecode closer to a virtual CPU, and likewise making it easier for the compiler to emit native code as it's working at a lower level.  </p></blockquote><p>Now that I'm getting close to finished I wanted to share some thoughts.  <a href="https://lists.gnu.org/archive/html/guile-devel/2018-01/msg00003.html">Previous progress reports on the mailing list</a>.</p><p><b>a simple loop</b></p><p>As an example, consider this loop that sums the 32-bit floats in a bytevector.  I've annotated the code with lines and columns so that you can correspond different pieces to the assembly.</p><pre>   0       8   12     19
 +-v-------v---v------v-
 |
1| (use-modules (rnrs bytevectors))
2| (define (f32v-sum bv)
3|   (let lp ((n 0) (sum 0.0))
4|     (if (&lt; n (bytevector-length bv))
5|         (lp (+ n 4)
6|             (+ sum (bytevector-ieee-single-native-ref bv n)))
7|          sum)))
</pre><p>The assembly for the loop before instruction explosion went like this:</p><pre>L1:
  17    (handle-interrupts)     at (unknown file):5:12
  18    (uadd/immediate 0 1 4)
  19    (bv-f32-ref 1 3 1)      at (unknown file):6:19
  20    (fadd 2 2 1)            at (unknown file):6:12
  21    (s64&lt;? 0 4)             at (unknown file):4:8
  22    (jnl 8)                ;; -&gt; L4
  23    (mov 1 0)               at (unknown file):5:8
  24    (j -7)                 ;; -&gt; L1
</pre><p>So, already Guile's compiler has hoisted the <tt>(bytevector-length bv)</tt> and unboxed the loop index <i>n</i> and accumulator <i>sum</i>.  This work aims to simplify further by exploding <tt>bv-f32-ref</tt>.</p><p><b>exploding the loop</b></p><p>In practice, instruction explosion happens in CPS conversion, as we are converting the Scheme-like <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Tree_002dIL.html#Tree_002dIL">Tree-IL</a> language down to the <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Continuation_002dPassing-Style.html#Continuation_002dPassing-Style">CPS soup</a> language.  When we see a Tree-Il primcall (a call to a known primitive), instead of lowering it to a corresponding CPS primcall, we inline a whole blob of code.</p><p>In the concrete case of <tt>bv-f32-ref</tt>, we'd inline it with something like the following:</p><pre>(unless (and (heap-object? bv)
             (eq? (heap-type-tag bv) %bytevector-tag))
  (error "not a bytevector" bv))
(define len (word-ref bv 1))
(define ptr (word-ref bv 2))
(unless (and (&lt;= 4 len)
             (&lt;= idx (- len 4)))
  (error "out of range" idx))
(f32-ref ptr len)
</pre><p>As you can see, there are four branches hidden in the <tt>bv-f32-ref</tt>: two to check that the object is a bytevector, and two to check that the index is within range.  In this explanation we assume that the offset <i>idx</i> is already unboxed, but actually unboxing the index ends up being part of this work as well.</p><p>One of the goals of instruction explosion was that by breaking the operation into a number of smaller, more orthogonal parts, native code generation would be easier, because the compiler would only have to know about those small bits.  However without an optimizing compiler, it would be better to <a href="https://docs.google.com/document/d/15hmBrCrmMZzra8ekhl7meQZBodeahsauilNDImif5Xg/edit#heading=h.8zevmmfemvcv">reify a call out to a specialized <tt>bv-f32-ref</tt> runtime routine</a> instead of inlining all of this code -- probably whatever language you write your runtime routine in (C, rust, whatever) will do a better job optimizing than your compiler will.</p><p>But with an optimizing compiler, there is the possibility of removing possibly everything but the <tt>f32-ref</tt>.  Guile doesn't quite get there, but almost; here's the post-explosion optimized assembly of the inner loop of f32v-sum:</p><pre>L1:
  27    (handle-interrupts)
  28    (tag-fixnum 1 2)
  29    (s64&lt;? 2 4)             at (unknown file):4:8
  30    (jnl 15)               ;; -&gt; L5
  31    (uadd/immediate 0 2 4)  at (unknown file):5:12
  32    (u64&lt;? 2 7)             at (unknown file):6:19
  33    (jnl 5)                ;; -&gt; L2
  34    (f32-ref 2 5 2)
  35    (fadd 3 3 2)            at (unknown file):6:12
  36    (mov 2 0)               at (unknown file):5:8
  37    (j -10)                ;; -&gt; L1
</pre><p><b>good things</b></p><p>The first thing to note is that unlike the "before" code, there's no instruction in this loop that can throw an exception.  Neat.</p><p>Next, note that there's no type check on the bytevector; the <a href="https://wingolog.org/archives/2015/07/28/loop-optimizations-in-guile">peeled iteration</a> preceding the loop already proved that the bytevector is a bytevector.</p><p>And indeed there's no reference to the bytevector at all in the loop!  The value being dereferenced in <tt>(f32-ref 2 5 2)</tt> is a raw pointer.  (Read this instruction as, "sp[2] = *(float*)((byte*)sp[5] + (uptrdiff_t)sp[2])".)  The compiler does something interesting; the <tt>f32-ref</tt> CPS primcall actually takes three arguments: the garbage-collected object protecting the pointer, the pointer itself, and the offset.  The object itself doesn't appear in the residual code, but including it in the <tt>f32-ref</tt> primcall's inputs keeps it alive as long as the <tt>f32-ref</tt> itself is alive.</p><p><b>bad things</b></p><p>Then there are the limitations.  Firstly, instruction 28 tags the u64 loop index as a fixnum, but never uses the result.  Why is this here?  Sadly it's because the value is used in the bailout at L2.  Recall this pseudocode:</p><pre>(unless (and (&lt;= 4 len)
             (&lt;= idx (- len 4)))
  (error "out of range" idx))
</pre><p>Here the <a>error</a> ends up lowering to a <a href="https://lists.gnu.org/archive/html/guile-devel/2018-01/msg00003.html"><tt>throw</tt> CPS term</a> that the compiler recognizes as a bailout and renders out-of-line; cool.  But it uses <i>idx</i> as an argument, as a tagged SCM value.  The compiler untags the loop index, but has to keep a tagged version around for the error cases.</p><p>The right fix is probably some kind of allocation sinking pass that sinks the <tt>tag-fixnum</tt> to the bailouts.  Oh well.</p><p>Additionally, there are two tests in the loop.  Are both necessary?  Turns out, yes :( Imagine you have a bytevector of length 102<b>5</b>.  The loop continues until the last ref at offset 1024, which is within bounds of the bytevector but there's one one byte available at that point, so we need to throw an exception at this point.  The compiler did as good a job as we could expect it to do.</p><p><b>is is worth it?  where to now?</b></p><p>On the one hand, instruction explosion is a step sideways.  The code is more optimal, but it's more instructions.  Because Guile currently has a bytecode VM, that means more total interpreter overhead.  Testing on a 40-megabyte bytevector of 32-bit floats, the exploded <tt>f32v-sum</tt> completes in 115 milliseconds compared to around 97 for the earlier version.</p><p>On the other hand, it is very easy to imagine how to compile these instructions to native code, either ahead-of-time or via a simple template JIT.  You practically just have to look up the instructions in the corresponding ISA reference, is all.  The result should perform quite well.</p><p>I will probably take a whack at a simple template JIT first that does no register allocation, then ahead-of-time compilation with register allocation.  Getting the AOT-compiled artifacts to dynamically link with runtime routines is a sufficient pain in my mind that I will put it off a bit until later.  I also need to figure out a good strategy for truly polymorphic operations like general integer addition; probably involving inline caches.</p><p>So that's where we're at :)  Thanks for reading, and happy hacking in Guile in 2018!</p></div></div></div><div style="clear: both;"></div></div>
    </div>
  </body>
</html>
