<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

<title>Planet GStreamer</title>

<meta name="generator" content="Planet/2.0 +http://www.planetplanet.org">

<link rel="shortcut icon" href="favicon.png">
<link rel="alternate" href="http://gstreamer.freedesktop.org/planet/atom.xml" title="" type="application/atom+xml">

<link media="all" href="index.css" type="text/css" rel="stylesheet">
</head>
<body>

<div id="banner">
<h1>Planet GStreamer</h1>
</div>

<div id="entries">
<div class="daygroup">
<h2 class="date">December 30, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://blog.mikeasoft.com/?p=752" lang="en-US">
<h3><a href="http://blog.mikeasoft.com/" title="Michael Sheldon's Stuff">Michael Sheldon</a> ‚Äî <a href="http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/">Speech Recognition ‚Äì Mozilla‚Äôs DeepSpeech, GStreamer and IBus</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="elleo.png" alt="(Michael Sheldon)" width="80" height="76">
<p>Recently Mozilla released <a href="https://github.com/mozilla/DeepSpeech">an open source implementation of Baidu‚Äôs DeepSpeech architecture</a>, along with a pre-trained model using data collected as part of their <a href="https://voice.mozilla.org/">Common Voice</a> project.</p>
<p>In an attempt to make it easier for application developers to start working with the DeepSpeech model I‚Äôve developed a GStreamer plugin, an IBus plugin and created some PPAs. To demonstrate what‚Äôs possible here‚Äôs a video of the IBus plugin providing speech recognition to any application under Linux:</p>
<p></p><center><br>
<br>
<br>
<a href="https://youtu.be/Kjos5s0ZZDM">Video of DeepSpeech IBus Plugin</a><br>
</center><br>
<p></p>
<h3>GStreamer DeepSpeech Plugin</h3>
<p>I‚Äôve created a GStreamer element which can be placed into an audio pipeline, it will then report any recognised speech via bus messages. It automatically segments audio based on configurable silence thresholds making it suitable for continuous dictation.</p>
<p>Here‚Äôs a couple of example pipelines using gst-launch.</p>
<p>To perform speech recognition on a file, printing all bus messages to the terminal:</p>
<p><code>gst-launch-1.0 -m filesrc location=/path/to/file.ogg ! decodebin ! audioconvert ! audiorate ! audioresample ! deepspeech ! fakesink</code></p>
<p>To perform speech recognition on audio recorded from the default system microphone, with changes to the silence thresholds:</p>
<p><code>gst-launch-1.0 -m pulsesrc ! audioconvert ! audiorate ! audioresample ! deepspeech silence-threshold=0.3 silence-length=20 ! fakesink</code></p>
<p>The source code is available here: <a href="https://github.com/Elleo/gst-deepspeech">https://github.com/Elleo/gst-deepspeech</a>.</p>
<h3>IBus Plugin</h3>
<p>I‚Äôve also created a proof of concept IBus plugin which allows speech recognition to be used as an input method for virtually any application. It uses the above GStreamer plugin to perform speech recognition and then commits the text to the currently focused input field whenever a bus message is received from the deepspeech element.</p>
<p>It‚Äôll need a lot more work before it‚Äôs really useful, especially in terms of adding in various voice editing commands, but hopefully it‚Äôll provide a useful starting point for something more complete.</p>
<p>The source code is available here: <a href="https://github.com/Elleo/ibus-deepspeech">https://github.com/Elleo/ibus-deepspeech</a></p>
<h3>PPAs</h3>
<p>To make it extra easy to get started playing around with these projects I‚Äôve also created a couple of PPAs for Ubuntu 17.10:</p>
<p><a href="https://launchpad.net/~michael-sheldon/+archive/ubuntu/deepspeech">DeepSpeech PPA</a> ‚Äì This contains packages for libdeepspeech, libdeepspeech-dev, libtensorflow-cc and deepspeech-model (be warned, the model is around 1.3GB).</p>
<p><a href="https://launchpad.net/~michael-sheldon/+archive/ubuntu/gst-deepspeech">gst-deepspeech PPA</a> ‚Äì This contains packages for my GStreamer and IBus plugins (gstreamer1.0-deepspeech and ibus-deepspeech). Please note that you‚Äôll also need the DeepSpeech PPA enabled to fulfil the dependencies of these packages.</p>
<p>I‚Äôd love to hear about any projects that find these plugins useful <img src="1f642.png" alt="üôÇ" class="wp-smiley"></p></div>

<p class="date">
<a href="http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/">by Mike at December 30, 2017 09:13 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 22, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=515" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> ‚Äî <a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/">GStreamer Rust bindings release 0.10.0 &amp; gst-plugin release 0.1.0</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dr√∂ge)" width="80" height="80">
<p>Today I‚Äôve released version 0.10.0 of the <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a> <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> <a href="https://crates.io/crates/gstreamer" rel="noopener" target="_top">bindings</a>, and <a href="https://coaxion.net/blog/2016/05/writing-gstreamer-plugins-and-elements-in-rust/" rel="noopener" target="_top">after a journey of more than 1¬Ω years</a> the first release of the GStreamer plugin writing infrastructure crate <a href="https://crates.io/crates/gst-plugin" rel="noopener" target="_top">‚Äúgst-plugin‚Äù</a>.</p>
<p>Check the repositories<a href="https://github.com/sdroege/gstreamer-rs" rel="noopener" target="_top">¬π</a><a href="https://github.com/sdroege/gst-plugin-rs" rel="noopener" target="_top">¬≤</a> of both for more details, the code and various examples.</p>
<h4>GStreamer Bindings</h4>
<p>Some of the changes since the 0.9.0 release were already outlined in the previous blog post, and most of the other changes were also things I found while writing GStreamer plugins. For the full changelog, take a look at the <a href="https://github.com/sdroege/gstreamer-rs/blob/master/gstreamer/CHANGELOG.md#0100---2017-12-22" rel="noopener" target="_top">CHANGELOG.md</a> in the repository.</p>
<p>Other changes include</p>
<ul>
<li>I went over the whole API in the last days, added any missing things I found, simplified API as it made sense, changed functions to take <i>Option&lt;_&gt;</i> if allowed, etc.</li>
<li>Bindings for <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.SliceTypeFind.html#method.type_find" rel="noopener" target="_top">using</a> and <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.TypeFind.html#method.register" rel="noopener" target="_top">writing</a> typefinders. Typefinders are the part of GStreamer that try to guess what kind of media is to be handled based on looking at the bytes. Especially writing those in Rust seems worthwhile, considering that basically all of the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-base/log/gst/typefind/gsttypefindfunctions.c" rel="noopener" target="_top">GIT log</a> of the existing typefinders consists of fixes for various kinds of memory-safety problems.</li>
<li>Bindings for the <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.Registry.html" rel="noopener" target="_top">Registry</a> and PluginFeature were added, as well as fixing the relevant API that works with paths/filenames to actually work on <a href="https://doc.rust-lang.org/std/path/struct.Path.html" rel="noopener" target="_top">Paths</a></li>
<li>Bindings for the GStreamer Net library were added, allowing to build applications that synchronize their media of the network by using <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.PtpClock.html" rel="noopener" target="_top">PTP</a>, <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.NtpClock.html" rel="noopener" target="_top">NTP</a> or a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.NetClientClock.html" rel="noopener" target="_top">custom</a> GStreamer protocol (for which there also exists a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.NetTimeProvider.html" rel="noopener" target="_top">server</a>). This could be used for building video-walls, systems recording the same scene from multiple cameras, etc. and provides (depending on network conditions) up to &lt; 1ms synchronization between devices.</li>
</ul>
<p>Generally, this is something like a ‚Äú1.0‚Äù release for me now (due to depending on too many pre-1.0 crates this is not going to be 1.0 anytime soon). The basic API is all there and nicely usable now and hopefully without any bugs, the known-missing APIs are not too important for now and can easily be added at a later time when needed. At this point I don‚Äôt expect many API changes anymore.</p>
<h4>GStreamer Plugins</h4>
<p>The other important part of this announcement is the first release of the <a href="https://crates.io/crates/gst-plugin" rel="noopener" target="_top">‚Äúgst-plugin‚Äù</a> crate. This provides the basic infrastructure for writing GStreamer plugins and elements in Rust, without having to write any unsafe code.</p>
<p>I started experimenting with using Rust for this more than 1¬Ω years ago, and while a lot of things have changed in that time, this release is a nice milestone. In the beginning there were no GStreamer bindings and I was writing everything manually, and there were also still quite a few pieces of code written in C. Nowadays everything is in Rust and using the automatically generated GStreamer bindings.</p>
<p>Unfortunately there is no real documentation for any of this yet, there‚Äôs only the autogenerated rustdoc documentation available from <a href="https://sdroege.github.io/rustdoc/gst-plugin/gst_plugin/" rel="noopener" target="_top">here</a>, and various example GStreamer plugins inside the <a href="https://github.com/sdroege/gst-plugin-rs" rel="noopener" target="_top">GIT repository </a>that can be used as a starting point. And various people already wrote their GStreamer plugins in Rust based on this.</p>
<p>The basic idea of the API is however that everything is as Rust-y as possible. Which might not be too much due to having to map subtyping, virtual methods and the like to something reasonable in Rust, but I believe it‚Äôs nice to use now. You basically only have to implement one or more traits on your structs, and that‚Äôs it. There‚Äôs still quite some boilerplate required, but it‚Äôs far less than what would be required in C. The best example at this point might be the <a href="https://github.com/sdroege/gst-plugin-rs/blob/master/gst-plugin-audiofx/src/audioecho.rs" rel="noopener" target="_top">audioecho</a> element.</p>
<p>Over the next days (or weeks?) I‚Äôm not going to write any documentation yet, but instead will write a couple of very simple, minimal elements that do basically nothing and can be used as starting points to learn how all this works together. And will write another blog post or two about the different parts of writing a GStreamer plugin and element in Rust, so that all of you can get started with that.</p>
<p>Let‚Äôs hope that the number of new GStreamer plugins written in C is going to decrease in the future, and maybe even new people who would‚Äôve never done that in C, with all the footguns everywhere, can get started with writing GStreamer plugins in Rust now.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/">by slomo at December 22, 2017 04:52 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="https://k-d-w.org/103 at https://k-d-w.org" lang="en">
<h3><a href="https://k-d-w.org/" title="Sebastian P√∂lsterl's blog">Sebastian P√∂lsterl</a> ‚Äî <a href="https://k-d-w.org/node/103">Denoising Autoencoder as TensorFlow estimator</a></h3>
<div class="entry">
<div class="content">
<div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even"><div class="tex2jax"><p>I recently started to use Google's deep learning framework TensorFlow. Since version 1.3, TensorFlow includes a <a href="https://www.tensorflow.org/get_started/estimator">high-level interface</a> inspired by scikit-learn. Unfortunately, as of version 1.4, only 3 different classification and 3 different regression models implementing the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface are included. To better understand the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface, <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> API, and components in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">tf-slim</a>, I started to implement a simple Autoencoder and applied it to the well-known MNIST dataset of handwritten digits. This post is about my journey and is split in the following sections:
</p><ol><li><a href="https://k-d-w.org/#estimators">Custom Estimators</a></li>
<li><a href="https://k-d-w.org/#autoencoder-net">Autoencoder network architecture</a></li>
<li><a href="https://k-d-w.org/#autoencoder-model-fn">Autoencoder as TensorFlow Estimator</a></li>
<li><a href="https://k-d-w.org/#dataset-api">Using the Dataset API</a></li>
<li><a href="https://k-d-w.org/#denoising-autoencoder">Denoising Autocendoer</a></li>
</ol><p>I will assume that you are familiar with TensorFlow basics. The full code is available at <a href="https://github.com/sebp/tf_autoencoder">https://github.com/sebp/tf_autoencoder</a>.</p>
<!--break--><h2>Estimators</h2>
<p><a name="estimators" id="estimators"></a>The <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">tf.estimator.Estimator</a> is at the heart TenorFlow's high-level interface and is similar to <a href="https://keras.io/models/model/">Kera's Model API</a>. It hides most of the boilerplate required to train a model: managing <span class="geshifilter"><code class="text geshifilter-text">Sessions</code></span>, writing summary statistics for TensorBoard, or saving and loading checkpoints. An <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> has three main methods: <span class="geshifilter"><code class="text geshifilter-text">train</code></span>, <span class="geshifilter"><code class="text geshifilter-text">evaluate</code></span>, and <span class="geshifilter"><code class="text geshifilter-text">predict</code></span>. Each of these methods requires a callable input function as first argument that feeds the data to the estimator (more on that later).</p>
<p><img src="estimators1.jpeg" width="350"></p>
<h2>Custom estimators</h2>
<p>You can write your own custom model implementing the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface by passing a function returning an instance of <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.EstimatorSpec</code></span> as first argument to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator</code></span>.</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">def</span> model_fn<span class="br0">(</span>features<span class="sy0">,</span> labels<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; ‚Ä¶
&nbsp; &nbsp; <span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; train_op<span class="sy0">=</span>train_op<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; eval_metric_ops<span class="sy0">=</span>eval_metric_ops<span class="br0">)</span></pre></div>
</div>
<p>The first argument ‚Äì <span class="geshifilter"><code class="text geshifilter-text">mode</code></span> ‚Äì is one of <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.TRAIN</code></span>, <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.EVAL</code></span> or <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.PREDICT</code></span> and determines which of the remaining values must be provided.</p>
<p>In <span class="geshifilter"><code class="text geshifilter-text">TRAIN</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">loss</code></span>: A Tensor containing a scalar loss value.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">train_op</code></span>: An Op that runs one step of training. We can use the return value of <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss">tf.contrib.layers.optimize_loss</a> here.</li>
</ul><p>In <span class="geshifilter"><code class="text geshifilter-text">EVAL</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">loss</code></span>: A scalar Tensor containing the loss on the validation data.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">eval_metric_ops</code></span>: A dictionary that maps metric names to Tensors of metrics to calculate, typically, one of the <a href="https://www.tensorflow.org/api_docs/python/tf/metrics">tf.metrics</a> functions.</li>
</ul><p>In <span class="geshifilter"><code class="text geshifilter-text">PREDICT</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">predictions</code></span>: A dictionary that maps key names of your choice to Tensors containing the predictions from the model.</li>
</ul><p>An important difference to the Estimators included with TensorFlow is that we need to call relevant <span class="geshifilter"><code class="text geshifilter-text">tf.summary</code></span> functions in <span class="geshifilter"><code class="text geshifilter-text">model_fn</code></span> ourselves. However, the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> will take care of writing summaries to disk so we can inspect them in TenorBoard.</p>
<h2>Autoencoder model</h2>
<p><a name="autoencoder-net" id="autoencoder-net"></a><img src="autoencoder.svg" alt="Autoencoder architecture" width="400"></p>
<p>The Autoencoder model is straightforward, it consists of two major parts: an encoder and an decoder. The encoder has an input layer (28*28 = 784 dimensions in the case of MNIST) and one or more hidden layers, decreasing in size. In the decoder, we reverse the operations of the encoder by blowing the output of the smallest hidden layer up to the size of the input (optionally, with hidden layers of increasing size in-between). The loss function computes the difference between the original image and the reconstructed image (the output of the decoder). Common loss functions are mean squared error and <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">cross-entropy</a>.</p>
<p>To construct the encoder network, we specify a list containing the number of hidden units for each layer and (optionally) add dropout layers in-between:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">def</span> encoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">for</span> num_hidden_units <span class="kw1">in</span> hidden_units:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_hidden_units<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> dropout <span class="kw1">is</span> <span class="kw1">not</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">dropout</span><span class="br0">(</span>net<span class="sy0">,</span> is_training<span class="sy0">=</span>is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; add_hidden_layer_summary<span class="br0">(</span>net<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div>
</div>
<p>where <span class="geshifilter"><code class="text geshifilter-text">add_hidden_layer_summary</code></span> adds a histogram of the activations and the fraction of non-zero activations to be displayed in TensorBoard. The latter is particularly useful when debugging networks with <a href="https://cs231n.github.io/neural-networks-1/">rectified linear units (ReLU)</a>. If too many hidden units return 0 values early during optimization, the model won't be able to learn anymore, in which case one would typically try to lower the learning rate or choose a different activation function.</p>
<p>The network of the decoder is almost identical, we just explicitly use a linear activation function (<span class="geshifilter"><code class="text geshifilter-text">activation_fn=None</code></span>) and no dropout in the last layer:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">def</span> decoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">for</span> num_hidden_units <span class="kw1">in</span> hidden_units<span class="br0">[</span>:-<span class="nu0">1</span><span class="br0">]</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_hidden_units<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> dropout <span class="kw1">is</span> <span class="kw1">not</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">dropout</span><span class="br0">(</span>net<span class="sy0">,</span> is_training<span class="sy0">=</span>is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; add_hidden_layer_summary<span class="br0">(</span>net<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>net<span class="sy0">,</span> hidden_units<span class="br0">[</span>-<span class="nu0">1</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>
&nbsp; &nbsp; tf.<span class="me1">summary</span>.<span class="me1">histogram</span><span class="br0">(</span><span class="st0">'activation'</span><span class="sy0">,</span> net<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div>
</div>
<p>You may have noticed that we did no specify any activation function so far. Thanks to TenorFlow's <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/framework/arg_scope">arg_scope context manager</a>, we can easily set the activation function for <em>all</em> fully connected layers. At the same time we set an appropriate weight initializer and  (optionally) use weight decay:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">def</span> autoencoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> activation_fn<span class="sy0">,</span> dropout<span class="sy0">,</span> weight_decay<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; is_training <span class="sy0">=</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>
&nbsp;
&nbsp; &nbsp; weights_init <span class="sy0">=</span> slim.<span class="me1">initializers</span>.<span class="me1">variance_scaling_initializer</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">if</span> weight_decay <span class="kw1">is</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp; &nbsp; <span class="kw1">else</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_reg <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">l2_regularizer</span><span class="br0">(</span>weight_decay<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">with</span> slim.<span class="me1">arg_scope</span><span class="br0">(</span><span class="br0">[</span>tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_initializer<span class="sy0">=</span>weights_init<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer<span class="sy0">=</span>weights_reg<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span>activation_fn<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> encoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; n_features <span class="sy0">=</span> inputs.<span class="me1">shape</span><span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>.<span class="me1">value</span>
&nbsp; &nbsp; &nbsp; &nbsp; decoder_units <span class="sy0">=</span> hidden_units<span class="br0">[</span>:-<span class="nu0">1</span><span class="br0">]</span><span class="br0">[</span>::-<span class="nu0">1</span><span class="br0">]</span> + <span class="br0">[</span>n_features<span class="br0">]</span>
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> decoder<span class="br0">(</span>net<span class="sy0">,</span> decoder_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div>
</div>
<p>where <span class="geshifilter"><code class="text geshifilter-text">slim.initializers.variance_scaling_initializer</code></span> corresponds to the <a href="https://arxiv.org/abs/1502.01852">initialization of He et al.</a>, which is the <a href="https://cs231n.github.io/neural-networks-2/#init">current recommendation</a> for networks with ReLU activations.</p>
<p>This concludes the architecture of the autoencoder. Next, we need to implement the <span class="geshifilter"><code class="text geshifilter-text">model_fn</code></span> function passed to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator</code></span> as outlined above.</p>
<h2>Autoencoder model_fn</h2>
<p><a name="autoencoder-model-fn" id="autoencoder-model-fn"></a>First, we construct the network's architecture using the <span class="geshifilter"><code class="text geshifilter-text">autoencoder</code></span> function described above:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">logits <span class="sy0">=</span> autoencoder<span class="br0">(</span>inputs<span class="sy0">=</span>features<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_units<span class="sy0">=</span>hidden_units<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;activation_fn<span class="sy0">=</span>activation_fn<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dropout<span class="sy0">=</span>dropout<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;weight_decay<span class="sy0">=</span>weight_decay<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mode<span class="sy0">=</span>mode<span class="br0">)</span></pre></div>
</div>
<p>Subsequent steps depend on the value of <span class="geshifilter"><code class="text geshifilter-text">mode</code></span>. In prediction mode, we merely have to return the reconstructed image, therefore we make sure all values are within the interval [0; 1] by applying the sigmoid function:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">probs <span class="sy0">=</span> tf.<span class="me1">nn</span>.<span class="me1">sigmoid</span><span class="br0">(</span>logits<span class="br0">)</span>
predictions <span class="sy0">=</span> <span class="br0">{</span><span class="st0">"prediction"</span>: probs<span class="br0">}</span>
<span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">PREDICT</span>:
&nbsp; &nbsp; <span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="br0">)</span></pre></div>
</div>
<p>In training and evaluation mode, we need to compute the loss, which is cross-entropy in this example:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">tf.<span class="me1">losses</span>.<span class="me1">sigmoid_cross_entropy</span><span class="br0">(</span>labels<span class="sy0">,</span> logits<span class="br0">)</span>
total_loss <span class="sy0">=</span> tf.<span class="me1">losses</span>.<span class="me1">get_total_loss</span><span class="br0">(</span>add_regularization_losses<span class="sy0">=</span>is_training<span class="br0">)</span></pre></div>
</div>
<p>The second line is needed to add the $\ell_2$-losses used in weight decay.</p>
<p>Most importantly, training relies on choosing an optimizer, here we use <a href="http://arxiv.org/abs/1412.6980">Adam</a> and an exponential learning rate decay. The latter dynamically updates the learning rate during training according to the formula<br>
$$<br>
\text{decayed learning rate} = \text{base learning rate} \cdot 0.96^{\lfloor i / 1000 \rfloor} ,<br>
$$ where $i$ is the current iteration. It would probably work as well without learning rate decay, but I included it for the sake of completeness.</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>:
&nbsp; &nbsp; train_op <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">optimize_loss</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; optimizer<span class="sy0">=</span><span class="st0">"Adam"</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; learning_rate<span class="sy0">=</span>learning_rate<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; learning_rate_decay_fn<span class="sy0">=</span><span class="kw1">lambda</span> lr<span class="sy0">,</span> gs: tf.<span class="me1">train</span>.<span class="me1">exponential_decay</span><span class="br0">(</span>lr<span class="sy0">,</span> gs<span class="sy0">,</span> <span class="nu0">1000</span><span class="sy0">,</span> <span class="nu0">0.96</span><span class="sy0">,</span> staircase<span class="sy0">=</span><span class="kw2">True</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; global_step<span class="sy0">=</span>tf.<span class="me1">train</span>.<span class="me1">get_global_step</span><span class="br0">(</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; summaries<span class="sy0">=</span><span class="br0">[</span><span class="st0">"learning_rate"</span><span class="sy0">,</span> <span class="st0">"global_gradient_norm"</span><span class="br0">]</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="co1"># Add histograms for trainable variables</span>
&nbsp; &nbsp; <span class="kw1">for</span> var <span class="kw1">in</span> tf.<span class="me1">trainable_variables</span><span class="br0">(</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">summary</span>.<span class="me1">histogram</span><span class="br0">(</span>var.<span class="me1">op</span>.<span class="me1">name</span><span class="sy0">,</span> var<span class="br0">)</span></pre></div>
</div>
<p>Note that we add a histogram of all trainable variables for TensorBoard in the last part.</p>
<p>Finally, we compute the root mean squared error when in evaluation mode:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">EVAL</span>:
&nbsp; &nbsp; eval_metric_ops <span class="sy0">=</span> <span class="br0">{</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="st0">"rmse"</span>: tf.<span class="me1">metrics</span>.<span class="me1">root_mean_squared_error</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">cast</span><span class="br0">(</span>labels<span class="sy0">,</span> tf.<span class="me1">float64</span><span class="br0">)</span><span class="sy0">,</span> tf.<span class="me1">cast</span><span class="br0">(</span>probs<span class="sy0">,</span> tf.<span class="me1">float64</span><span class="br0">)</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="br0">}</span></pre></div>
</div>
<p>and return the specification of our autoencoder estimator:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="sy0">,</span>
&nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; train_op<span class="sy0">=</span>train_op<span class="sy0">,</span>
&nbsp; &nbsp; eval_metric_ops<span class="sy0">=</span>eval_metric_ops<span class="br0">)</span></pre></div>
</div>
<p>&nbsp;</p>
<h2>Feeding data to an Estimator via the Dataset API</h2>
<p><a name="dataset-api" id="dataset-api"></a>Once we constructed our estimator, e.g. via</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">estimator <span class="sy0">=</span> AutoEncoder<span class="br0">(</span>hidden_units<span class="sy0">=</span><span class="br0">[</span><span class="nu0">128</span><span class="sy0">,</span> <span class="nu0">64</span><span class="sy0">,</span> <span class="nu0">32</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropout<span class="sy0">=</span><span class="kw2">None</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weight_decay<span class="sy0">=</span><span class="nu0">1e-5</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; learning_rate<span class="sy0">=</span><span class="nu0">0.001</span><span class="br0">)</span></pre></div>
</div>
<p>we would like to train it by calling <span class="geshifilter"><code class="text geshifilter-text">train</code></span>, which expects a callable that returns two tensors, one representing the input data and one the groundtruth data. The easiest way would be to use <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn">tf.estimator.inputs.numpy_input_fn</a>, but instead I want to introduce TensorFlow's Dataset API, which is more generic.</p>
<p>The Dataset API comprises two elements:</p>
<ol><li><span class="geshifilter"><code class="text geshifilter-text">tf.data.Dataset</code></span> represents a dataset and any transformations applied to it.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">tf.data.Iterator</code></span> is used to extract elements from a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span>. In particular, <span class="geshifilter"><code class="text geshifilter-text">Iterator.get_next()</code></span> returns the next element of a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> and typically is what is fed to an estimator.</li>
</ol><p>Here, I'm using what is called an <em>initializable</em> Iterator, inspired by <a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0">this post</a>. We define one placeholder for the input image and one for the groundtruth image and initialize the placeholders before training starts using a hook. First, let's create a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> from the placeholders:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">placeholders <span class="sy0">=</span> <span class="br0">[</span>
&nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span>data.<span class="me1">dtype</span><span class="sy0">,</span> data.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'input_image'</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span>data.<span class="me1">dtype</span><span class="sy0">,</span> data.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'groundtruth_image'</span><span class="br0">)</span>
<span class="br0">]</span>
dataset <span class="sy0">=</span> tf.<span class="me1">data</span>.<span class="me1">Dataset</span>.<span class="me1">from_tensor_slices</span><span class="br0">(</span>placeholders<span class="br0">)</span></pre></div>
</div>
<p>Next, we shuffle the dataset and allow retrieving data from it until the specified number of epochs has been reached:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="me1">shuffle</span><span class="br0">(</span>buffer_size<span class="sy0">=</span><span class="nu0">10000</span><span class="br0">)</span>
dataset <span class="sy0">=</span> dataset.<span class="me1">repeat</span><span class="br0">(</span>num_epochs<span class="br0">)</span></pre></div>
</div>
<p>When creating input for evaluation or prediction, we are going to skip these two steps.</p>
<p>Finally, we combine multiple elements into a batch and create an iterator from the dataset:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="me1">batch</span><span class="br0">(</span>batch_size<span class="br0">)</span>
&nbsp;
iterator <span class="sy0">=</span> dataset.<span class="me1">make_initializable_iterator</span><span class="br0">(</span><span class="br0">)</span>
next_example<span class="sy0">,</span> next_label <span class="sy0">=</span> iterator.<span class="me1">get_next</span><span class="br0">(</span><span class="br0">)</span></pre></div>
</div>
<p>To initialize the placeholders, we need to call <span class="geshifilter"><code class="text geshifilter-text">tf.Sesssion.run</code></span> with <span class="geshifilter"><code class="python geshifilter-python">feed_dict <span class="sy0">=</span> <span class="br0">{</span>placeholders<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span>: input_data<span class="sy0">,</span> placeholders<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>: groundtruth_data<span class="br0">}</span></code></span>. Since the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> will create a <span class="geshifilter"><code class="text geshifilter-text">Session</code></span> for us, we need a way to call our initialization code after the session has been created and before training begins. The Estimator's train, evaluate and predict methods accept a list of <span class="geshifilter"><code class="text geshifilter-text">SessionRunHook</code></span> subclasses as the hooks argument, which we can use to inject our code in the right place. Therefore, we first create a generic hook that runs after the session has been created:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">class</span> IteratorInitializerHook<span class="br0">(</span>tf.<span class="me1">train</span>.<span class="me1">SessionRunHook</span><span class="br0">)</span>:
&nbsp; &nbsp; <span class="st0">"""Hook to initialise data iterator after Session is created."""</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">def</span> <span class="kw4">__init__</span><span class="br0">(</span><span class="kw2">self</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span> <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">def</span> after_create_session<span class="br0">(</span><span class="kw2">self</span><span class="sy0">,</span> session<span class="sy0">,</span> coord<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="st0">"""Initialise the iterator after the session has been created."""</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">assert</span> <span class="kw2">callable</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span><span class="br0">(</span>session<span class="br0">)</span></pre></div>
</div>
<p>To make things a little bit nicer, we create an <span class="geshifilter"><code class="text geshifilter-text">InputFunction</code></span> class which implements the <span class="geshifilter"><code class="text geshifilter-text">__call__</code></span> method. Thus, it will behave like a function and we can pass it directly to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator.train</code></span> and related methods.</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">class</span> InputFunction:
&nbsp; &nbsp; <span class="kw1">def</span> <span class="kw4">__init__</span><span class="br0">(</span><span class="kw2">self</span><span class="sy0">,</span> data<span class="sy0">,</span> batch_size<span class="sy0">,</span> num_epochs<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">data</span> <span class="sy0">=</span> data
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">batch_size</span> <span class="sy0">=</span> batch_size
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">mode</span> <span class="sy0">=</span> mode
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">num_epochs</span> <span class="sy0">=</span> num_epochs
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">init_hook</span> <span class="sy0">=</span> IteratorInitializerHook<span class="br0">(</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp;<span class="kw1">def</span> <span class="kw4">__call__</span><span class="br0">(</span><span class="kw2">self</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># Define placeholders</span>
&nbsp; &nbsp; &nbsp; &nbsp; placeholders <span class="sy0">=</span> <span class="br0">[</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">dtype</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'input_image'</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">dtype</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'reconstruct_image'</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="br0">]</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># Build dataset pipeline</span>
&nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> tf.<span class="me1">data</span>.<span class="me1">Dataset</span>.<span class="me1">from_tensor_slices</span><span class="br0">(</span>placeholders<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> <span class="kw2">self</span>.<span class="me1">mode</span> <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">shuffle</span><span class="br0">(</span>buffer_size<span class="sy0">=</span><span class="nu0">10000</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">repeat</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">num_epochs</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">batch</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">batch_size</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># create iterator from dataset</span>
&nbsp; &nbsp; &nbsp; &nbsp; iterator <span class="sy0">=</span> dataset.<span class="me1">make_initializable_iterator</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; next_example<span class="sy0">,</span> next_label <span class="sy0">=</span> iterator.<span class="me1">get_next</span><span class="br0">(</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># create initialization hook</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">def</span> _init<span class="br0">(</span>sess<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; feed_dict <span class="sy0">=</span> <span class="kw2">dict</span><span class="br0">(</span><span class="kw2">zip</span><span class="br0">(</span>placeholders<span class="sy0">,</span> <span class="br0">[</span><span class="kw2">self</span>.<span class="me1">data</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span><span class="br0">]</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sess.<span class="me1">run</span><span class="br0">(</span>iterator.<span class="me1">initializer</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;feed_dict<span class="sy0">=</span>feed_dict<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">init_hook</span>.<span class="me1">iterator_initializer_func</span> <span class="sy0">=</span> _init
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">return</span> next_example<span class="sy0">,</span> next_label</pre></div>
</div>
<p>Finally, we can use the <span class="geshifilter"><code class="text geshifilter-text">InputFunction</code></span> class to train our autoencoder for 30 epochs:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1"><span class="kw1">from</span> tensorflow.<span class="me1">examples</span>.<span class="me1">tutorials</span>.<span class="me1">mnist</span> <span class="kw1">import</span> input_data <span class="kw1">as</span> mnist_data
&nbsp;
mnist <span class="sy0">=</span> mnist_data.<span class="me1">read_data_sets</span><span class="br0">(</span><span class="st0">'mnist_data'</span><span class="sy0">,</span> one_hot<span class="sy0">=</span><span class="kw2">False</span><span class="br0">)</span>
train_input_fn <span class="sy0">=</span> InputFunction<span class="br0">(</span>
&nbsp; &nbsp; data<span class="sy0">=</span>mnist.<span class="me1">train</span>.<span class="me1">images</span><span class="sy0">,</span>
&nbsp; &nbsp; batch_size<span class="sy0">=</span><span class="nu0">256</span><span class="sy0">,</span>
&nbsp; &nbsp; num_epochs<span class="sy0">=</span><span class="nu0">30</span><span class="sy0">,</span>
&nbsp; &nbsp; mode<span class="sy0">=</span>tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span><span class="br0">)</span>
autoencoder.<span class="me1">train</span><span class="br0">(</span>train_input_fn<span class="sy0">,</span> hooks<span class="sy0">=</span><span class="br0">[</span>train_input_fn.<span class="me1">init_hook</span><span class="br0">]</span><span class="br0">)</span></pre></div>
</div>
<p>The video below shows ten reconstructed images from the test data and their corresponding groundtruth after each epoch of training:<br>Your browser does not support the video tag.</p>
<h2>Denoising Autoencoder</h2>
<p><a name="denoising-autoencoder" id="denoising-autoencoder"></a>A <em>denoising autoencoder</em> is slight variation on the autoencoder described above. The only difference is that input images are randomly corrupted before they are fed to the autoencoder (we still use the original, uncorrupted image to compute the loss). This acts as a form of regularization to avoid overfitting.</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">noise_factor <span class="sy0">=</span> <span class="nu0">0.5</span> &nbsp;<span class="co1"># a float in [0; 1)</span>
&nbsp;
<span class="kw1">def</span> add_noise<span class="br0">(</span>input_img<span class="sy0">,</span> groundtruth_img<span class="br0">)</span>:
&nbsp; &nbsp; noise <span class="sy0">=</span> noise_factor * tf.<span class="me1">random_normal</span><span class="br0">(</span>input_img.<span class="me1">shape</span>.<span class="me1">as_list</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
&nbsp; &nbsp; input_corrupted <span class="sy0">=</span> tf.<span class="me1">clip_by_value</span><span class="br0">(</span>tf.<span class="me1">add</span><span class="br0">(</span>input_img<span class="sy0">,</span> noise<span class="br0">)</span><span class="sy0">,</span> <span class="nu0">0</span>.<span class="sy0">,</span> <span class="nu0">1</span>.<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> input_corrupted<span class="sy0">,</span> groundtruth</pre></div>
</div>
<p>The function above takes two Tensors representing the input and groundtruth image, respectively, and corrupts the input image by the specified amount of noise. We can use this function to transform all of the images using Dataset's <span class="geshifilter"><code class="text geshifilter-text">map</code></span> function:</p>
<div class="geshifilter">
<div class="python geshifilter-python">
<pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="kw2">map</span><span class="br0">(</span>add_noise<span class="sy0">,</span> num_parallel_calls<span class="sy0">=</span><span class="nu0">4</span><span class="br0">)</span>
dataset <span class="sy0">=</span> dataset.<span class="me1">prefetch</span><span class="br0">(</span><span class="nu0">512</span><span class="br0">)</span></pre></div>
</div>
<p>The function passed to map will be part of the compute graph, thus you have to use TensorFlow operations to modify your input or use <a href="https://www.tensorflow.org/api_docs/python/tf/py_func">tf.py_func</a>. The <span class="geshifilter"><code class="text geshifilter-text">num_parallel_calls</code></span> arguments speeds up preprocessing significantly, because multiple images are transformed in parallel. The second line ensures a certain amount of corrupted images are precomputed, otherwise the transformation would only be applied when executing <span class="geshifilter"><code class="text geshifilter-text">iterator.get_next()</code></span>, which would result in a delay for each batch and bad GPU utilization. The video below shows the groundtruth, input and output of the denoising autoencoder for up to 60 epochs:<br>Your browser does not support the video tag.</p>
<p>I hope this tutorial gave you some insight on how to implement a custom TensorFlow estimator and use the Dataset API.</p>
<h2>References</h2>
<ul><li><a href="https://www.tensorflow.org/extend/estimators">Creating Estimators in tf.estimator</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/datasets">Importing data</a></li>
<li><a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html"> Introduction to TensorFlow Datasets and Estimators </a></li>
<li><a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0">Higher-Level APIs in TensorFlow</a></li>
</ul></div></div></div></div></div>

<p class="date">
<a href="https://k-d-w.org/node/103">by sebp at December 22, 2017 11:39 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 19, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2780" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> ‚Äî <a href="https://blogs.gnome.org/uraeus/2017/12/19/why-hasnt-the-year-of-the-linux-desktop-happened-yet/">Why hasn‚Äôt The Year of the Linux Desktop happened yet?</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>Having spent 20 years of my life on Desktop Linux I thought I should write up my thinking about why we so far hasn‚Äôt had the Linux on the Desktop breakthrough and maybe more importantly talk about the avenues I see for that breakthrough still happening. There has been a lot written of this over the years, with different people coming up with their explanations. My thesis is that there really isn‚Äôt one reason, but rather a range of issues that all have contributed to holding the Linux Desktop back from reaching a bigger market. Also to put this into context, success here in my mind would be having something like 10% market share of desktop systems, that to me means we reached critical mass. So let me start by listing some of the main reasons I see for why we are not at that 10% mark today before going onto talking about how I think that goal might possible to reach going forward.</p>
<p><b>Things that have held us back</b></p>
<ul>
<li>Fragmented market</li>
<p>One of the most common explanations for why the Linux Desktop never caught on more is the fragmented state of the Linux Desktop space. We got a large host of desktop projects like GNOME, KDE, Enlightenment, Cinnamon etc. and a even larger host of distributions shipping these desktops. I used to think this state should get a lot of the blame, and I still believe it owns some of the blame, but I have also come to conclude in recent years that it is probably more of a symptom than a cause. If someone had come up with a model strong enough to let Desktop Linux break out of its current technical user niche then I am now convinced that model would easily have also been strong enough to leave the Linux desktop fragmentation behind for all practical purposes. Because at that point the alternative desktops for Linux would be as important as the alternative MS Windows shells are. So in summary, the fragmentation hasn‚Äôt helped for sure and is still not helpful, but it is probably a problem that has been overstated.</p>
<li>Lack of special applications</li>
<p>Another common item that has been pointed to is the lack of applications. We know that for sure in the early days of Desktop Linux the challenge you always had when trying to convince anyone of moving to Desktop Linux was that they almost invariably had one or more application they relied on that was only available on Windows. I remember in one of my first jobs after University when I worked as a sysadmin we had a long list of these applications that various parts of the organization relied on, be that special tools to interface with a supplier, with the bank, dealing with nutritional values of food in the company cafeteria etc. This is a problem that has been in rapid decline for the last 5-10 years due to the move to web applications, but I am sure that in a given major organization you can still probably find a few of them. But between the move to the web and Wine I don‚Äôt think this is a major issue anymore. So in summary this was a major roadblock in the early years, but is a lot less of an impediment these days.
</p>
<li>Lack of big name applications</li>
<p>
Adopting a new platform is always easier if you can take the applications you are familiar with you. So the lack of things like MS Office and Adobe Photoshop would always contribute to making a switch less likely. Just because in addition to switching OS you would also have to learn to use new tools. And of course along those lines there where always the challenge of file format compatibility, in the early days in a hard sense that you simply couldn‚Äôt reliably load documents coming from some of these applications, to more recently softer problems like lack of metrically identical fonts. The font for example issue has been mostly resolved due to Google releasing fonts metrically compatible with MS default fonts a few years ago, but it was definitely a hindrance for adoption for many years. The move to web for a lot of these things has greatly reduced this problem too, with organizations adopting things like Google Docs at rapid pace these days. So in summary, once again something that used to be a big problem, but which is at least a lot less of a problem these days, but of course there are still apps not available for Linux that does stop people from adopting desktop linux.
</p>
<li>Lack of API and ABI stability</li>
<p>This is another item that many people have brought up over the years. I think I have personally vacillated over the importance of this one multiple times over the years. Changing APIs are definitely not a fun thing for developers to deal with, it adds extra work often without bringing direct benefit to their application. Linux packaging philosophy probably magnified this problem for developers with anything that could be split out and packaged separately was, meaning that every application was always living on top of a lot of moving parts. That said the reason I am sceptical to putting to much blame onto this is that you could always find stable subsets to rely on. So for instance if you targeted GTK2 or Qt back in the day and kept away from some of the more fast moving stuff offered by GNOME and KDE you would not be hit with this that often. And of course if the Linux Desktop market share had been higher then people would have been prepared to deal with these challenges regardless, just like they are on other platforms that keep changing and evolving quickly like the mobile operating systems.</p>
<li>Apple resurgence</li>
<p>This might of course be the result of subjective memory, but one of the times where it felt like there could have been a Linux desktop breakthrough was at the same time as Linux on the server started making serious inroads. The old Unix workstation market was coming apart and moving to Linux already, the worry of a Microsoft monopoly was at its peak and Apple was in what seemed like mortal decline. There was a lot of media buzz around the Linux desktop and VC funded companies was set up to try to build a business around it. Reaching some kind of critical mass seemed like it could be within striking distance. Of course what happened here was that Steve Jobs returned to Apple and we suddenly had MacOSX come onto the scene taking at least some air out of the Linux Desktop space. The importance of this one I do find exceptionally hard to quantify though, part of me feels it had a lot of impact, but on the other hand it isn‚Äôt 100% clear to me that the market and the players at the time would have been able to capitalize even if Apple had gone belly-up.</p>
<li>Microsoft aggressive response</li>
<p>In the first 10 years of Desktop linux there was no doubt that Microsoft was working hard to try to nip any sign of Desktop Linux gaining any kind of foothold or momentum. I do remember for instance that Novell for quite some time was trying to establish a serious Desktop Linux business after having bought Miguel de Icaza‚Äôs company Helix Code. However it seemed like a pattern quickly emerged that every time Novell or anyone else tried to announce a major Linux desktop deal, Microsoft came running in offering next to free Windows licensing to get people to stay put. Looking at Linux migrations even seemed like it became a goto policy for negotiating better prices from Microsoft. So anyone wanting to attack the desktop market with Linux would have to contend with not only market inertia, but a general depression of the price of a desktop operating systems, and knowing that Microsoft would respond to any attempt to build momentum around Linux desktop deals with very aggressive sales efforts. So in summary, this probably played an important part as it meant that the pay per copy/subscription business model that for instance Red Hat built their server business around became really though to make work in the desktop space. Because the price point ended up so low it required gigantic volumes to become profitable, which of course is a hard thing to quickly achieve when fighting against an entrenched market leader. So in summary Microsoft in some sense successfully fended of Linux breaking through as a competitor although it could be said they did so at the cost of fatally wounding the per copy fee business model they built their company around and ensured that the next wave of competitors Microsoft had to deal with like iOS and Android based themselves on business models where the cost of the OS was assumed to be zero, thus contributing to the Windows Phone efforts being doomed.
</p>
<li>Piracy</li>
<p>One of the big aspirations of the Linux community from the early days was the idea that a open source operating system would enable more people to be able to afford running a computer and thus take part in the economic opportunities that the digital era would provide. For the desktop space there was always this idea that while Microsoft was entrenched in North America and Europe there was this ocean of people in the rest of the world that had never used a computer before and thus would be more open to adopting a desktop linux system. I think this so far panned out only in a limited degree, where running a Linux distribution has surely opened job and financial opportunities for a lot of people, yet when you look at things from a volume perspective most of these potential Linux users found that a pirated Windows copy suited their needs just as much or more. As an anecdote here, there was recently a bit of noise and writing around the sudden influx of people on Steam playing Player Unknown: Battlegrounds, as it caused the relatively Linux marketshare to decline. So most of these people turned out to be running Windows in Mandarin language. Studies have found that about 70% of all software in China is unlicensed so I don‚Äôt think I am going to far out on a limb here assuming that most of these gamers are not providing Microsoft with Windows licensing revenue, but it does illustrate the challenge of getting these people onto Linux as they already are getting an operating system for free. So in summary, in addition to facing cut throat pricing from Microsoft in the business sector one had to overcome the basically free price of pirated software in the consumer sector.</p>
<li>Red Hat mostly stayed away</li>
<p>So few people probably don‚Äôt remember or know this, but Red Hat was actually founded as a desktop Linux company. The first major investment in software development that Red Hat ever did was setting up the Red Hat Advanced Development Labs, hiring a bunch of core GNOME developers to move that effort forward. But when Red Hat pivoted to the server with the introduction of Red Hat Enterprise Linux the desktop quickly started playing second fiddle. And before I proceed, all these events where many years before I joined the company, so just as with my other points here, read this as an analysis of someone without first hand knowledge. So while Red Hat has always offered a desktop product and have always been a major contributor to keeping the Linux desktop ecosystem viable, Red Hat was focused on the server side solutions and the desktop offering was always aimed more narrowly things like technical workstation customers and people developing towards the RHEL server. It is hard to say how big an impact Red Hats decision to not go after this market has had, on one side it would probably have been beneficial to have the Linux company with the deepest pockets and the strongest brand be a more active participant, but on the other hand staying mostly out of the fight gave other companies a bigger room to give it a go.</p>
<li>Canonical business model not working out</li>
<p>This bullet point is probably going to be somewhat controversial considering I work for Red Hat (although this is my private blog my with own personal opinions), but on the other hand I feel one can not talk about the trajectory of the Linux Desktop over the last decade without mentioning Canonical and Ubuntu. So I have to assume that when Mark Shuttleworth was mulling over doing Ubuntu he probably saw a lot of the challenges that I mention above, especially the revenue generation challenges that the competition from Microsoft provided. So in the end he decided on the standard internet business model of the time, which was to try to quickly build up a huge userbase and then dealing with how to monetize it later on. So Ubuntu was launched with an effective price point of zero, in fact you could even get install media sent to you for free. The effort worked in the sense that Ubuntu quickly became the biggest player in the Linux desktop space and it certainly helped the Linux desktop marketshare grow in the early years. Unfortunately I think it still basically failed, and the reason I am saying that is that it didn‚Äôt manage to grow big enough to provide Ubuntu with enough revenue through their appstore or their partner agreements to allow them to seriously re-invest in the Linux Desktop and invest in the kind of marketing effort needed to take Linux to a less super technical audience. So once it plateaued what they had was enough revenue to keep what is a relatively barebones engineering effort going, but not the kind of income that would allow them to steadily build the Linux Desktop market further. Mark then tried to capitalize on the mindshare and market share he had managed to build, by branching out into efforts like their TV and Phone efforts, but all those efforts eventually failed.<br>
It would probably be an article in itself to deeply discuss why the grow userbase strategy failed here vs why for instance Android succeeded with this model, but I think the short version goes back to the fact that you had an entrenched market leader and the Linux Desktop isn‚Äôt different enough from a Mac or Windows desktops to drive the type of market change the transition from feature phones to smartphones was.<br>
And to be clear I am not criticizing Mark here for the strategy he choose, if I where in his shoes back when he started Ubuntu I am not sure I would have been able to come up a different strategy that would have been plausible to succeed from his starting point. That said it did contribute to even further push the expected price of desktop Linux down and thus making it even harder for people to generate significant revenue from desktop linux. On the other hand one can argue that this would likely have happened anyway due to competitive pressure and Windows piracy. Canonicals recent focus pivot away from the desktop towards trying to build a business in the server and IoT space is in some sense a natural consequence of hitting the desktop growth plateau and not having enough revenue to invest in further growth.<br>
So in summary, what was once seen as the most likely contender to take the Linux Desktop to critical mass turned out to have taken off with to little rocket fuel and eventually gravity caught up with them. And what we can never know for sure is if they during this run sucked so much air out of the market that it kept someone who could have taken us further with a different business model from jumping in.</p>
<li>Original device manufacturer support</li>
<p>THis one is a bit of a chicken and egg issue. Yes, lack of (perfect) hardware support has for sure kept Linux back on the Desktop, but lack of marketshare has also kept hardware support back. As with any system this is a question of reaching critical mass despite your challenges and thus eventually being so big that nobody can afford ignoring you. This is an area where we even today are still not fully there yet, but which I do feel we are getting closer all the time. When I installed Linux for the very first time, which I think was Red Hat Linux 3.1 (pre RHEL days) I spent about a weekend fiddling just to get my sound card working. I think I had to grab a experimental driver from somewhere and compile it myself. These days I mostly expect everything to work out of the box except more unique hardware like ambient light sensors or fingerprint readers, but even such devices are starting to land, and thanks to efforts from vendors such as Dell things are looking pretty good here. But the memory of these issues is long so a lot of people, especially those not using Linux themselves, but have heard about Linux, still assume hardware support is a very much hit or miss issue still.
</p>
</ul>
<h1>What does the future hold?</h1>
<p>So any who has read my blog posts probably know I am an optimist by nature. This isn‚Äôt just some kind of genetic disposition towards optimism, but also a philosophical belief that optimism breeds opportunity while pessimism breeds failure. So just because we haven‚Äôt gotten the Linux Desktop to 10% marketshare so far doesn‚Äôt mean it will not happen going forward. It just means we haven‚Äôt achieved it so far. One of the key identifies of open source is that it is incredibly hard to kill, because unlike proprietary software, just because a company goes out of business or decides to shut down a part of its business, the software doesn‚Äôt go away or stop getting developed. As long as there is a strong community interested in pushing it forward it remains and evolves and thus when opportunity comes knocking again it is ready to try again. And that is definitely true of Desktop Linux which from a technical perspective is better than it has ever been, the level of polish is higher than ever before, the level of hardware support is better than ever before and the range of software available is better than ever before.</p>
<p>
And the important thing to remember here is that we don‚Äôt exist in a vacuum, the world around us constantly change too, which means that the things that blocked us in the past or the companies that blocked us in the past might no be around or able to block us tomorrow. Apple and Microsoft are very different companies today than they where 10 or 20 years ago and their focus and who they compete with are very different. The dynamics of the desktop software market is changing with new technologies and paradigms all the time. Like how online media consumption has moved from things like your laptop to phones and tablets for instance. 5 years ago I would have considered iTunes a big competitive problem, today the move to streaming services like Spotify, Hulu, Amazon or Netflix has made iTunes feel archaic and a symbol of bygone times.
</p>
<p>
And many of the problems we faced before, like weird Windows applications without a Linux counterpart has been washed away by the switch to browser based applications. And while Valve‚Äôs SteamOS effort didn‚Äôt taken off, it has provided Linux users with access to a huge catalog of games, removing a reason that I know caused a few of my friends to mostly abandon using Linux on their computers. And you can actually as a consumer buy linux from a range of vendors now, who try to properly support Linux on their hardware. And this includes a major player like Dell and smaller outfits like System76 and Purism.</p>
<p>
And since I do work for Red Hat managing our Desktop Engineering team I should address the question of if Red Hat will be a major driver in taking Desktop linux to that 10%? Well Red Hat will continue to support end evolve our current RHEL Workstation product, and we are seeing a steady growth of new customers for it. So if you are looking for a solid developer workstation for your company you should absolutely talk to Red Hat sales about RHEL Workstation, but Red Hat is not looking at aggressively targeting general consumer computers anytime soon. Caveat here, I am not a C-level executive at Red Hat, so I guess there is always a chance Jim Whitehurst or someone else in the top brass is mulling over a gigantic new desktop effort and I simply don‚Äôt know about it, but I don‚Äôt think it is likely and thus would not advice anyone to hold their breath waiting for such a thing to be announced :). That said Red Hat like any company out there do react to market opportunities as they arise, so who knows what will happen down the road. And we will definitely keep pushing Fedora Workstation forward as the place to experience the leading edge of the Desktop Linux experience and a great portal into the world of Linux on servers and in the cloud.</p>
<p>So to summarize; there are a lot of things happening in the market that could provide the right set of people the opportunity they need to finally take Linux to critical mass. Whether there is anyone who has the timing and skills to pull it off is of course always an open question and it is a question which will only be answered the day someone does it. The only thing I am sure of is that Linux community are providing a stronger technical foundation for someone to succeed with than ever before, so the question is just if someone can come up with the business model and the market skills to take it to the next level. There is also the chance that it will come in a shape we don‚Äôt appreciate today, for instance maybe <a href="https://en.wikipedia.org/wiki/Chrome_OS">ChromeOS</a> evolves into a more full fledged operating system as it grows in popularity and thus ends up being the Linux on the Desktop end game? Or maybe Valve decides to relaunch their SteamOS effort and it provides the foundation for a major general desktop growth? Or maybe market opportunities arise that will cause us at <a href="http://www.redhat.com/">Red Hat</a> to decide to go after the desktop market in a wider sense than we do today? Or maybe <a href="https://endlessos.com/home/">Endless</a> succeeds with their vision for a Linux desktop operating system?  Or maybe the idea of a desktop operating system gets supplanted to the degree that we in the end just sit there saying ‚ÄòAlexa, please open the IDE and take dictation of this new graphics driver I am writing‚Äô (ok, probably not that last one ;)
</p>
<p>
And to be fair there are a lot of people saying that Linux already made it on the desktop in the form of things like Android tablets. Which is technically correct as Android does run on the Linux kernel, but I think for many of us it feels a bit more like a distant cousin as opposed to a close family member both in terms of use cases it targets and in terms of technological pedigree.</p>
<p>
As a sidenote, I am heading of on Yuletide vacation tomorrow evening, taking my wife and kids to Norway to spend time with our family there. So don‚Äôt expect a lot new blog posts from me until I am back from <a href="https://devconf.cz/">DevConf</a> in early February. I hope to see many of you at DevConf though, it is a great conference and Brno is a great town even in freezing winter. As we say in Norway, there is no such thing as bad weather, it is only bad clothing.</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/12/19/why-hasnt-the-year-of-the-linux-desktop-happened-yet/">by uraeus at December 19, 2017 04:00 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 15, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2773" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> ‚Äî <a href="https://blogs.gnome.org/uraeus/2017/12/15/some-predictions-for-2018/">Some predictions for 2018</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>So I spent a few hours polishing my crystal ball today, so here are some predictions for Linux on the Desktop in 2018. The advantage of course for me to publish these now is that I can then later selectively quote the ones I got right to prove my brilliance and the internet can selectively quote the ones I got wrong to prove my stupidity :) </p>
<p><b>Prediction 1: Meson becomes the defacto build system of the Linux community</b></p>
<p>Meson has been going from strength to strength this year and a lot of projects<br>
which passed on earlier attempts to replace autotools has adopted it. I predict this<br>
trend will continue in 2018 and that by the end of the year everyone agrees that Meson<br>
has replaced autotools as the Linux community build system of choice. That said I am not<br>
convinced the Linux kernel itself will adopt Meson in 2018.</p>
<p><b>Prediction 2: Rust puts itself on a clear trajectory to replace C and C++ for low level programming</b></p>
<p>Another rising start of 2017 is the programming language Rust. And while its pace of adoption<br>
will be slower than Meson I do believe that by the time 2018 comes to a close the general opinion is<br>
that Rust is the future of low level programming, replacing old favorites like C and C++. Major projects<br>
like GNOME and GStreamer are already adopting Rust at a rapid pace and I believe even more projects will<br>
join them in 2018.</p>
<p><b>Prediction 3: Apples decline as a PC vendor becomes obvious</b></p>
<p>
Ever since Steve Jobs died it has become quite clear in my opinion that the emphasis<br>
on the traditional desktop is fading from Apple. The pace of hardware refreshes seems<br>
to be slowing and MacOS X seems to be going more and more stale. Some pundits have already<br>
started pointing this out and I predict that in 2018 Apple will be no longer consider the<br>
cool kid on the block for people looking for laptops, especially among the tech savvy crowd.<br>
Hopefully a good opportunity for Linux on the desktop to assert itself more.</p>
<p><b>Prediction 4: Traditional distro packaging for desktop applications<br>
will start fading away in favour of Flatpak</b></p>
<p>From where I am standing I think 2018 will be the breakout year for Flatpak as a replacement<br>
for gettings your desktop applications as RPMS or debs. I predict that by the end of 2018 more or<br>
less every Linux Desktop user will be at least running 1 flatpak on their system.</p>
<p><b>Prediction 5: Linux Graphics competitive across the board</b></p>
<p>I think 2018 will be a breakout year for Linux graphics support. I think our GPU drivers and API will be competitive with any other platform both in completeness and performance. So by the end of 2018 I predict that you will see Linux game ports by major porting houses<br>
like Aspyr and Feral that perform just as well as their Windows counterparts. What is more I also predict that by the end of 2018 discreet graphics will be considered a solved problem on Linux.</p>
<p><b>Prediction 6: H265 will be considered a failure</b></p>
<p>I predict that by the end of 2018 H265 will be considered a failed codec effort and the era of royalty bearing media codecs will effectively start coming to and end. H264 will be considered the last successful royalty bearing codec and all new codecs coming out will<br>
all be open source and royalty free.</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/12/15/some-predictions-for-2018/">by uraeus at December 15, 2017 08:53 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="tag:blogger.com,1999:blog-977684764667858073.post-2171196726578581902">
<h3><a href="http://hadess.net/" title="/b…ës Ààtj…õÃÉ no Ààse  Å…ë/  (hadess) | News">Bastien Nocera</a> ‚Äî <a href="http://www.hadess.net/2017/12/more-bluetooth-and-gaming-features.html">More Bluetooth (and gaming) features</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="hadess.png" alt="(Bastien Nocera)" width="63" height="80">
In the midst of post-release bug fixing, we've also added a fair number of new features to our stack. As usual, new features span a number of different components, so integrators will have to be careful picking up all the components when, well, integrating.<br><br><b>PS3 clones joypads support</b><br><br>Do you have a PlayStation 3 joypad that feels just a little bit "off"? You can't find the Sony logo anywhere on it? The figures on the face buttons look like barbed wire? And if it were a YouTube video, it would say "No copyright intended"?<br><br><div class="separator"><a href="https://3.bp.blogspot.com/-I-4jcauFVog/WjPwiaGhH-I/AAAAAAAAA4A/oL_SNbHFfWYsP6bt-BMmatKpzmqBZyUEQCLcBGAs/s1600/shanwan.jpg"><img src="shanwan.jpg" width="320" height="201" border="0"></a></div><br>Bingo. When plugged in via USB, those devices advertise themselves as SHANWAN or Gasia, and implement the bare minimum to work when plugged into a PlayStation 3 console. But as a Linux computer would behave slightly differently, we need to fix a couple of things.<br><br>The first fix was simple, but necessary to be able to do any work: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/drivers/hid/hid-sony.c?id=492ca83c3d19fba1622164f07cd7b775596a7db2">disable the rumble motor that starts as soon as you plug the pad through USB</a>.<br><br>Once that's done, we could work around the fact that the device isn't Bluetooth compliant, and <a href="https://git.kernel.org/pub/scm/bluetooth/bluez.git/commit/plugins/sixaxis.c?id=1629c39ededef07988a5403b27331e0e317f1e08">hard-code the HID service</a> it's supposed to offer.<br><br><b>Bluetooth LE Battery reporting</b><br><br><a href="https://en.wikipedia.org/wiki/Bluetooth_Low_Energy">Bluetooth Low Energy</a> is the new-fangled (7-year old) protocol for low throughput devices, from a single coin-cell powered sensor, to input devices. What's great is that there's finally a <a href="https://www.bluetooth.com/specifications/gatt/viewer?attributeXmlFile=org.bluetooth.characteristic.battery_level.xml">standardised way for devices to export their battery statuses</a>. I've added support for this in BlueZ, which <a href="https://cgit.freedesktop.org/upower/commit/?id=ccb1b0ed96baf5937d1bc36d5b4b0c65eb873964">UPower then picks up</a> for desktop integration goodness.<br><br>There are a number of Bluetooth LE joypads available for pickup, including a few that <a href="https://blogs.gnome.org/hughsie/2016/08/18/updating-firmware-on-8bitdo-game-controllers/">should be firmware upgradeable</a>. Look for "Bluetooth 4" as well as "Bluetooth LE" when doing your holiday shopping.<br><br><b>gnome-bluetooth work</b><br><br>Finally, this is the boring part. <a href="https://blogs.gnome.org/benzea/">Benjamin</a> and I reworked code that's internal to gnome-bluetooth, as used in the Settings panel as well as the Shell, to make it use modern facilities like GDBusObjectManager. The overall effect of this is, less code, less brittle and more reactive when Bluetooth adapters come and go, such as when using airplane mode.<br><br>Apart from the kernel patch mentioned above (you'll know if you need it :), those features have been integrated in UPower 0.99.7 and in the upcoming BlueZ 5.48. And they will of course be available in Fedora, both in rawhide and as updates to Fedora 27 as soon as the releases have been done and built.<br><br>GG!</div>

<p class="date">
<a href="http://www.hadess.net/2017/12/more-bluetooth-and-gaming-features.html">by Bastien Nocera (noreply@blogger.com) at December 15, 2017 03:57 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 14, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=505" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> ‚Äî <a href="https://coaxion.net/blog/2017/12/a-gstreamer-plugin-like-the-rec-button-on-your-tape-recorder-a-multi-threaded-plugin-written-in-rust/">A GStreamer Plugin like the Rec Button on your Tape Recorder ‚Äì A Multi-Threaded Plugin written in Rust</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dr√∂ge)" width="80" height="80">
<p>As <a href="https://www.rust-lang.org/" rel="noopener" target="_top">Rust</a> is known for <a href="https://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html" rel="noopener" target="_top">‚ÄúFearless Concurrency‚Äù</a>, that is being able to write concurrent, multi-threaded code without fear, it seemed like a good fit for a <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> element that we had to write at <a href="https://centricular.com/" rel="noopener" target="_top">Centricular</a>.</p>
<p>Previous experience with Rust for writing (mostly) single-threaded GStreamer elements and applications (also multi-threaded) were all quite successful and promising already. And in the end, this new element was also a pleasure to write and probably faster than doing the equivalent in C. For the impatient, the <a href="https://github.com/sdroege/gst-plugin-rs/blob/master/gst-plugin-togglerecord/src/togglerecord.rs" rel="noopener" target="_top">code</a>, <a href="https://github.com/sdroege/gst-plugin-rs/blob/master/gst-plugin-togglerecord/tests/tests.rs" rel="noopener" target="_top">tests</a> and a <a href="https://www.gtk.org/" rel="noopener" target="_top">GTK+</a> <a href="https://github.com/sdroege/gst-plugin-rs/blob/master/gst-plugin-togglerecord/examples/gtk_recording.rs" rel="noopener" target="_top">example application</a> (written with the great <a href="http://gtk-rs.org/" rel="noopener" target="_top">Rust GTK bindings</a>, but the GStreamer element is also usable from C or any other language) can be found <a href="https://github.com/sdroege/gst-plugin-rs/tree/master/gst-plugin-togglerecord" rel="noopener" target="_top">here</a>.</p>
<h4>What does it do?</h4>
<p>The main idea of the element is that it basically works like the rec button on your tape recorder. There is a single boolean property called ‚Äúrecord‚Äù, and whenever it is set to <i>true</i> it will pass-through data and whenever it is set to <i>false</i> it will drop all data. But different to the existing <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-plugins/html/gstreamer-plugins-valve.html" rel="noopener" target="_top">valve</a> element, it</p>
<ul>
<li>Outputs a contiguous timeline without gaps, i.e. there are no gaps in the output when not recording. Similar to the recording you get on a tape recorder, you don‚Äôt have 10s of silence if you didn‚Äôt record for 10s.</li>
<li>Handles and synchronizes multiple streams at once. When recording e.g. a video stream and an audio stream, every recorded segment starts and stops with both streams at the same time</li>
<li>Is key-frame aware. If you record a compressed video stream, each recorded segment starts at a keyframe and ends right before the next keyframe to make it most likely that all frames can be successfully decoded</li>
</ul>
<p>The multi-threading aspect here comes from the fact that in GStreamer each stream usually has its own thread, so in this case the video stream and the audio stream(s) would come from different threads but would have to be synchronized between each other.</p>
<p>The GTK+ example application for the plugin is playing a video with the current playback time and a <i>beep</i> every second, and allows to record this as an MP4 file in the current directory.</p>
<h4>How did it go?</h4>
<p>This new element was again based on the <a href="https://github.com/sdroege/gstreamer-rs" rel="noopener" target="_top">Rust GStreamer bindings</a> and the <a href="https://github.com/sdroege/gst-plugin-rs" rel="noopener" target="_top">infrastructure</a> that I was writing over the last year or two for writing GStreamer plugins in Rust.</p>
<p>As written above, it generally went all fine and was quite a pleasure but there were a few things that seem noteworthy. But first of all, writing this in Rust was much more convenient and fun than writing it in C would‚Äôve been, and I‚Äôve written enough similar code in C before. It would‚Äôve taken quite a bit longer, I would‚Äôve had to debug more problems in the new code during development (there were actually surprisingly few things going wrong during development, I expected more!), and probably would‚Äôve written less exhaustive tests because writing tests in C is just so inconvenient.</p>
<h5>Rust does not prevent deadlocks</h5>
<p>While this should be clear, and was also clear to myself before, this seems like it might need some reiteration. Safe Rust prevents data races, but not all possible bugs that multi-threaded programs can have. Rust is not magic, only a tool that helps you prevent some classes of potential bugs.</p>
<p>For example, you can‚Äôt just stop thinking about lock order if multiple mutexes are involved, or that you can carelessly use <a href="https://doc.rust-lang.org/std/sync/struct.Condvar.html" rel="noopener" target="_top">condition variables</a> without making sure that your conditions actually make sense and accessed atomically. As a wise man once said, ‚Äúthe safest program is the one that does not run at all‚Äù, and a deadlocking program is very close to that.</p>
<p>The part about condition variables might be something that can be improved in Rust. Without this, you can easily end up in situations where you wait forever or your conditions are actually inconsistent. Currently Rust‚Äôs condition variables only require a mutex to be passed to the functions for waiting for the condition to be notified, but it would probably also make sense to require passing the same mutex to the constructor and notify functions to make it absolutely clear that you need to ensure that your conditions are always accessed/modified while this specific mutex is locked. Otherwise you might end up in debugging hell.</p>
<p>Fortunately during development of the plugin I only ran into a simple deadlock, caused by accidentally keeping a mutex locked for too long and then running into conflict with another one. Which is probably an easy trap if the most common way of unlocking a mutex is to let the <a href="https://doc.rust-lang.org/std/sync/struct.MutexGuard.html" rel="noopener" target="_top">mutex lock guard fall out of scope</a>. This makes it impossible to forget to unlock the mutex, but also makes it less explicit when it is unlocked and sometimes explicit unlocking by manually dropping the mutex lock guard is still necessary.</p>
<p>So in summary, while a big group of potential problems with multi-threaded programs are prevented by Rust, you still have to be careful to not run into any of the many others. Especially if you use lower-level constructs like condition variables, and not just e.g. channels. Everything is however far more convenient than doing the same in C, and with more support by the compiler, so I definitely prefer writing such code in Rust over doing the same in C.</p>
<h5>Missing API</h5>
<p>As usual, for the first dozen projects using a new library or new bindings to an existing library, you‚Äôll notice some missing bits and pieces. That I missed relatively core part of GStreamer, the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstRegistry.html" rel="noopener" target="_top">GstRegistry API</a>, was surprising nonetheless. True, you usually don‚Äôt use it directly and I only need to use it here for loading the new plugin from a non-standard location, but it was still surprising. Let‚Äôs hope this was the biggest oversight. If you look at the <a href="https://github.com/sdroege/gstreamer-rs/issues" rel="noopener" target="_top">issues page</a> on GitHub, you‚Äôll find a few other things that are still missing though. But nobody needed them yet, so it‚Äôs probably fine for the time being.</p>
<p>Another part of missing APIs that I noticed during development was that many manual (i.e. not auto-generated) bindings didn‚Äôt have the <a href="https://doc.rust-lang.org/std/fmt/trait.Debug.html" rel="noopener" target="_top">Debug</a> trait implemented, or not in a too useful way. This is solved now, as otherwise I wouldn‚Äôt have been able to properly log what is happening inside the element to allow easier debugging later if something goes wrong.</p>
<p>Apart from that there were also various other smaller things that were missing, or bugs (see below) that I found in the bindings while going through all these. But those seem not very noteworthy ‚Äì check the commit logs if you‚Äôre interested.</p>
<h5>Bugs, bugs, bgsu</h5>
<p>I also found a couple of bugs in the bindings. They can be broadly categorized in two categories</p>
<ul>
<li>Annotation bugs in GStreamer. The auto-generated parts of the bindings are generated from an XML description of the API, that is generated from the C headers and code and annotations in there. There were a couple of annotations that were wrong (or missing) in GStreamer, which then caused memory leaks in my case. Such mistakes could also easily cause memory-safety issues though. The annotations are fixed now, which will also benefit all the other language bindings for GStreamer (and I‚Äôm not sure why nobody noticed the memory leaks there before me).</li>
<li>Bugs in the manually written parts of the bindings. Similarly to the above, there was one memory leak and another case where a function could‚Äôve returned <i>NULL</i> but did not have this case covered on the Rust-side by returning an <a href="https://doc.rust-lang.org/std/option/enum.Option.html" rel="noopener" target="_top">Option_&gt;</a>.</li>
</ul>
<p>Generally I was quite happy with the lack of bugs though, the bindings are really ready for production at this point. And especially, all the bugs that I found are things that are unfortunately ‚Äúnormal‚Äù and common when writing code in C, while Rust is preventing exactly these classes of bugs. As such, they have to be solved only once at the bindings layer and then you‚Äôre free of them and you don‚Äôt have to spent any brain capacity on their existence anymore and can use your brain to solve the actual task at hand.</p>
<h5>Inconvenient API</h5>
<p>Similar to the missing API, whenever using some rather new API you will find things that are inconvenient and could ideally be done better. The biggest case here was the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstSegment.html" rel="noopener" target="_top">GstSegment</a> API. A segment represents a (potentially open-ended) playback range and contains all the information to convert timestamps to the different time bases used in GStreamer. I‚Äôm not going to get into details here, best check the documentation for them.</p>
<p>A segment can be in different formats, e.g. in time or bytes. In the C API this is handled by storing the format inside the segment, and requiring you to pass the format together with the value to every function call, and internally there are some checks then that let the function fail if there is a format mismatch. In the previous version of the Rust segment API, this was done the same, and caused lots of <i>unwrap()</i> calls in this element.</p>
<p>But in Rust we can do better, and the new API for the segment now encodes the format in the type system (i.e. there is a <i>Segment&lt;Time&gt;</i>) and only values with the correct type (e.g. <i>ClockTime</i>) can be passed to the corresponding functions of the segment. In addition there is a type for a generic segment (which still has all the runtime checks) and functions to ‚Äúcast‚Äù between the two.</p>
<p>Overall this gives more type-safety (the compiler already checks that you don‚Äôt mix calculations between seconds and bytes) and makes the API usage more convenient as various error conditions just can‚Äôt happen and thus don‚Äôt have to be handled. Or like in C, are simply ignored and not handled, potentially leaving a trap that can cause hard to debug bugs at a later time.</p>
<p>That Rust requires all errors to be handled makes it very obvious how many potential error cases the average C code out there is not handling at all, and also shows that a more expressive language than C can easily prevent many of these error cases at compile-time already.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/12/a-gstreamer-plugin-like-the-rec-button-on-your-tape-recorder-a-multi-threaded-plugin-written-in-rust/">by slomo at December 14, 2017 10:41 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 11, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2017-12-11T00:00:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> ‚Äî <a href="https://gstreamer.freedesktop.org/news/#2017-12-11T00:00:00Z">GStreamer 1.12.4 stable release (binaries)</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
Pre-built binary images of the 1.12.4 stable release of GStreamer are now
available for Windows 32/64-bit, iOS and Mac OS X and Android.
</p><p>
        The builds are available for download from:
        <a href="https://gstreamer.freedesktop.org/data/pkg/android/1.12.4/">Android</a>,
	<a href="https://gstreamer.freedesktop.org/data/pkg/ios/1.12.4/">iOS</a>,
	<a href="https://gstreamer.freedesktop.org/data/pkg/osx/1.12.4/">Mac OS X</a> and
        <a href="https://gstreamer.freedesktop.org/data/pkg/windows/1.12.4/">Windows</a>.
        </p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2017-12-11T00:00:00Z">December 11, 2017 12:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 09, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://k-d-w.org/102 at https://k-d-w.org" lang="en">
<h3><a href="https://k-d-w.org/" title="Sebastian P√∂lsterl's blog">Sebastian P√∂lsterl</a> ‚Äî <a href="https://k-d-w.org/node/102">scikit-survival 0.5 released</a></h3>
<div class="entry">
<div class="content">
<div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even"><div class="tex2jax"><p>Today, I released a new version of <a href="https://github.com/sebp/scikit-survival">scikit-survival</a>. This release adds support for the latest version of scikit-learn (0.19) and pandas (0.21). In turn, support for Python 3.4, scikit-learn 0.18 and pandas 0.18 has been dropped.</p>
<p>Many people are confused about the meaning of predictions. Often, they assume that predictions of a survival model should always be non-negative since the input is the time to an event. However, this not always the case. In general, predictions are risk scores of arbitrary scale. In particular, survival models usually do not predict the exact time of an event, but the relative order of events. If samples are ordered according to their predicted risk score (in ascending order), one obtains the sequence of events, as predicted by the model. A more detailed explanation is available in the <a href="https://scikit-survival.readthedocs.io/en/latest/understanding_predictions.html">Understanding Predictions in Survival Analysis</a> section of the documentation.</p>
<h2>Download</h2>
<p>
You can install the latest version via Anaconda (Linux, OSX and Windows):</p>
<div class="geshifilter">
<div class="bash geshifilter-bash">
<pre class="de1">conda <span class="kw2">install</span> <span class="re5">-c</span> sebp scikit-survival</pre></div>
</div>
<p>or via pip:</p>
<div class="geshifilter">
<div class="bash geshifilter-bash">
<pre class="de1">pip <span class="kw2">install</span> <span class="re5">-U</span> scikit-survival</pre></div>
</div>
</div></div></div></div></div>

<p class="date">
<a href="https://k-d-w.org/node/102">by sebp at December 09, 2017 11:33 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 07, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2017-12-07T18:30:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> ‚Äî <a href="https://gstreamer.freedesktop.org/news/#2017-12-07T18:30:00Z">GStreamer 1.12.4 stable release</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is pleased to announce the fourth bugfix release in the
stable 1.12 release series of your favourite cross-platform multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from
1.12.x.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.12/#1.12.4">/releases/1.12/</a>
for the full release notes.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Check out the release notes for
<a href="https://gstreamer.freedesktop.org/releases/gstreamer/1.12.4.html">GStreamer core</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-base/1.12.4.html">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-good/1.12.4.html">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-ugly/1.12.4.html">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-bad/1.12.4.html">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-libav/1.12.4.html">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-rtsp-server/1.12.4.html">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-python/1.12.4.html">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-editing-services/1.12.4.html">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-validate/1.12.4.html">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/releases/gstreamer-vaapi/1.12.4.html">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/releases/gst-omx/1.12.4.html">gst-omx</a>,
or download tarballs for
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.12.4.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.12.4.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.12.4.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.12.4.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.12.4.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.12.4.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.12.4.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.12.4.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.12.4.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.12.4.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.12.4.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.12.4.tar.xz">gst-omx</a>.
        </p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2017-12-07T18:30:00Z">December 07, 2017 06:30 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="http://blogs.igalia.com/vjaquez/?p=625" lang="en-US">
<h3><a href="https://blogs.igalia.com/vjaquez" title="gstreamer ‚Äì Herostratus‚Äô legacy">V√≠ctor J√°quez</a> ‚Äî <a href="https://blogs.igalia.com/vjaquez/2017/12/07/enabling-huc-for-sklkbl-in-debiantesting/">Enabling HuC for SKL/KBL in Debian/testing</a></h3>
<div class="entry">
<div class="content">
<p>Recently, our friend Florent complained that it was <a href="https://bugzilla.gnome.org/show_bug.cgi?id=789472">impossible to set a constant bitrate when encoding H.264 using low-power profile</a> with <code>gstreamer-vaapi</code> .</p>
<p>Low-power (LP) profiles are VA-API entry points, <strong>available in Intel SkyLake-based procesor and succesors</strong>, which provide video encoding with low power consumption.</p>
<p>Later on, Ullysses and Sree, pointed out that CBR in LP <a href="https://github.com/01org/intel-vaapi-driver/issues/312#issuecomment-349726243">is ony possible if HuC is enabled</a> in the kernel.</p>
<p>HuC is a firmware, loaded by i915 kernel module, designed to offload some of the media functions from the CPU to GPU. One of these functions is bitrate control when encoding. HuC saves unnecessary CPU-GPU synchronization.</p>
<p>In order to load HuC, it is required first to load GuC, another Intel‚Äôs firmware designed to perform graphics workload scheduling on the various graphics parallel engines.</p>
<p>How we can install and configure these firmwares to enable CBR in low-power profile, among other things, in Debian/testing?</p>
<h2>Check i915 parameters</h2>
<p>First we shall confirm that our kernel and our i915 kernel module is capable to handle this functionality:</p>
<pre><code>$ sudo modinfo i915 | egrep -i "guc|huc|dmc"
firmware:       i915/bxt_dmc_ver1_07.bin
firmware:       i915/skl_dmc_ver1_26.bin
firmware:       i915/kbl_dmc_ver1_01.bin
firmware:       i915/kbl_guc_ver9_14.bin
firmware:       i915/bxt_guc_ver8_7.bin
firmware:       i915/skl_guc_ver6_1.bin
firmware:       i915/kbl_huc_ver02_00_1810.bin
firmware:       i915/bxt_huc_ver01_07_1398.bin
firmware:       i915/skl_huc_ver01_07_1398.bin
parm:           enable_guc_loading:Enable GuC firmware loading (-1=auto, 0=never [default], 1=if available, 2=required) (int)
parm:           enable_guc_submission:Enable GuC submission (-1=auto, 0=never [default], 1=if available, 2=required) (int)
parm:           guc_log_level:GuC firmware logging level (-1:disabled (default), 0-3:enabled) (int)
parm:           guc_firmware_path:GuC firmware path to use instead of the default one (charp)
parm:           huc_firmware_path:HuC firmware path to use instead of the default one (charp)
</code></pre>
<h2>Install firmware</h2>
<pre><code>$ sudo apt install firmware-misc-nonfree
</code></pre>
<p>UPDATE: In order to install this Debian package, you should have enabled the <code>non-free</code> apt repository in your sources list.</p>
<p>Verify the firmware are installed:</p>
<pre><code>$ ls -1 /lib/firmware/i915/
bxt_dmc_ver1_07.bin
bxt_dmc_ver1.bin
bxt_guc_ver8_7.bin
bxt_huc_ver01_07_1398.bin
kbl_dmc_ver1_01.bin
kbl_dmc_ver1.bin
kbl_guc_ver9_14.bin
kbl_huc_ver02_00_1810.bin
skl_dmc_ver1_23.bin
skl_dmc_ver1_26.bin
skl_dmc_ver1.bin
skl_guc_ver1.bin
skl_guc_ver4.bin
skl_guc_ver6_1.bin
skl_guc_ver6.bin
skl_huc_ver01_07_1398.bin
</code></pre>
<h2>Update modprobe configuration</h2>
<p>Edit or create the configuration file <code>/etc/modprobe.d/i915.con</code></p>
<pre><code>$ sudo vim /etc/modprobe.d/i915.conf
....
$ cat /etc/modprobe.d/i915.conf
options i915 enable_guc_loading=1 enable_guc_submission=1
</code></pre>
<h2>Reboot</h2>
<pre><code>$ sudo systemctl reboot 
</code></pre>
<h2>Verification</h2>
<p>Now it is possible to verify that the i915 module kernel loaded the firmware correctly by looking at the kenrel logs:</p>
<pre><code>$ journalctl -b -o short-monotonic -k | egrep -i "i915|dmr|dmc|guc|huc"
[   10.303849] miau kernel: Setting dangerous option enable_guc_loading - tainting kernel
[   10.303852] miau kernel: Setting dangerous option enable_guc_submission - tainting kernel
[   10.336318] miau kernel: i915 0000:00:02.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=io+mem:owns=io+mem
[   10.338664] miau kernel: i915 0000:00:02.0: firmware: direct-loading firmware i915/kbl_dmc_ver1_01.bin
[   10.339635] miau kernel: [drm] Finished loading DMC firmware i915/kbl_dmc_ver1_01.bin (v1.1)
[   10.361811] miau kernel: i915 0000:00:02.0: firmware: direct-loading firmware i915/kbl_huc_ver02_00_1810.bin
[   10.362422] miau kernel: i915 0000:00:02.0: firmware: direct-loading firmware i915/kbl_guc_ver9_14.bin
[   10.393117] miau kernel: [drm] GuC submission enabled (firmware i915/kbl_guc_ver9_14.bin [version 9.14])
[   10.410008] miau kernel: [drm] Initialized i915 1.6.0 20170619 for 0000:00:02.0 on minor 0
[   10.559614] miau kernel: snd_hda_intel 0000:00:1f.3: bound 0000:00:02.0 (ops i915_audio_component_bind_ops [i915])
[   11.937413] miau kernel: i915 0000:00:02.0: fb0: inteldrmfb frame buffer device
</code></pre>
<p>That means that HuC and GuC firmwares were loaded successfully.</p>
<p>Now we can check the status of the modules using <code>sysfs</code></p>
<pre><code>$ sudo cat /sys/kernel/debug/dri/0/i915_guc_load_status
GuC firmware status:
        path: i915/kbl_guc_ver9_14.bin
        fetch: SUCCESS
        load: SUCCESS
        version wanted: 9.14
        version found: 9.14
        header: offset is 0; size = 128
        uCode: offset is 128; size = 142272
        RSA: offset is 142400; size = 256

GuC status 0x800330ed:
        Bootrom status = 0x76
        uKernel status = 0x30
        MIA Core status = 0x3

Scratch registers:
         0:     0xf0000000
         1:     0x0
         2:     0x0
         3:     0x5f5e100
         4:     0x600
         5:     0xd5fd3
         6:     0x0
         7:     0x8
         8:     0x3
         9:     0x74240
        10:     0x0
        11:     0x0
        12:     0x0
        13:     0x0
        14:     0x0
        15:     0x0
$ sudo cat /sys/kernel/debug/dri/0/i915_huc_load_status
HuC firmware status:
        path: i915/kbl_huc_ver02_00_1810.bin
        fetch: SUCCESS
        load: SUCCESS
        version wanted: 2.0
        version found: 2.0
        header: offset is 0; size = 128
        uCode: offset is 128; size = 218304
        RSA: offset is 218432; size = 256

HuC status 0x00006080:
</code></pre>
<h2>Test GStremer</h2>
<pre><code>$ gst-launch-1.0 videotestsrc num-buffers=1000 ! video/x-raw, format=NV12, width=1920, height=1080, framerate=\(fraction\)30/1 ! vaapih264enc bitrate=8000 keyframe-period=30 tune=low-power rate-control=cbr ! mp4mux ! filesink location=test.mp4
Setting pipeline to PAUSED ...
Pipeline is PREROLLING ...
Got context from element 'vaapiencodeh264-0': gst.vaapi.Display=context, gst.vaapi.Display=(GstVaapiDisplay)"\(GstVaapiDisplayGLX\)\ vaapidisplayglx0";
Pipeline is PREROLLED ...
Setting pipeline to PLAYING ...
New clock: GstSystemClock
Got EOS from element "pipeline0".
Execution ended after 0:00:11.620036001
Setting pipeline to PAUSED ...
Setting pipeline to READY ...
Setting pipeline to NULL ...
Freeing pipeline ...
$ gst-discoverer-1.0 test.mp4 
Analyzing file:///home/vjaquez/gst/master/intel-vaapi-driver/test.mp4
Done discovering file:///home/vjaquez/test.mp4

Topology:
  container: Quicktime
    video: H.264 (High Profile)

Properties:
  Duration: 0:00:33.333333333
  Seekable: yes
  Live: no
  Tags: 
      video codec: H.264 / AVC
      bitrate: 8084005
      encoder: VA-API H264 encoder
      datetime: 2017-12-07T14:29:23Z
      container format: ISO MP4/M4A
</code></pre>
<p>Misison accomplished!</p>
<h2>References</h2>
<ul>
<li><a href="https://gist.github.com/Brainiarc7/aa43570f512906e882ad6cdd835efe57">Tuning Intel Skylake and beyond for optimal performance and feature level support on Linux</a></li>
<li><a href="https://01.org/linuxgraphics/downloads/firmware">Intel Linux Graphifs Firmware</a></li>
</ul></div>

<p class="date">
<a href="https://blogs.igalia.com/vjaquez/2017/12/07/enabling-huc-for-sklkbl-in-debiantesting/">by vjaquez at December 07, 2017 02:35 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 06, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="tag:blogger.com,1999:blog-977684764667858073.post-1089943682647987815">
<h3><a href="http://hadess.net/" title="/b…ës Ààtj…õÃÉ no Ààse  Å…ë/  (hadess) | News">Bastien Nocera</a> ‚Äî <a href="http://www.hadess.net/2017/12/utc-and-anywhere-on-earth-support.html">UTC and Anywhere on Earth support</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="hadess.png" alt="(Bastien Nocera)" width="63" height="80">
A quick post to tell you that we finally added UTC support to Clocks' and the Shell's World Clocks section. And if you're into it, there's also <a href="https://en.wikipedia.org/wiki/Anywhere_on_Earth">Anywhere on Earth</a> support.<br><br>You will need to have git master versions of libgweather (our cities and timezones database), and gnome-clocks. This feature will land in GNOME 3.28.<br><br><div class="separator"><a href="https://4.bp.blogspot.com/-78CXcIC0Wyo/Wif_ASERmDI/AAAAAAAAA3w/3ohoH3LAlT4mUmuuFE8OAUKdQzjt5PPrACLcBGAs/s1600/gnome-clocks-UTC.png"><img src="gnome-clocks-utc.png" width="400" height="351" border="0"></a></div><br><br>Many thanks to Giovanni for coming up with an API he was happy with after I attempted a couple of iterations on one. Enjoy!<br><br><i>Update</i>: As expected, a bug crept in. Thanks to Colin Guthrie for <a href="https://git.gnome.org/browse/libgweather/commit/?id=8556440ac4cf515e70027cf24fb3d19e15ea43a3">spotting the error in the "Anywhere on Earth" timezone</a>. See <a href="https://en.wikipedia.org/wiki/Tz_database#Area">this section for the fun we have to deal with</a>.</div>

<p class="date">
<a href="http://www.hadess.net/2017/12/utc-and-anywhere-on-earth-support.html">by Bastien Nocera (noreply@blogger.com) at December 06, 2017 04:06 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">November 26, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=499" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> ‚Äî <a href="https://coaxion.net/blog/2017/11/gstreamer-rust-bindings-release-0-9/">GStreamer Rust bindings release 0.9</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dr√∂ge)" width="80" height="80">
<p>About 3 months, a <a href="https://gstreamer.freedesktop.org/conference/2017/" rel="noopener" target="_top">GStreamer Conference</a> and two bug-fix releases have passed now since the <a href="https://coaxion.net/blog/2017/08/gstreamer-rust-bindings-release-0-8-0/">GStreamer Rust bindings release 0.8.0</a>. Today <a href="https://crates.io/crates/gstreamer" rel="noopener" target="_top">version 0.9.0</a> (and 0.9.1 with a small bugfix to export some forgotten types) with a couple of API improvements and lots of additions and cleanups was released. This new version depends <a href="http://gtk-rs.org/blog/2017/11/26/new-release.html" rel="noopener" target="_top">on the new set of releases of the gtk-rs crates</a> (glib/etc).</p>
<p>The full changelog can be found <a href="https://github.com/sdroege/gstreamer-rs/blob/80ebc86e94468e292ae39512b13e059e69794530/gstreamer/CHANGELOG.md#091---2017-11-26" rel="noopener" target="_top">here</a>, but below is a short overview of the (in my opinion) most interesting changes.</p>
<h4>Tutorials</h4>
<p>The <a href="https://github.com/sdroege/gstreamer-rs/tree/master/tutorials" rel="noopener" target="_top">basic tutorials 1 to 8</a> were ported from C to Rust by various contributors. The C versions and the corresponding explanatory text can be found <a href="https://gstreamer.freedesktop.org/documentation/tutorials/" rel="noopener" target="_top">here</a>, and it should be relatively easy to follow the text together with the Rust code.</p>
<p>This should make learning to use <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> from Rust much easier, in combination with the <a href="https://github.com/sdroege/gstreamer-rs/tree/master/examples" rel="noopener" target="_top">few example applications</a> that exist in the repository.</p>
<h4>Type-safety Improvements</h4>
<p>Previously querying the current playback position from a pipeline (and various other things analogous) was giving you a plain 64-bit integer, just like in C. However in Rust we can easily do better.</p>
<p>The main problem with just getting an integer was that there are ‚Äúspecial‚Äù values that have the meaning of ‚Äúno value known‚Äù, specifically <em>GST_CLOCK_TIME_NONE</em> for values in time. In C this often causes bugs by code ignoring this special case and then doing calculations with such a value, resulting in completely wrong numbers. In the Rust bindings these are now expressed as an <em>Option_&gt;</em> so that the special case has to be handled separately, and in combination with that for timed values there is a new type called <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.ClockTime.html" rel="noopener" target="_top"><em>ClockTime</em></a> that is implementing all the arithmetic traits and others so you can still do normal arithmetic operations on the values, while the implementation of those operations takes care of <em>GST_CLOCK_TIME_NONE</em>. Also it was previously easy to get a value in bytes and add it to a value in time. Whenever multiple formats are possible, a new type called <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/enum.FormatValue.html" rel="noopener" target="_top"><em>FormatValue</em></a> is now used that combines the value itself with its format to prevent such mistakes.</p>
<h4>Error Handling</h4>
<p>Various operations in GStreamer can fail with a custom enum type: link pads (<a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/enum.PadLinkReturn.html" rel="noopener" target="_top"><em>PadLinkReturn</em></a>), pushing a buffer (<a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/enum.FlowReturn.html" rel="noopener" target="_top"><em>FlowReturn</em></a>), changing an element‚Äôs state (<a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/enum.StateChangeReturn.html" rel="noopener" target="_top"><em>StateChangeReturn</em></a>). Previously handling this was not as convenient as the usual <em>Result</em>-based error handling in Rust. With this release, all these types provide a function <em>into_result()</em> that allows to convert into a <em>Result</em> that splits the enum into its good and bad cases, e.g. <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/enum.FlowSuccess.html" rel="noopener" target="_top"><em>FlowSuccess</em></a> and <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/enum.FlowError.html" rel="noopener" target="_top"><em>FlowError</em></a>. Based on this, the usual Rust error handling is possible, including usage of the ?-operator. Once the <a href="https://doc.rust-lang.org/nightly/std/ops/trait.Try.html" rel="noopener" target="_top">Try trait</a> is stable, it will also be possible to directly use the ?-operator on <em>FlowReturn</em> and the others before conversion into a <em>Result</em>.</p>
<p>All these enums are also marked as <em>#[must_use]</em> now, which causes a compiler warning if code is not specifically handling them (which could mean to explicitly ignore them), making it even harder to ignore errors caused by any failures of such operations.</p>
<p>In addition, all the examples and tutorials make use of the above now and many examples were ported to the <a href="https://crates.io/crates/failure" rel="noopener" target="_top">failure</a> crate and implement proper error handling in all situations now, for example the <a href="https://github.com/sdroege/gstreamer-rs/blob/ea3d08d65a9210b7f422b8719ff4189c4bb0e37a/examples/src/bin/decodebin.rs" rel="noopener" target="_top">decodebin example</a>.</p>
<h4>Various New API</h4>
<p>Apart from all of the above, a lot of new API was added. Both for writing GStreamer-based applications, and making that easier, as well as for writing GStreamer plugins in Rust. For the latter, the <a href="https://github.com/sdroege/gst-plugin-rs/" rel="noopener" target="_top">gst-plugin-rs</a> repository with various crates (and plugins) was ported to the GStreamer bindings and completely rewritten, but more on that in another blog post in the next couple of days once the <a href="https://github.com/sdroege/gst-plugin-rs/tree/master/gst-plugin" rel="noopener" target="_top">gst-plugin</a> crate is released and published on <a href="https://crates.io/" rel="noopener" target="_top">crates.io</a>.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/11/gstreamer-rust-bindings-release-0-9/">by slomo at November 26, 2017 06:59 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">November 24, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://blogs.igalia.com/vjaquez/?p=611" lang="en-US">
<h3><a href="https://blogs.igalia.com/vjaquez" title="gstreamer ‚Äì Herostratus‚Äô legacy">V√≠ctor J√°quez</a> ‚Äî <a href="https://blogs.igalia.com/vjaquez/2017/11/24/intel-mediasdk-on-debian-testing/">Intel MediaSDK on Debian (testing)</a></h3>
<div class="entry">
<div class="content">
<p>Everybody knows it: install Intel MediaSDK in GNU/Linux is a <em>PITA</em>. With CentOS or Yocto is less cumbersome, if you trust blindly on scripts ran as root.</p>
<p>I don‚Äôt like CentOS, I feel it like if I were living in the past. I like Debian (testing, of course) and I also wanted to understand a little more about MediaSDK.  And this is what I did to have Intel MediaSDK working in Debian/testing.</p>
<p>First, I did a pristine installation of Debian testing with a <a href="https://www.debian.org/devel/debian-installer/">netinst image</a> in my <a href="https://www.intel.com/content/www/us/en/products/boards-kits/nuc/kits/nuc6i5syk.html">NUC 6i5SYK</a>, with a normal desktop user setup (Gnome3).</p>
<p>The madness comes later.</p>
<p>Intel‚Äôs identifies two types of MediaSDK installation: <em>Gold</em> and <em>Generic</em>. Gold is for CentOS, and Generic for the rest of distributions. Obviously, Generic means <strong>you‚Äôre on your own</strong>. For the purpose of this exercise I used as reference <a href="https://software.intel.com/en-us/articles/how-to-setup-media-server-studio-on-secondary-os-of-linux">Generic Linux* Intel¬Æ Media Server Studio Installation</a>.</p>
<p>Let‚Äôs begin by grabbing the <a href="https://software.intel.com/en-us/media-sdk">Intel¬Æ Media Server Studio ‚Äì Community Edition</a>. You will need to register yourself and accept the user agreement, because this is proprietary software.</p>
<p>At the end, you should have a tarball named <code>MediaServerStudioEssentials2017R3.tar.gz</code></p>
<h4>Extract the files for Generic instalation</h4>
<pre><code>$ cd ~
$ tar xvf MediaServerStudioEssentials2017R3.tar.gz
$ cd MediaServerStudioEssentials2017R3
$ tar xvf SDK2017Production16.5.2.tar.gz
$ cd SDK2017Production16.5.2/Generic
$ mkdir tmp
$ tar -xvC tmp -f intel-linux-media_generic_16.5.2-64009_64bit.tar.gz
</code></pre>
<h3>Kernel</h3>
<p>Bad news: in order to get MediaSDK working you need to patch the mainlined kernel.</p>
<p>Worse news: the available patches are only for the version 4.4 the kernel.</p>
<p>Still, <code>systemd</code> works on 4.4, as far as I know, so it would not be a big problem.</p>
<h5>Grab building dependencies</h5>
<pre><code>$ sudo apt install build-essential devscripts libncurses5-dev
$ sudo apt build-dep linux
</code></pre>
<h4>Grab kernel source</h4>
<p>I like to use the sources from the git repository, since it would be possible to do some rebasing and blaming in the future.</p>
<pre><code>$ cd ~
$ git clone https://github.com/torvalds/linux.git
...
$ git pull -v --tags
$ git checkout -b 4.4 v4.4
</code></pre>
<h4>Extract MediaSDK patches</h4>
<pre><code>$ cd ~/MediaServerStudioEssentials2017R3/SDK2017Production16.5.2/Generic/tmp/opt/intel/mediasdk/opensource/patches/kmd/4.4
$ tar xvf intel-kernel-patches.tar.bz2
$ cd intel-kernel-patches
$ PATCHDIR=$(pwd)
</code></pre>
<h4>Patch the kernel</h4>
<pre><code>cd ~/linux
$ git am $PATCHDIR/*.patch
</code></pre>
<p>The patches should apply with some warnings but no fatal errors (don‚Äôt worry, be happy).</p>
<p>Still, there‚Äôs a problem with this old kernel: <a href="https://askubuntu.com/questions/851433/kernel-doesnt-support-pic-mode-for-compiling">our recent compiler doesn‚Äôt build it as it is</a>. Another patch is required:</p>
<pre><code>$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.8-rc2/0002-UBUNTU-SAUCE-no-up-disable-pie-when-gcc-has-it-enabl.patch
$ git am 0002-UBUNTU-SAUCE-no-up-disable-pie-when-gcc-has-it-enabl.patch
</code></pre>
<p>TODO: Shall I need to modify the EXTRAVERSION string in kernel‚Äôs Makefile?</p>
<h4>Build and install the kernel</h4>
<p>Notice that we are using our current kernel configuration. That is error prone. I guess that is why I had to select NVM manually.</p>
<pre><code>$ cp /boot/config-4.12.0-1-amd64 ./.config
$ make olddefconfig
$ make nconfig # -- select NVM
$ scripts/config --disable DEBUG_INFO
$ make deb-pkg
...
$ sudo dpkg -i linux-image-4.4.0+_4.4.0+-2_amd64.deb linux-headers-4.4.0+_4.4.0+-2_amd64.deb linux-firmware-image-4.4.0+_4.4.0+-2_amd64.deb
</code></pre>
<h4>Configure GRUB2 to boot Linux 4.4. by default</h4>
<p>This part was absolutely tricky for me. It took me a long time to figure out how to specify the kernel ID in the <code>grubenv</code>.</p>
<pre><code>$ sudo vi /etc/default/grub
</code></pre>
<p>And change the line <code>GRUB_DEFAULT=saved</code>. By default it is set to <code>0</code>. And update GRUB.</p>
<pre><code>$ sudo update-grub
</code></pre>
<p>Now look for the ID of the installed kernel image in <code>/etc/grub/grub.cfg</code> and use it:</p>
<pre><code>$ sudo grub-set-default "gnulinux-4.4.0+-advanced-2c246bc6-65bb-48ea-9517-4081b016facc&gt;gnulinux-4.4.0+-advanced-2c246bc6-65bb-48ea-9517-4081b016facc"
</code></pre>
<p>Please note it is twice and separated by a <em>&gt;</em>. Don‚Äôt ask me why.</p>
<h4>Copy MediaSDK firmware (and libraries too)</h4>
<p>I like to use <code>rsync</code> rather normal <code>cp</code> because there are the options like <code>--dry-run</code> and  <code>--itemize-changes</code> to verify  what I am doing.</p>
<pre><code>$ cd ~/MediaServerStudioEssentials2017R3/SDK2017Production16.5.2/Generic/tmp
$ sudo rsync -av --itemize-changes ./lib /
$ sudo rsync -av --itemize-changes ./opt/intel/common /opt/intel
$ sudo rsync -av --itemize-changes ./opt/intel/mediasdk/{include,lib64,plugins} /opt/intel/mediasdk
</code></pre>
<p>All these directories contain blobs that do the MediaSDK magic. They are <em>dlopened</em> by hard coded paths by <code>mfx_dispatch</code>, which will be explain later.</p>
<p>In <code>/lib</code> lives the firmware (kernel blob).</p>
<p>In <code>/opt/intel/common</code>‚Ä¶ I have no idea what are those shared objects.</p>
<p>In <code>/opt/intel/mediasdk/include</code> live header files for programming an compilation.</p>
<p>In <code>/opt/intel/mediasdk/lib64</code> live the driver for the modified <code>libva</code> (iHD) and other libraries.</p>
<p>In <code>/opt/intel/mediasdk/plugins</code> live, well, plugins‚Ä¶</p>
<p>In conclusion, all these bytes are darkness and mystery.</p>
<h4>Reboot</h4>
<pre><code>$ sudo systemctl reboot
</code></pre>
<p>The system should boot, automatically, in GNU/Linux 4.4.</p>
<p>Please, log with Xorg, not in Wayland, since it is not supported, as far as I know.</p>
<h3>GStreamer</h3>
<p>For compiling GStreamer I will use <code>gst-uninstalled</code>. Someone may say that I should use <code>gst-build</code> because is newer and faster, but I feel more comfortable doing the following kind of hacks with the old&amp;good autotools.</p>
<p>Basically this is a reproduction of <a href="https://arunraghavan.net/2014/07/quick-start-guide-to-gst-uninstalled-1-x/">Quick-start guide to gst-uninstalled for GStreamer 1.x</a>.</p>
<pre><code>$ sudo apt build-dep gst-plugins-{base,good,bad}1.0
$ wget https://cgit.freedesktop.org/gstreamer/gstreamer/plain/scripts/create-uninstalled-setup.sh -q -O - | sh
</code></pre>
<p>I will modify the <code>gst-uninstalled</code> script, and keep it outside of the repository. For that I will use the <a href="https://www.freedesktop.org/software/systemd/man/file-hierarchy.html">systemd file-hierarchy spec</a> for user‚Äôs executables.</p>
<pre><code>$ cd ~/gst
$ mkdir -p ~/.local/bin
$ mv master/gstreamer/scripts/gst-uninstalled ~/.local/bin
$ ln -sf ~/.local/bin/gst-uninstalled ./gst-master
</code></pre>
<p>Do not forget to edit your <code>~/.profile</code> to add <code>~/.local/bin</code> in the environment variable <code>PATH</code>.</p>
<h4>Patch ~/.local/bin/gst-uninstalled</h4>
<p>The modifications are to handle the three dependencies libraries that are required by MediaSDK: <code>libdrm</code>, <code>libva</code> and <code>mfx_dispatch</code>.</p>
<pre><code>diff --git a/scripts/gst-uninstalled b/scripts/gst-uninstalled
index 81f83b6c4..d79f19abd 100755
--- a/scripts/gst-uninstalled
+++ b/scripts/gst-uninstalled
@@ -122,7 +122,7 @@ GI_TYPELIB_PATH=$GST/gstreamer/gst:$GI_TYPELIB_PATH
 export LD_LIBRARY_PATH
 export DYLD_LIBRARY_PATH
 export GI_TYPELIB_PATH
-  
+
 export PKG_CONFIG_PATH="\
 $GST_PREFIX/lib/pkgconfig\
 :$GST/gstreamer/pkgconfig\
@@ -140,6 +140,9 @@ $GST_PREFIX/lib/pkgconfig\
 :$GST/orc\
 :$GST/farsight2\
 :$GST/libnice/nice\
+:$GST/drm\
+:$GST/libva/pkgconfig\
+:$GST/mfx_dispatch\
 ${PKG_CONFIG_PATH:+:$PKG_CONFIG_PATH}"

 export GST_PLUGIN_PATH="\
@@ -227,6 +230,16 @@ export GST_VALIDATE_APPS_DIR=$GST_VALIDATE_APPS_DIR:$GST/gst-editing-services/te
 export GST_VALIDATE_PLUGIN_PATH=$GST_VALIDATE_PLUGIN_PATH:$GST/gst-devtools/validate/plugins/
 export GIO_EXTRA_MODULES=$GST/prefix/lib/gio/modules:$GIO_EXTRA_MODULES

+# MediaSDK
+export LIBVA_DRIVERS_PATH=/opt/intel/mediasdk/lib64
+export LIBVA_DRIVER_NAME=iHD
+export LD_LIBRARY_PATH="\
+/opt/intel/common/mdf/lib64\
+:$GST/drm/.libs\
+:$GST/drm/intel/.libs\
+:$GST/libva/va/.libs\
+:$LD_LIBRARY_PATH"
+
</code></pre>
<p>Now, initialize the gst-uninstalled environment:</p>
<pre><code>$ cd ~/gst
$ ./gst-master
</code></pre>
<h5>libdrm</h5>
<p>Grab libdrm from its repository and switch to the branch with the supported version by MediaSDK.</p>
<pre><code>$ cd ~/gst/master
$ git clone git://anongit.freedesktop.org/mesa/drm
$ cd drm
$ git checkout -b intel libdrm-2.4.67
</code></pre>
<p>Extract the distributed tarball in the cloned repository.</p>
<pre><code>$ tar -xv --strip-components=1 -C . -f ~/MediaServerStudioEssentials2017R3/SDK2017Production16.5.2/Generic/tmp/opt/intel/mediasdk/opensource/libdrm/2.4.67-64009/libdrm-2.4.67.tar.bz2
</code></pre>
<p>Then we could check the <em>big</em> delta between upstream and the changes done by Intel for MediaSDK.</p>
<p>Let‚Äôs put it in a commit for later rebases.</p>
<pre><code>$ git add -u
$ git add .
$ git commit -m "mediasdk changes"
</code></pre>
<p>Get build dependencies and compile.</p>
<pre><code>$ sudo apt build-dep libdrm
$ ./configure
$ make -j8
</code></pre>
<p>Since the pkgconfig files (*.pc) of libdrm are generated to work installed, it is needed to modify them in order to work uninstalled.</p>
<pre><code>$ prefix=${HOME}/gst/master/drm
$ sed -i -e "s#^libdir=.*#libdir=${prefix}/.libs#" ${prefix}/*.pc
$ sed -i -e "s#^includedir=.*#includedir=${prefix}#" ${prefix}/*.pc
</code></pre>
<p>In order to C preprocessor could find the uninstalled libdrm header files we need to make them available in the expected path according to the pkgconfig file and right now they are not there. To fix that it is possible to create proper symbolic links.</p>
<pre><code>$ cd ~/gst/master/drm
$ ln -s include/drm/ libdrm
</code></pre>
<h4>libva</h4>
<p>This modified a version of libva. These modifications messed a bit with the opensource version of libva, because Intel decided not to prefix the library, or some other strategy. In <code>gstreamer-vaapi</code> we had to blacklist VA-API version 0.99, because it is the version number, arbitrary set, of this modified version of libva for MediaSDK.</p>
<p>Again, grab the original libva from repo and change the branch aiming to the divert point. It was difficult to find the divert commit id since even the libva version number was changed. Doing some archeology I guessed the branch point was in version 1.0.15, but I‚Äôm not sure.</p>
<pre><code>$ cd ~/gst/master
$ git clone https://github.com/01org/libva.git
$ cd libva
$ git checkout -b intel libva-1.0.15
$ tar -xv --strip-components=1 -C . -f ~/MediaServerStudioEssentials2017R3/SDK2017Production16.5.2/Generic/tmp/opt/intel/mediasdk/opensource/libva/1.67.0.pre1-64009/libva-1.67.0.pre1.tar.bz2
$ git add -u
$ git add .
$ git commit -m "mediasdk"
</code></pre>
<p>Before compile, verify that Makefile is going to link against the uninstalled libdrm. You can do that  by grepping for LIBDRM  in Makefile.</p>
<p>Get compilation dependencies and build.</p>
<pre><code>$ sudo apt build-dep libva
$ ./configure
$ make -j8
</code></pre>
<p>Moidify the pkgconfig files for uninstalled</p>
<pre><code>$ prefix=${HOME}/gst/master/libva
$ sed -i -e "s#^libdir=.*#libdir=${prefix}/va/.libs#" ${prefix}/pkgconfig/*.pc
$ sed -i -e "s#^includedir=.*#includedir=${prefix}#" ${prefix}/pkgconfig/*.pc
</code></pre>
<p>Fix header path with symbolic links</p>
<pre><code>$ cd ~/gst/master/libva/va
$ ln -sf drm/va_drm.h
</code></pre>
<h4>mfx_dispatch</h4>
<p>This static library which must be linked with MediaSDK applications. In our case, to the GStreamer plugin.</p>
<p>According to its documentation (included in the tarball):</p>
<blockquote><p>
  the dispatcher is a layer that lies between application and the SDK implementations. Upon initialization, the dispatcher locates the appropiate platform-specific SDK implementation. If there is none, it will select the software SDK implementation. The dispatcher will redirect subsequent function calls to the same functions in the selected SDK implementation.
</p></blockquote>
<p>In the tarball there is the source of the mfx_dispatcher, but it only compiles with <code>cmake</code>. I have not worked with <code>cmake</code> on uninstalled setups, but we are lucky, there is a repository with autotools support:</p>
<pre><code>$ cd ~/gst/master
$ git clone https://github.com/lu-zero/mfx_dispatch.git
</code></pre>
<p>And compile. After running <code>./configure</code> it is better to confirm, grepping the generated Makefie, that the uninstalled versions of libdrm and libva are going to be used.</p>
<pre><code>$ autoreconf  --install
$ ./configure
$ make -j8
</code></pre>
<p>Finally, just as the other libraries, it is required to fix the pkgconfig files:d</p>
<pre><code>$ prefix=${HOME}/gst/master/mfx_dispatch
$ sed -i -e "s#^libdir=.*#libdir=${prefix}/.libs#" ${prefix}/*.pc
$ sed -i -e "s#^includedir=.*#includedir=${prefix}#" ${prefix}/*.pc
</code></pre>
<h4>Test it!</h4>
<p>At last we are in a position where it is possible to test if everything works as expected. For it we are going to run the pre-compiled version of <code>vainfo</code> bundled in the tarball.</p>
<p>We will copy it to our uninstalled setup, thus we would running without specifing the path.</p>
<pre><code>$ sync -av /home/vjaquez/MediaServerStudioEssentials2017R3/SDK2017Production16.5.2/Generic/tmp/usr/bin/vainfo ./prefix/bin/
$ vainfo
libva info: VA-API version 0.99.0
libva info: va_getDriverName() returns 0
libva info: User requested driver 'iHD'
libva info: Trying to open /opt/intel/mediasdk/lib64/iHD_drv_video.so
libva info: Found init function __vaDriverInit_0_32
libva info: va_openDriver() returns 0
vainfo: VA-API version: 0.99 (libva 1.67.0.pre1)
vainfo: Driver version: 16.5.2.64009-ubit
vainfo: Supported profile and entrypoints
      VAProfileH264ConstrainedBaseline: VAEntrypointVLD
      VAProfileH264ConstrainedBaseline: VAEntrypointEncSlice
      VAProfileH264ConstrainedBaseline: &lt;unknown entrypoint&gt;
      VAProfileH264ConstrainedBaseline: &lt;unknown entrypoint&gt;
      VAProfileH264Main               : VAEntrypointVLD
      VAProfileH264Main               : VAEntrypointEncSlice
      VAProfileH264Main               : &lt;unknown entrypoint&gt;
      VAProfileH264Main               : &lt;unknown entrypoint&gt;
      VAProfileH264High               : VAEntrypointVLD
      VAProfileH264High               : VAEntrypointEncSlice
      VAProfileH264High               : &lt;unknown entrypoint&gt;
      VAProfileH264High               : &lt;unknown entrypoint&gt;
      VAProfileMPEG2Simple            : VAEntrypointEncSlice
      VAProfileMPEG2Simple            : VAEntrypointVLD
      VAProfileMPEG2Main              : VAEntrypointEncSlice
      VAProfileMPEG2Main              : VAEntrypointVLD
      VAProfileVC1Advanced            : VAEntrypointVLD
      VAProfileVC1Main                : VAEntrypointVLD
      VAProfileVC1Simple              : VAEntrypointVLD
      VAProfileJPEGBaseline           : VAEntrypointVLD
      VAProfileJPEGBaseline           : VAEntrypointEncPicture
      VAProfileVP8Version0_3          : VAEntrypointEncSlice
      VAProfileVP8Version0_3          : VAEntrypointVLD
      VAProfileVP8Version0_3          : &lt;unknown entrypoint&gt;
      VAProfileHEVCMain               : VAEntrypointVLD
      VAProfileHEVCMain               : VAEntrypointEncSlice
      VAProfileVP9Profile0            : &lt;unknown entrypoint&gt;
      &lt;unknown profile&gt;               : VAEntrypointVideoProc
      VAProfileNone                   : VAEntrypointVideoProc
      VAProfileNone                   : &lt;unknown entrypoint&gt;
</code></pre>
<p>It works!</p>
<h4>Compile GStreamer</h4>
<p>I normally make a copy of <code>~/gst/master/gstreamer/script/git-update.sh</code> in <code>~/.local/bin</code> in order to modify it, like adding support for <code>ccache</code>, disabling gtkdoc and gobject-instrospections, increase the parallel tasks, etc. But that is out of the scope of this document.</p>
<pre><code>$ cd ~/gst/master/
$ ./gstreamer/scripts/git-update.sh
</code></pre>
<p>Everything should be built without issues and, at the end, we could test if the gst-msdk elements are available:</p>
<pre><code>$ gst-inspect-1.0 msdk
Plugin Details:
  Name                     msdk
  Description              Intel Media SDK encoders
  Filename                 /home/vjaquez/gst/master/gst-plugins-bad/sys/msdk/.libs/libgstmsdk.so
  Version                  1.13.0.1
  License                  BSD
  Source module            gst-plugins-bad
  Source release date      2017-11-23 16:39 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

  msdkh264dec: Intel MSDK H264 decoder
  msdkh264enc: Intel MSDK H264 encoder
  msdkh265dec: Intel MSDK H265 decoder
  msdkh265enc: Intel MSDK H265 encoder
  msdkmjpegdec: Intel MSDK MJPEG decoder
  msdkmjpegenc: Intel MSDK MJPEG encoder
  msdkmpeg2enc: Intel MSDK MPEG2 encoder
  msdkvp8dec: Intel MSDK VP8 decoder
  msdkvp8enc: Intel MSDK VP8 encoder

  9 features:
  +-- 9 elements
</code></pre>
<p>Great!</p>
<p>Now, let‚Äôs run a simple pipeline. Please note that gst-msdk elements have rank zero, then they will not be autoplugged, it is necessary to craft the pipeline manually:</p>
<pre><code>$ gst-launch-1.0 filesrc location= ~/test.264 ! h264parse ! msdkh264dec ! videoconvert ! xvimagesink
Setting pipeline to PAUSED ...
Pipeline is PREROLLING ...
libva info: VA-API version 0.99.0
libva info: va_getDriverName() returns 0
libva info: User requested driver 'iHD'
libva info: Trying to open /opt/intel/mediasdk/lib64/iHD_drv_video.so
libva info: Found init function __vaDriverInit_0_32
libva info: va_openDriver() returns 0
Redistribute latency...
Pipeline is PREROLLED ...
Setting pipeline to PLAYING ...
New clock: GstSystemClock
Got EOS from element "pipeline0".
Execution ended after 0:00:02.502411331
Setting pipeline to PAUSED ...
Setting pipeline to READY ...
Setting pipeline to NULL ...
Freeing pipeline ...
</code></pre>
<p>\o/</p></div>

<p class="date">
<a href="https://blogs.igalia.com/vjaquez/2017/11/24/intel-mediasdk-on-debian-testing/">by vjaquez at November 24, 2017 09:57 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">November 20, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2017-11-20T17:00:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> ‚Äî <a href="https://gstreamer.freedesktop.org/news/#2017-11-20T17:00:00Z">Orc 0.4.28 bug-fix release</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is pleased to announce another maintenance bug-fix release
of liborc, the Optimized Inner Loop Runtime Compiler. Main changes since the
previous release:
        </p><p>
          </p><ul>
            <li>Numerous undefined behaviour fixes</li>
            <li>Ability to disable tests</li>
            <li>Fix meson dist behaviour</li>
          </ul>
        <p></p><p>
Direct tarball download: <a href="https://gstreamer.freedesktop.org/src/orc/orc-0.4.28.tar.xz">orc-0.4.28</a>.
        </p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2017-11-20T17:00:00Z">November 20, 2017 05:00 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">November 17, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://gkiagia.wordpress.com/?p=200" lang="en">
<h3><a href="https://gkiagia.wordpress.com/" title="GStreamer ‚Äì Gkiagia‚Äôs Blog">George Kiagiadakis</a> ‚Äî <a href="https://gkiagia.wordpress.com/2017/11/17/ipcpipeline-splitting-a-gstreamer-pipeline-into-multiple-processes/">ipcpipeline-2.png</a></h3>
<div class="entry">
<div class="content">
<p>Earlier this year I worked on a certain GStreamer plugin that is called ‚Äúipcpipeline‚Äù. This plugin provides elements that make it possible to interconnect GStreamer pipelines that run in different processes.&nbsp; In this blog post I am going to explain how this plugin works and the reason why you might want to use it in your application.</p>
<p><span id="more-200"></span></p>
<h2>Why <em>ipcpipeline</em>?</h2>
<p>In GStreamer, pipelines are meant to be built and run inside a single process. Normally one wouldn‚Äôt even think about involving multiple processes for a single pipeline. You can (and <em>should</em>) involve multiple threads, of course, which is easily done using the <em>queue</em> element, in order to do parallel processing. But since you can involve multiple threads, why would you want to involve multiple processes as well?</p>
<p>Splitting part of a pipeline to a different process is useful when there is one or more elements that need to be isolated for <strong>security</strong> reasons. Imagine the case where you have an application that uses a hardware video decoder and therefore has device access privileges. Also imagine that in the same pipeline you have elements that download and parse video content directly from a network server, like most Video On Demand applications would do. Although <span>I don‚Äôt mean to say that GStreamer is not secure</span>, it can be a good idea to think ahead and make it as hard as possible for an attacker to take advantage of potential security flaws. <strong>In theory</strong>, maybe someone could exploit a bug in the container parser by sending it crafted data from a fake server and then take control of other things by exploiting those device access privileges, or cause a system crash. <em>ipcpipeline</em> could help to prevent that.</p>
<h2>How does it work?</h2>
<p>In the ‚Äì oversimplified ‚Äì diagram below we can see how the media pipeline in a video player would look like with GStreamer:</p>
<p><img></p>
<p>With <em>ipcpipeline</em>, this pipeline can be split into two processes, like this:</p>
<p><img></p>
<p>As you can see, the split mainly involves 2 elements: <em>ipcpipelinesink</em>, which serves as the sink for the first pipeline, and <em>ipcpipelinesrc</em>, which serves as the source for the second pipeline. These two elements internally talk to each other through a unix pipe or socket, transferring buffers, events, queries and messages over this socket, thus linking the two pipelines together.</p>
<p>This mechanism doesn‚Äôt look very special, though. You might be wondering at this point, what is the difference between using <em>ipcpipeline</em> and some other existing mechanism like a pair of fdsink/fdsrc or udpsink/udpsrc or RTP? <strong>What is special</strong> about these elements is that the two pipelines behave <strong>as if</strong> they were a single pipeline, with the elements of the second one being part of a <em>GstBin</em> in the first one:</p>
<p><img></p>
<p>The diagram above illustrates how you can think of a pipeline that uses the <em>ipcpipeline</em> mechanism. As you can see, <em>ipcpipelinesink</em> behaves as a <em>GstBin</em> that contains the whole remote pipeline. This practically means that whenever you change the state of <em>ipcpipelinesink</em>, the remote pipeline‚Äôs state changes as well. It also means that all messages, events and queries that make sense are forwarded from one pipeline to the other, trying to implement as closely as possible the behavior that a <em>GstBin</em> would have.</p>
<p>This design practically allows you to modify an existing application to use this split-pipeline mechanism without having to change the pipeline control logic or implement your own IPC for controlling the second pipeline. It is all integrated in the mechanism already.</p>
<p><em>ipcpipeline</em> follows a master-slave design. The pipeline that controls the state changes of the other pipeline is called the ‚Äúmaster‚Äù, while the other one is called the ‚Äúslave‚Äù. In the above example, the pipeline that contains&nbsp;the <em>ipcpipelinesink</em> element is the ‚Äúmaster‚Äù, while the other one is the ‚Äúslave‚Äù. At the moment of writing, the opposite setup is not implemented, so it‚Äôs always the downstream part of the pipeline that can be slaved and <em>ipcpipelinesink</em> is always the ‚Äúmaster‚Äù.</p>
<p>While it is possible to have only one ‚Äúmaster‚Äù pipeline, it is possible to have multiple ‚Äúslave‚Äù ones. This allows, for example, to split an audio decoder and a video decoder into different processes:</p>
<p><img></p>
<p>It is also possible to have multiple <em>ipcpipelinesink</em> elements connect to the same slave pipeline. In this case, the slave pipeline will follow the state that is closest to PLAYING between the two states that it will get from the two <em>ipcpipelinesink</em>s. Also, messages from the slave pipeline will only be forwarded through one of the two <em>ipcpipelinesink</em>s, so you will not notice any duplicate messages. Behavior should be exactly the same as in the split slaves scenario.</p>
<p><img></p>
<h2>Where is the code?</h2>
<p><em>ipcpipeline</em> is part of the GStreamer bad plugins set (<a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/sys/ipcpipeline">here</a>). Documentation is included with the code and there are also some <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/tests/examples/ipcpipeline">examples</a> that you can try out to get familiar with it. Happy hacking!</p><br>  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/gkiagia.wordpress.com/200/"><img alt="" src="untitled" border="0"></a> <img alt="" src="b.gif" width="1" height="1" border="0"></div>

<p class="date">
<a href="https://gkiagia.wordpress.com/2017/11/17/ipcpipeline-splitting-a-gstreamer-pipeline-into-multiple-processes/">by gkiagia at November 17, 2017 01:25 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">November 01, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://k-d-w.org/101 at https://k-d-w.org" lang="en">
<h3><a href="https://k-d-w.org/" title="Sebastian P√∂lsterl's blog">Sebastian P√∂lsterl</a> ‚Äî <a href="https://k-d-w.org/node/101">scikit-survival 0.4 released and presented at PyCon UK 2017</a></h3>
<div class="entry">
<div class="content">
<div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even"><div class="tex2jax"><p>I'm pleased to announce that <a href="https://github.com/sebp/scikit-survival">scikit-survival</a> version 0.4 has been released.</p>
<p>This release adds <a href="https://scikit-survival.readthedocs.io/en/latest/generated/sksurv.linear_model.CoxnetSurvivalAnalysis.html">CoxnetSurvivalAnalysis</a>, which implements an <a href="http://www.jstatsoft.org/v39/i05">efficient algorithm</a> to fit Cox‚Äôs proportional hazards model with LASSO, ridge, and elastic net penalty. This allows fitting a Cox model to high-dimensional data and perform feature selection. Moreover, it includes support for Windows with Python 3.5 and later by making the cvxopt package optional.</p>
<h2>Download</h2>
<p>
You can install the latest version via Anaconda (OSX and Linux):</p>
<div class="geshifilter">
<div class="bash geshifilter-bash">
<pre class="de1">conda <span class="kw2">install</span> scikit-survival</pre></div>
</div>
<p>or via pip (all platforms):</p>
<div class="geshifilter">
<div class="bash geshifilter-bash">
<pre class="de1">pip <span class="kw2">install</span> <span class="re5">-U</span> scikit-survival</pre></div>
</div>

<h2>PyCon UK</h2>
<p>
Last week, I presented an <a href="http://2017.pyconuk.org/sessions/talks/introduction-to-survival-analysis-with-scikit-survival/">Introduction to Survival Analysis with scikit-survival</a> at PyCon UK in Cardiff in front of a packed audience of genuinely interested people. I hope some people will give scikit-survial a try and use it in their work.</p>
<p>
The slides of my presentation are available at <a href="https://k-d-w.org/pyconuk-2017/">https://k-d-w.org/pyconuk-2017/</a>.
</p>
</div></div></div></div></div>

<p class="date">
<a href="https://k-d-w.org/node/101">by sebp at November 01, 2017 10:29 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 30, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://blogs.igalia.com/vjaquez/?p=598" lang="en-US">
<h3><a href="https://blogs.igalia.com/vjaquez" title="gstreamer ‚Äì Herostratus‚Äô legacy">V√≠ctor J√°quez</a> ‚Äî <a href="https://blogs.igalia.com/vjaquez/2017/10/30/gstreamer-conference-2017/">GStreamer Conference 2017</a></h3>
<div class="entry">
<div class="content">
<p>This year, the <a href="https://gstreamer.freedesktop.org/conference/2017/">GStreamer Conference</a>  happened in Prague, along with the traditional autumn Hackfest.</p>
<p>Prague is a beautiful city, though this year I couldn‚Äôt visit it as much as I wanted, since the Embedded Linux Conference Europe and the Open Source Summit also took place there, and <a href="https://www.igalia.com/">Igalia</a>, being a Linux Foundation sponsor, had a booth in the venue, where I talked about our work with WebKit, Snabb, and obviously, GStreamer.</p>
<p>But, let‚Äôs back to the GStreamer Hackfest and Conference.</p>
<p>One of the features that I like the most of the GStreamer project is its community, the people involved in it, by writing code, sharing their work with many others. They might appear a bit tough at beginning (or at least that looked to me) but in real they are all kind and talented persons. And I‚Äôm proud of consider myself part of this community. Nonetheless it has a diversity problem, as many other Open Source communities.</p>
<p><img src="gstconf2017-580x326.jpg" alt="GStreamer Conference 2017" class="aligncenter size-medium wp-image-599" width="580" height="326"></p>
<p>During the Hackfest, Hyunjun and I, met with Sree and talked about the plans for GStreamer-VAAPI, the new features in VA-API and libva and how we could map them to the GStreamer‚Äôs design. Also we talked about the future developments in the <em>msdk</em> elements, merged one year ago in <code>gst-plugins-bad</code>. Also, I talked a bit with Nicolas Dufresne regarding <code>kmssink</code> and <code>DMABuf</code>.</p>
<p>In the Conference, which happened in the same venue as the hackfest, I talked wit the authors of <a href="https://github.com/intel/gstreamer-media-SDK">gstreamer-media-SDK</a>. They are really energetic.</p>
<p>I delivered my usual talk about <code>GStreamer-VAAPI</code>. You can find the slides, as a web presentation, <a href="https://people.igalia.com/vjaquez/talks/gstvaapi-201710">here</a>. Also, as every year, our friends of <a href="https://www.ubicast.eu/">Ubicast</a>, recorded the talks, and <a href="https://gstconf.ubicast.tv/channels/#gstreamer-conference-2017">made them available for streaming</a> almost instantaneously:</p>
<p></p>
<p>My colleague Enrique talked in the Conference about the <a href="https://gstconf.ubicast.tv/videos/media-source-extension-on-webkit">Media Source Extensions (MSE) on WebKit</a>, and Hyunjun shared his experience with <a href="https://gstconf.ubicast.tv/videos/va-api-rust-binding/">VA-API on Rust</a>.</p>
<p>Also, in the conference venue, we showed a couple demos. One of them was a MinnowBoard running <a href="https://www.igalia.com/wpe/">WPE</a>, rendering videos from YouTube using <code>gstreamer-vaapi</code> to decode video.</p></div>

<p class="date">
<a href="https://blogs.igalia.com/vjaquez/2017/10/30/gstreamer-conference-2017/">by vjaquez at October 30, 2017 04:24 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 25, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=408" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> ‚Äî <a href="https://coaxion.net/blog/2017/10/multi-threaded-raw-video-conversion-and-scaling-in-gstreamer/">Multi-threaded raw video conversion and scaling in GStreamer</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dr√∂ge)" width="80" height="80">
<p>Another new feature that landed in <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> <a href="https://bugzilla.gnome.org/show_bug.cgi?id=778974" rel="noopener" target="_top">already a while ago</a>, and is included in the 1.12 release, is multi-threaded raw video conversion and scaling. The short story is that it lead to e.g. 3.2x speed-up converting 1080p video to 4k with 4 cores.</p>
<p>I had a few cases where a single core was not able to do rescaling in real-time anymore, even on a quite fast machine. One of the cases was 60fps 4k video in the v210 (10 bit YUV) color format, which is a lot of bytes per second in a not very processing-friendly format. GStreamer‚Äôs video converter and scaler is already quite optimized and using SIMD instructions like SSE or Neon, so there was not much potential for further optimizations in that direction.<br>
However basically every machine nowadays has multiple CPU cores that could be used and raw video conversion/scaling is an almost perfectly parallelizable problem, and the way how the conversion code was already written it was relatively easy to add.</p>
<p>The way it works now is similar to the processing model of libraries like <a href="https://en.wikipedia.org/wiki/OpenMP" rel="noopener" target="_top">OpenMP</a> or <a href="https://github.com/rayon-rs/rayon" rel="noopener" target="_top">Rayon</a>. The whole work is divided into smaller, equal sub-problems that are then handled in parallel, then it is waiting until all parts are done and the result is combined. In our specific case that means that each plane of the video frame is cut into 2, 4, or more slices of full rows, which are then converted separately. The ‚Äúcombining‚Äù step does not exist, all sub-conversions are directly written to the correct place in the output already.</p>
<p>As a small helper object for this kind of processing model, I wrote <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-base/tree/gst-libs/gst/video/video-converter.c?id=32ef8f54d4728da465bfd20456e22d182a822570#n118" rel="noopener" target="_top">GstParallelizedTaskRunner</a> which might also be useful for other pieces of code that want to do the same.</p>
<p>In the end it was not much work, but the results were satisfying. For example the conversion of 1080p to 4k video in the v210 color format with 4 threads gave a speedup of 3.2x. At that point it looks like the main bottleneck was memory bandwidth, but I didn‚Äôt look closer as this is already more than enough for the use cases I was interested in.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/10/multi-threaded-raw-video-conversion-and-scaling-in-gstreamer/">by slomo at October 25, 2017 04:35 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 21, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=490" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> ‚Äî <a href="https://coaxion.net/blog/2017/10/rendering-html5-video-in-servo-with-gstreamer/">Rendering HTML5 video in Servo with GStreamer</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dr√∂ge)" width="80" height="80">
<p>At the <a href="https://webengineshackfest.org/" rel="noopener" target="_top">Web Engines Hackfest</a> in A Coru√±a at the beginning of October 2017, I was working on adding some proof-of-concept code to <a href="https://servo.org/" rel="noopener" target="_top">Servo</a> to render HTML5 videos with <a href="https://gstreamer.freedesktop.org-TRICKMODE-KEY-UNITS:CAPS" rel="noopener" target="_top">GStreamer</a>. For the impatient, the results can be seen in this video here</p>
<p></p>
<p>And the code can be found <a href="https://github.com/sdroege/servo/tree/gst" rel="noopener" target="_top">here</a> and <a href="https://github.com/sdroege/rust-playground" rel="noopener" target="_top">here</a>.</p>
<h5>Details</h5>
<p>Servo is <a href="https://www.mozilla.org/" rel="noopener" target="_top">Mozilla</a>‚Äòs experimental browser engine written in <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a>, optimized for high-performance, parallelized rendering. Some of the parts of Servo are being merged in Firefox as part of the <a href="https://www.mozilla.org/en-US/firefox/quantum/" rel="noopener" target="_top">Project Quantum</a>, and already provide a lot of performance and stability improvements there.</p>
<p>During the hackfest I actually spent most of the time trying to wrap my head around the huge Servo codebase. It seems very well-structured and designed, exactly what you would expect from starting such a project from scratch by a company that has decades of experience writing browser engines already. After also having worked on <a href="https://webkit.org/" rel="noopener" target="_top">WebKit</a> in the past, I would say that you can see the difference of a legacy codebase from the end of the 90s and something written in a modern language with modern software engineering practices.</p>
<p>To the actual implementation of HTML5 video rendering via GStreamer, I actually started on top of the initial implementation that <a href="http://base-art.net/" rel="noopener" target="_top">Philippe Normand</a> started before already. That one was rendering the video in a separate window though, and did not work with the latest version of Servo anymore. I cleaned it up and made it work again (probably the best task you can do to learn a new codebase), and then added support for actually rendering the video inside the web view.</p>
<p>This required quite a few additions on the Servo side, some of which are probably more hacks than anything else, but from the GStreamer-side is was extremely simple. In Servo currently all the infrastructure for media rendering is still missing, while GStreamer has more than a decade of polishing for making integration into other software as easy as possible.</p>
<p>All the GStreamer code was written with the <a href="https://crates.io/crates/gstreamer" rel="noopener" target="_top">GStreamer Rust bindings</a>, containing not a single line of unsafe code.</p>
<p>As you can see from the above video, the results work quite well already. Media controls or anything more fancy are not working though. Also rendering is currently done completely in software, and a RGBA frame is then uploaded via OpenGL to the GPU for rendering. However, hardware codecs can already be used just fine, and basically every media format out there is supported.</p>
<h5>Future</h5>
<p>While this all might sound great, unfortunately Mozilla‚Äôs plans for media support in Servo are different. They‚Äôre planning to use the C++ Firefox/Gecko media backend instead of GStreamer. Best to ask them for reasons, I would probably not repeat them correctly.</p>
<p>Nonetheless, I‚Äôll try to keep the changes updated with latest Servo and once they add more things for media support themselves add the corresponding GStreamer implementations in my branch. It still provides value for both showing that GStreamer is very well capable of handling web use cases (which it already showed in WebKit), as well as being a possibly better choice for people trying to use Servo on embedded systems or with hardware codecs in general. But as I‚Äôll have to work based on what they do, I‚Äôm not going to add anything fundamentally new myself at this point as I would have to rewrite it around whatever they decide for the implementation of it anyway.</p>
<p>Also once that part is there, having GStreamer directly render to an OpenGL texture would be added, which would allow direct rendering with hardware codecs to the screen without having the CPU worry about all the raw video data.</p>
<p>But for now, it‚Äôs waiting until they catch up with the Firefox/Gecko media backend.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/10/rendering-html5-video-in-servo-with-gstreamer/">by slomo at October 21, 2017 11:25 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 20, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=414" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> ‚Äî <a href="https://coaxion.net/blog/2017/10/dash-trick-mode-playback-in-gstreamer-fast-forwardrewind-without-saturating-your-network-and-cpu/">DASH trick-mode playback in GStreamer: Fast-forward/rewind without saturating your network and CPU</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dr√∂ge)" width="80" height="80">
<p><a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> now has support for I-frame-only (aka keyframe) trick mode playback of <a href="https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP" rel="noopener" target="_top">DASH</a> streams. It works only on DASH streams with ISOBMFF (aka MP4) fragments, and only if these contain all the required information. This is something I wanted to blog about since many months already, and <a href="https://bugzilla.gnome.org/show_bug.cgi?id=741104" rel="noopener" target="_top">it‚Äôs even included</a> in the GStreamer 1.10 release already.</p>
<p>When trying to play back a DASH stream with rates that are much higher than real-time (say 32x), or playing the streams in reverse, you can easily run into various problems. This is something that was already supported by GStreamer in older versions, for both DASH streams as well as local files or HLS streams but it‚Äôs far from ideal. What would happen is that you usually run out of available network bandwidth (you need to be able to download the stream 32x faster than usual), or out of CPU/GPU resources (it needs to be decoded 32x faster than usual) and even if all that works, there‚Äôs no point in displaying 960 (30fps at 32x) frames per second.</p>
<p>To get around that, GStreamer 1.10 can now (if explicitly requested with <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstEvent.html#GST-SEEK-FLAG-TRICKMODE-KEY-UNITS:CAPS" rel="noopener" target="_top">GST_SEEK_FLAG_TRICKMODE_KEY_UNITS</a>) only download and decode I-frames. Depending on the distance of I-frames in the stream and the selected playback speed, this looks more or less smooth. Also depending on that, this might still yield to many frames to be downloaded or decoded in real-time, so GStreamer also measures the distance between I-frames, how fast data can be downloaded and whether decoders and sinks can catch up to decide whether to skip over a couple of I-frames and maybe only download every third I-frame.</p>
<p>If you want to test this, grab the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-base/tree/tests/examples/playback/playback-test.c" rel="noopener" target="_top">playback-test</a> from GStreamer, select the trickmode key-units mode, and seek in a DASH stream while providing a higher positive or negative (reverse) playback rate.</p>
<p>Let us know if you run into any problems with any specific streams!</p>
<h5>Short Implementation Overview</h5>
<p>From an implementation point of view this works by having the DASH element in GStreamer (dashdemux) not only download the ISOBMFF fragments but also parses the headers of each to get the positions and distances of each I-frame in the fragment. Based on that it then decides which ones to download or whether to skip ahead one or more fragments. The ISOBMFF headers are then passed to the MP4 demuxer (qtdemux), followed by discontinuous buffers that only contain the actual I-frames and nothing else. While this sounds rather simple from an high-level point of view, getting this all right in the details was the result of a couple of months of work by Edward Hervey and myself.</p>
<p>Currently the heuristics for deciding which I-frames to download and how much to skip ahead are rather minimal, but it‚Äôs working fine in many situations already. A lot of tuning can still be done though, and some streams are working less well than others which can also be improved.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/10/dash-trick-mode-playback-in-gstreamer-fast-forwardrewind-without-saturating-your-network-and-cpu/">by slomo at October 20, 2017 10:53 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 19, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2756" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> ‚Äî <a href="https://blogs.gnome.org/uraeus/2017/10/19/looking-back-at-fedora-workstation-so-far/">Looking back at Fedora Workstation so far</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>So I have over the last few years blogged regularly about upcoming features in Fedora Workstation. Well I thought as we putting the finishing touches on Fedora Workstation 27 I should try to look back at everything we have achieved since Fedora Workstation was launched with Fedora 21. The efforts I highlight here are efforts where we have done significant or most development. There are of course a lot of other big changes that has happened over the last few years by the wider community that we leveraged and offer in Fedora Workstation, examples here include things like Meson and Rust. This post is not about those, but that said I do want to write a post just talking about the achievements of the wider community at some point, because they are very important and crucial too. And along the same line this post will not be speaking about the large number of improvements and bugfixes that we contributed to a long list of projects, like to GNOME itself. This blog is about taking stock and taking some pride in what we achieved so far and major hurdles we past on our way to improving the Linux desktop experience.<br>
This blog is also slightly different from my normal format as I will not call out individual developers by name as I usually do, instead I will focus on this being a totality and thus just say ‚Äòwe‚Äô.</p>
<ul>
<li>Wayland ‚Äì We been the biggest contributor since we joined the effort and have taken the lead on putting in place all the pieces needed for actually using it on a desktop, including starting to ship it as our primary offering in Fedora Workstation 25. This includes putting a lot of effort into ensuring that XWayland works smoothly to ensure full legacy application support.</li>
<li>Libinput ‚Äì A new library we created for handling all input under both X and Wayland. This came about due to needing input handling that was not tied to X due to Wayland, but it has even improved input handling for X itself. Libinput is being rapidly developed and improved, with 1.9 coming out just a few days ago.</li>
<li>glvnd ‚Äì Dealing with multiple OpenGL implementations have been a pain under Linux for years. We worked with NVidia on this effort to ensure that you can install multiple OpenGL implementations on the system and have your system be able to use the correct one depending on which GPU and driver you are using. We keep expanding on this solution to cover more usecases, so for Fedora Workstation 27 we expect to bring glvnd support to XWayland for instance.</li>
<li>Porting Firefox to GTK3 ‚Äì We ported Firefox to GTK3, including making sure it works under Wayland. This work also provided the foundation for HiDPI support in Firefox. We are the single biggest contributor to Firefox Linux support.</li>
<li>Porting LibreOffice to GTK3 ‚Äì We ported LibreOffice to GTK3, which included Wayland support, touch support and HiDPI support. Our team is one of the major contributors to LibreOffice and help the project forward on a lot of fronts.</li>
<li>Google Drive integration ‚Äì We extended the general Google integration in GNOME 3 to include support for Google Drive as we found that a lot of our users where relying on Google Apps at their work.</li>
<li>Flatpak ‚Äì We created Flatpak to lead the way in moving desktop applications into their own namespaces and containers, resolving a lot of long term challenges for desktop applications on Linux. We expect to have new infrastructure in place in Fedora soon to allow Fedora packagers to quickly and easily turn their applications into Flatpaks.</li>
<li>Linux Firmware Service ‚Äì We created the Linux Firmware service to provide a way for Linux users to get easy access to UEFI firmware on their linux system and worked with great vendors such as Dell and Logitech to get them to support it for their devices. Many bugs experienced by Linux users over the years could have been resolved by firmware updates, but with tooling being spotty many Linux users where not even aware that there was fixes available.</li>
<li>GNOME Software ‚Äì We created GNOME Software to give us a proper Software Store on Fedora and extended it over time to include features such as fonts, GStreamer plugins, GNOME Shell extensions and UEFI firmware updates. Today it is the main Store type application used not just by us, but our work has been adopted by other major distributions too.</li>
<li>mp3, ac3 and aac support ‚Äì We have spent a lot of time to be able to bring support for some of the major audio codecs to Fedora like MP3, AC3 and AAC. In the age of streaming supporting codecs is maybe of less importance than it used to be, but there is still a lot of media on peoples computers they need and want access to.</li>
<li>Fedora Media Creator ‚Äì Cross platform media creator making it very easy to create Fedora Workstation install media regardless of if you are on Windows, Mac or Linux. As we move away from optical media offering ISO downloads started feeling more and more outdated, with the media creator we have given a uniform user experience to quickly create your USB install media, especially important for new users coming in from Windows and Mac environments.</li>
<li>Captive portal ‚Äì We added support for captive portals in Network Manager and GNOME 3, ensuring easy access to the internet over public wifi networks. This feature has been with us for a few years now, but it is still a much appreciated addition.</li>
<li>HiDPI support ‚Äì We worked to add support for HiDPI across X, Wayland, GTK3 and GNOME3. We lead the way on HiDPI support under Linux and keep working on various applications to this date to polish up the support.</li>
<li>Touch support ‚Äì We worked to add support for touchscreens across X, Wayland, GTK3 and GNOME3. We spent significant resources enabling this, both on laptop touchscreens, but also to support modern wacom devices.</li>
<li>QGNOME Platform ‚Äì We created the QGNOME Platform to ensure that Qt applications work well under GNOME3 and gives a nice native and integrated feel. So while we ship GNOME as our desktop offering we want Qt applications to work well and feel native. This is an ongoing effort, but for many important applications it already is a great improvement.</li>
<li>Nautilus improvements. Nautilus had been undermaintained for quite a while so we had Carlos Soriano spend significant time on reworking major parts of it and adding new features like renaming multiple files at ones, updating the views and in general bring it up to date.</li>
<li>Night light support in GNOME ‚Äì We added support for automatic adjusting the color and light settings on your system based on light sensors found in modern laptops. This integrated functionality that you before had to install extra software like Red Shift to enable.</li>
<li>libratbag ‚Äì We created a library that enable easy configuration of high end mice and other kind of input devices. This has led to increased collaboration with a lot of gaming mice manufacturers to ensure full support for their devices under Linux.</li>
<li>RADV ‚Äì We created a full open source Vulkan implementation for ADM GPUs which recently got certified as Vulkan compliant. We wanted to give open source Vulkan a boost, so we created the RADV project, which now has an active community around it and is being tested with major games.</li>
<li>GNOME Shell performance improvements ‚Äì We been working on various performance improvements to GNOME Shell over the last few years, with significant improvements having happened. We want to push the envelope on this further though and are planning a major performance hackfest around Shell performance and resource usage early next year.</li>
<li>GNOME terminal developer improvements ‚Äì We worked to improve the features of GNOME Terminal to make it an even better tool for developers with items such as easier naming of terminals and notifications for long running jobs.</li>
<li>GNOME Builder ‚Äì Improving the developer story is crucial for us and we been doing a lot of work to make GNOME Builder a great tool for developer to use to both improve the desktop itself, but also development in general.</li>
<li>Pipewire ‚Äì We created a new media server to unify audio, pro-audio and video. First version which we are shipping in Fedora 27 to handle our video capture.</li>
<li>Fleet Commander ‚Äì We launched Fleet Commander our new tool for managing large Linux desktop deployments. This answer a long standing call from many of Red Hats major desktop customers and many admins of large scale linux deployments at Universities and similar for a powerful yet easy to use administration tool for large desktop deployments. </li>
</ul>
<p>I am sure  I missed something, but this is at least a decent list of Fedora Workstation highlights for the last few years. Next onto working on my Fedora Workstation 27 blogpost :)</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/10/19/looking-back-at-fedora-workstation-so-far/">by uraeus at October 19, 2017 06:35 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 18, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2755" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> ‚Äî <a href="https://blogs.gnome.org/uraeus/2017/10/18/fleet-commander-ready-for-takeoff/">Fleet Commander ready for takeoff!</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>Alberto Ruiz just announced Fleet Commander as production ready! Fleet Commander is our new tool for managing large deployments of Fedora Workstation and RHEL desktop systems. So get our to <a href="https://siliconislandblog.wordpress.com/2017/10/18/fleet-commander-production-ready/">Albertos Fleet Commander blog post for all the details</a>.</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/10/18/fleet-commander-ready-for-takeoff/">by uraeus at October 18, 2017 12:01 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 17, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://eocanha.org/blog/?p=526" lang="en-US">
<h3><a href="http://eocanha.org/blog" title="GStreamer ‚Äì Happy coding">Enrique Oca√±a Gonz√°lez</a> ‚Äî <a href="http://eocanha.org/blog/2017/10/17/attending-the-gstreamer-conference-2017/">Attending the GStreamer Conference 2017</a></h3>
<div class="entry">
<div class="content">
<p>This weekend I‚Äôll be in <a href="https://gstreamer.freedesktop.org/conference/2017/">Node5</a> (Prague) presenting our Media Source Extensions platform implementation work in WebKit using GStreamer.</p>
<p>The <a href="https://w3c.github.io/media-source/">Media Source Extensions HTML5 specification</a> allows JavaScript to generate media streams for playback and lets the web page have more control on complex use cases such as adaptive streaming.</p>
<p>My plan for the talk is to start with a brief introduction about the motivation and basic usage of MSE. Next I‚Äôll show a design overview of the WebKit implementation of the spec. Then we‚Äôll go through the iterative evolution of the GStreamer platform-specific parts, as well as its implementation quirks and challenges faced during the development. The talk continues with a demo, some clues about the future work and a final round of questions.</p>
<p>Our recent MSE work has been on desktop <a href="https://webkitgtk.org/">WebKitGTK+</a>&nbsp;(the WebKit version powering the <a href="https://wiki.gnome.org/Apps/Web">Epiphany</a>, aka: GNOME Web), but we also have MSE&nbsp;working on <a href="https://trac.webkit.org/wiki/WPE">WPE</a> and optimized for a Raspberry Pi 2. We will be showing it in the <a href="https://www.igalia.com/">Igalia</a> booth, in case you want to see it working live.</p>
<p>I‚Äôll be also attending the <a href="https://wiki.gnome.org/Hackfests/GstAutumnHackfest2017">GStreamer Hackfest</a> the days before. There I plan to work on webm support in MSE, focusing on any issue in the Matroska demuxer or the vp9/opus/vorbis decoders breaking our use cases.</p>
<p>See you there!</p>
<p>UPDATE 2017-10-22:</p>
<p>The talk slides are available at&nbsp;<a href="https://eocanha.org/talks/gstconf2017/gstconf-2017-mse.pdf">https://eocanha.org/talks/gstconf2017/gstconf-2017-mse.pdf</a>&nbsp;and the video is available at&nbsp;<a href="https://gstconf.ubicast.tv/videos/media-source-extension-on-webkit">https://gstconf.ubicast.tv/videos/media-source-extension-on-webkit</a> (the rest of the talks <a href="https://gstconf.ubicast.tv/channels/#gstreamer-conference-2017">here</a>).</p></div>

<p class="date">
<a href="http://eocanha.org/blog/2017/10/17/attending-the-gstreamer-conference-2017/">by eocanha at October 17, 2017 10:48 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 12, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2754" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> ‚Äî <a href="https://blogs.gnome.org/uraeus/2017/10/12/aac-support-will-be-available-in-fedora-workstation-27/">AAC support will be available in Fedora Workstation 27!</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>So I am really happy to announce another major codec addition to Fedora Workstation 27 namely the addition of the codec called AAC. As you might have seen from <a href="https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/F64JBJI2IZFT2A5QDXGHNMPALCQIVJAX/">Tom Callaways announcement</a> this has just been cleared for inclusion in Fedora.</p>
<p>For those not well versed in the arcane lore of audio codecs AAC is the codec used for things like iTunes and is found in a lot of general media files online. AAC stands for Advanced Audio Coding and was created by the MPEG working group as the successor to mp3. Especially due to Apple embracing the format there is a lot of files out there using it and thus we wanted to support it in Fedora too.</p>
<p>What we will be shipping in Fedora is a modified version of the AAC implementation released by Google, which was originally written by Frauenhoffer. On top of that we will of course be providing GStreamer plugins to enable full support for playing and creating AAC files for GStreamer applications.</p>
<p>Be aware though that AAC is a bit of an umbrella term for a lot of different technologies and thus you might be able to come across files that claims to use AAC, but which we can not play back. The most likely reason for that would be that it requires a AAC profile we do not support. The version of AAC that we will be shipping has also be carefully created to fit within the requirements for software in Fedora, so if you are a packager be aware that unlike with for instance mp3, this change does not mean you can package and ship any AAC implementation you want to in Fedora.</p>
<p>I am expecting to have more major codec announcements soon, so stay tuned :)</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/10/12/aac-support-will-be-available-in-fedora-workstation-27/">by uraeus at October 12, 2017 04:34 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">October 06, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://fortintam.com/blog/?p=3668" lang="en-US">
<h3><a href="http://fortintam.com/blog" title="J.F. Fortin Tam">Jean-Fran√ßois Fortin Tam</a> ‚Äî <a href="http://fortintam.com/blog/2017/10/05/la-sphere-logiciel-et-materiel-libre/">Libert√© logicielle et mat√©rielle, compte rendu de l‚Äô√©mission La Sph√®re du 16 septembre</a></h3>
<div class="entry">
<div class="content">
<p>Le 13 septembre, je re√ßus un curieux courriel m‚Äôinvitant √† participer √† l‚Äô√©mission <em>¬´&nbsp;La Sph√®re&nbsp;¬ª</em> pour <a href="http://ici.radio-canada.ca/premiere/emissions/la-sphere/episodes/389873/logiciels-libres" target="_top" rel="noopener">un √©pisode d√©di√© au logiciel libre</a>, sur la principale cha√Æne radiophonique de <a href="https://fr.wikipedia.org/wiki/Soci%C3%A9t%C3%A9_Radio-Canada" target="_top" rel="noopener">Radio-Canada</a> le samedi 16 septembre.</p>
<img class="wp-image-3755 size-full" src="src-premiere-sphere-jf.jpg" alt="" width="300" height="169">Quelques minutes avant le d√©but de l‚Äô√©mission
<p><span id="more-3668"></span></p>
<p>L‚Äô√©pisode dure environ une heure, et la version baladodiffusion est divis√©e en divers segments, mais comme on m‚Äôa amen√© √† commenter √† travers pas mal tous les segments ou presque, je vous invite √† <a href="http://medias-balado.radio-canada.ca/diffusion/2017/09/balado/src/CBF/2017-09-16_14_14_13_laspherebalado_0000.mp3">√©couter l‚Äô√©pisode int√©gral</a> si le coeur vous en dit.</p>
<h1>Loi de Murphy</h1>
<p>Le tout s‚Äôest bien d√©roul√©, bien que les sujets potentiels pour lesquels je m‚Äô√©tais pr√©par√© ne correspondaient pas aux questions m‚Äô√©tant pos√©es en ondes:</p>
<ul>
<li><img class="alignright size-full wp-image-1993" src="cat-writing.jpg" alt="" width="250" height="275">Ayant re√ßu quatre th√©matiques √† minuit la veille de l‚Äô√©mission, je r√©digeai en vitesse, le matin m√™me‚Äîavant de me diriger vers les studios‚Äîquelques 1200 mots pour r√©pondre √† ces th√©matiques de fa√ßon structur√©e. Je m‚Äô√©tais donc pr√©par√© des points de discussion et exemples clairs √† citer‚Äîau cas o√π on m‚Äôam√®nerait √† parler de s√©curit√© informatique, d‚Äôabus de corporations non-transparentes, ou de la ¬´ futilit√© ¬ª per√ßue du logiciel libre dans un monde o√π le mat√©riel n‚Äôest pas forc√©ment sous notre contr√¥le‚Ä¶</li>
<li>Ce document, affich√© √† l‚Äô√©cran du <a href="https://puri.sm/products/" target="_top" rel="noopener">Librem</a> que j‚Äôavais devant moi en studio, n‚Äôa finalement pas servi, les discussions ayant pris des tournures compl√®tement diff√©rentes. D√®s les premi√®res questions, je r√©alisais que je n‚Äôaurais pas l‚Äôopportunit√© de rentrer dans du technique/l√©gal/philo de profondeur, et qu‚Äôil fallait donc que je r√©oriente toute ma strat√©gie de discussion sur-le-champ. Mes r√©ponses lors de l‚Äô√©mission √©taient donc <em>toutes</em> construites en temps r√©el, dans le feu de l‚Äôaction.</li>
</ul>
<p>On m‚Äôa parfois lanc√© des questions st√©r√©otyp√©es‚Äîun peu r√©thoriques certes, mais c‚Äô√©tait sans doute pour soulever des questions que le public cible se pose probablement!‚Äîme for√ßant dans une position corrective/d√©fensive (o√π il fallait que je corrige avant toute chose l‚Äôid√©e re√ßue avant de pouvoir m√™me <em>envisager</em> parler d‚Äôautre chose), mais il est justement pertinent de d√©busquer ces id√©es re√ßues, puisqu‚Äôil s‚Äôagit d‚Äôune √©mission de vulgarisation pour le grand public‚Ä¶</p>
<h1>Quelques moments de surprise</h1>
<p>Durant l‚Äô√©mission, j‚Äôai √©galement flair√© quelques propos autour de la table qui n‚Äô√©taient pas aussi nuanc√©s que je l‚Äôaurais souhait√©, ou encore des questions m‚Äôayant parfois laiss√© bouche b√©e (telles que ¬´ Dans le fond, les gens ne contribuent-t-ils pas au libre principalement pour se faire du CV et laisser tomber une fois embauch√©s? ¬ª ainsi que ¬´ Si je veux installer un CRM dans une compagnie, je pourrai jamais utiliser un Librem pour le faire ¬ª ‚Äî dans le deuxi√®me cas, j‚Äô√©tais tellement d√©concert√© de la largesse d‚Äôune telle affirmation que je ne pouvais que vaguement r√©pondre ¬´ Hum‚Ä¶ √ßa d√©pend? ¬ª)</p>
<img class="size-full wp-image-3396" src="hehe-wait-what.gif" alt="" width="500" height="213">Ma r√©action
<p>Si j‚Äôavais pu pr√©parer une r√©ponse √† ces deux questions √† br√ªle-pourpoint, j‚Äôaurais par exemple voulu:</p>
<ul>
<li>dire que personne ne contribue au libre d‚Äôune fa√ßon ainsi machiav√©llique‚Äîcontribuer au libre est une question de philosophie et d‚Äô√©thique autant que de m√©thodologie, et si le coeur n‚Äôy est pas les contributions ne seront pas convaincantes, il n‚Äôy aurait pas de quoi se b√¢tir une riche carri√®re;</li>
<li>chercher √† savoir de quoi mon interlocuteur parlait exactement c√¥t√© CRM, √©tant donn√© que les CRMs libres (ou au moins ouverts) sont multiples et que, g√©n√©ralement, les applications sont majoritairement des infrastructures web aujourd‚Äôhui.</li>
</ul>
<p>J‚Äôaurais voulu ouvrir la bo√Æte de pandore (tout le volet s√©curit√© informatique et vie priv√©e, qui est extr√™mement riche d‚Äôactualit√©s et particuli√®rement frappant), r√©pondre √† toutes les pr√©conceptions, donner des exemples et contre-arguments impeccables, mais il n‚Äôy avait pas le temps (comme on peut l‚Äôentendre, l‚Äô√©mission en direct a m√™me d√ª se terminer de fa√ßon pr√©cipit√©e). Rendu √† un certain point, il faut √™tre conscient des contraintes du flot de conversation et aller au plus simple et direct‚Ä¶ sinon, c‚Äôest trois heures de discussion qu‚Äôil aurait fallu.</p>
<h1>Un r√©sultat positif</h1>
<p>Je suis certes perfectionniste (comme vous avez pu le constater ci-haut), mais il reste que c‚Äô√©tait <strong>une bonne √©mission</strong>. Apr√®s tout, remettons la chose en contexte:</p>
<ul>
<li>il s‚Äôagit ici d‚Äôune √©mission <strong>destin√©e au grand public,</strong> et la majorit√© des auditeurs cibles ne sont pas des experts en informatique;</li>
<li>√† mes yeux, il est quasi miraculeux que <strong>la majeure portion d‚Äôune heure d‚Äô√©mission sur une cha√Æne <em>nationale</em> ait √©t√© consacr√©e au sujet du logiciel libre.</strong></li>
</ul>
<p>Je suis donc tout √† fait reconnaissant envers l‚Äô√©quipe de <em>La Sph√®re</em> d‚Äôavoir cherch√© √† faire une vulgarisation du sujet et des enjeux du logiciel libre‚Ä¶ dans la sph√®re publique!</p></div>

<p class="date">
<a href="http://fortintam.com/blog/2017/10/05/la-sphere-logiciel-et-materiel-libre/">by Jeff at October 06, 2017 03:52 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">September 20, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="tag:blogger.com,1999:blog-977684764667858073.post-1420436992814845104">
<h3><a href="http://hadess.net/" title="/b…ës Ààtj…õÃÉ no Ààse  Å…ë/  (hadess) | News">Bastien Nocera</a> ‚Äî <a href="http://www.hadess.net/2017/09/bluetooth-on-fedora-joypads-and-more.html">Bluetooth on Fedora: joypads and (more) security</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="hadess.png" alt="(Bastien Nocera)" width="63" height="80">
It's been a while since I posted about Fedora specific Bluetooth enhancements, and even longer that I <a href="http://www.hadess.net/2009/05/sixaxis-support-in-bluez.html">posted about PlayStation controllers support</a>.<br><br>Let's start with the nice feature.<br><br><b>Dual-Shock 3 and 4 support</b><br><br>We've had support for Dual-Shock 3 (aka Sixaxis, aka PlayStation 3 controllers) for a long while, but I've added a long-standing patchset to the Fedora packages that changes the way devices are setup.<br><br>The old way was: plug in your joypad via USB, disconnect it, and press the "P" button on the pad. At this point, and since GNOME 3.12, you would have needed the Bluetooth Settings panel opened for a question to pop up about whether the joypad can connect.<br><br>This is broken in a number of ways. If you were trying to just charge the joypad, then it would forget its original "console" and you would need to plug it in again. If you didn't have the Bluetooth panel opened when trying to use it wirelessly, then it just wouldn't have worked.<br><br>Set up is now simpler. Open the Bluetooth panel, plug in your device, and answer the question. You just want to charge it? Dismiss the query, or simply don't open the Bluetooth panel, it'll work dandily and won't overwrite the joypad's settings.<br><br><div class="separator"><a href="https://2.bp.blogspot.com/-HvYsG4jnNlo/WcJnZOUfDVI/AAAAAAAAA3c/PsM_00iStnkK56HR28ZyS_gja6geeS_-gCLcBGAs/s1600/ps3.png"><img src="ps3.png" width="640" height="456" border="0"></a></div><br>And finally, we also made sure that it works with PlayStation 4 controllers.<br><br><div class="separator"><a href="https://3.bp.blogspot.com/-ohJ4SbU6WDM/WcJnZW2dvGI/AAAAAAAAA3g/x84CxzlxA1c46fZhdINQ-2jgR5c6JcoLQCEwYBhgL/s1600/ps4.png"><img src="ps4.png" width="640" height="456" border="0"></a></div><br><br>Note that the PlayStation 4 controller <a href="https://www.playstation.com/en-ie/content/dam/support/manuals/scee/web-manuals/peripherals/ps4/ds4-usb-adapter/DS4_USB_Adapter_Ins_Manual_EN.pdf/">has a button combination</a> that allows it to be visible and pairable, except that if the device trying to connect with it doesn't behave in a particular way (probably the same way the 25‚Ç¨ RRP USB adapter does), it just wouldn't work. And it didn't work for me on a number of different devices.<br><br>Cable pairing for the win!<br><br><b>And the boring stuff</b><br><br>Hey, do you know what happened last week? There was <a href="https://arstechnica.com/information-technology/2017/09/bluetooth-bugs-open-billions-of-devices-to-attacks-no-clicking-required/">a security problem</a> in a package that I glance at sideways sometimes! Yes. Again.<br><br>A good way to minimise the problems caused by problems like this one is to lock the program down. In much the same way that you'd want to <a href="http://www.hadess.net/2017/07/security-for-security-gods-sandboxing.html">restrict thumbnailers</a>, or even <a href="http://flatpak.org/">end-user applications</a>, we can <a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html">forbid certain functionality from being available when launched via systemd</a>.<br><br>We've finally done this in recent <a href="https://lists.freedesktop.org/archives/fprint/2017-September/000890.html">fprintd</a> and <a href="https://github.com/hadess/iio-sensor-proxy/releases">iio-sensor-proxy</a> upstream releases, as well as for bluez in Fedora Rawhide. If testing goes well, we will integrate this in Fedora 27.</div>

<p class="date">
<a href="http://www.hadess.net/2017/09/bluetooth-on-fedora-joypads-and-more.html">by Bastien Nocera (noreply@blogger.com) at September 20, 2017 02:31 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">September 19, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2726" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> ‚Äî <a href="https://blogs.gnome.org/uraeus/2017/09/19/launching-pipewire/">Launching Pipewire!</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>In quite a few blog posts I been referencing Pipewire our new Linux infrastructure piece to handle multimedia under Linux better. Well we are finally ready to formally launch pipewire as a project and have created a <a href="http://pipewire.org/">Pipewire website</a> and logo.<img src="logo-6dff1f76.svg_.png" alt="Pipewire logo" class="aligncenter size-full wp-image-2732" width="200" height="200"></p>
<p>To give you all some background, Pipewire is the latest creation of GStreamer co-creator <a href="https://twitter.com/wtaymans">Wim Taymans</a>. The original reason it was created was that we realized that as desktop applications would be moving towards primarly being shipped as containerized <a href="http://flatpak.org/">Flatpaks</a> we would need something for video similar to what <a>PulseAudio</a> was doing for Audio. As part of his job here at Red Hat Wim had already been contributing to PulseAudio for a while, including implementing a new security model for PulseAudio to ensure we could securely have containerized applications output sound through PulseAudio. So he set out to write Pipewire, although initially the name he used was PulseVideo. As he was working on figuring out the core design of PipeWire he came to the conclusion that designing Pipewire to just be able to do video would be a mistake as a major challenge he was familiar with working on <a href="https://www.gstreamer.net/">GStreamer</a> was how to ensure perfect audio and video syncronisation. If both audio and video could be routed through the same media daemon then ensuring audio and video worked well together would be a lot simpler and frameworks such as GStreamer would need to do a lot less heavy lifting to make it work. So just before we starting sharing the code publicaly we renamed the project to Pinos, named after Pinos de Alhaur√≠n, a small town close to where Wim is living in southern Spain. In retrospect Pinos was probably not the worlds best name :)</p>
<p>Anyway as work progressed Wim decided to also take a look at Jack, as supporting the pro-audio usecase was an area PulseAudio had never tried to do, yet we felt that if we could ensure Pipewire supported the pro-audio usecase in addition to consumer level audio and video it would improve our multimedia infrastructure significantly and ensure pro-audio became a first class citizen on the Linux desktop. Of course as the scope grew the development time got longer too.</p>
<p>Another major usecase for Pipewire for us was that we knew that with the migration to Wayland we would need a new mechanism to handle screen capture as the way it was done under X was very insecure. So Jonas √Ödahl started working on creating an API we could support in the compositor and use Pipewire to output. This is meant to cover both single frame capture like screenshot, to local desktop recording and remoting protocols. It is important to note here that what we have done is not just implement support for a specific protocol like RDP or VNC, but we ensured there is an advaned infrastructure in place to support any protocol on top of. For instance we will be working with the <a href="https://www.spice-space.org/">Spice</a> team here at Red Hat to ensure SPICE can take advantage of Pipewire and the new API for instance. We will also ensure Chrome and Firefox supports this so that you can share your Wayland desktop through systems such as Blue Jeans.</p>
<p><strong>Where we are now</strong><br>
So after multiple years of development we are now landing Pipewire in Fedora Workstation 27. This initial version is video only as that is the most urgent thing we need supported for Flatpaks and Wayland. So audio is completely unaffected by this for now and rolling that out will require quite a bit of work as we do not want to risk breaking audio on your system as a result of this change. We know that for many the original rollout of PulseAudio was painful and we do not want a repeat of that history.</p>
<p>So I strongly recommend grabbing the Fedora Workstation 27 beta to test pipewire and check out the new website at <a href="http://pipewire.org/">Pipewire.org</a> and the initial documentation at the <a href="https://github.com/PipeWire/pipewire/wiki">Pipewire wiki</a>. Especially interesting is probably the pages that will eventually outline our plans for handling <a href="https://github.com/PipeWire/pipewire/wiki/PulseAudio">PulseAudio</a> and <a href="https://github.com/PipeWire/pipewire/wiki/JACK">JACK</a> usecases.</p>
<p>If you are interested in Pipewire please join us on IRC in #pipewire on freenode. Also if things goes as planned Wim will be on <a href="http://www.jupiterbroadcasting.com/show/linuxun/">Linux Unplugged tonight</a> talking to Chris Fisher and the Unplugged crew about Pipewire, so tune in!</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/09/19/launching-pipewire/">by uraeus at September 19, 2017 01:18 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">September 18, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://andrescolubri.net/visualization,/mirador,/updates,/fixes,/tools/2017/09/18/mirador-update">
<h3><a href="http://andrescolubri.net/feed.xml" title="andrescolubri.net">Gustavo Orrillo</a> ‚Äî <a href="http://andrescolubri.net/visualization,/mirador,/updates,/fixes,/tools/2017/09/18/mirador-update.html">Mirador update</a></h3>
<div class="entry">
<div class="content">
<p>Mirador is an open source visualization tool I have been developing at the <a href="https://www.sabetilab.org/" target="_top">Sabeti lab</a>, 
in collaboration with <a href="https://fathom.info/" target="_top">Fathom Information Design</a>. 
The main goal of this tool is to facilitate the initial exploratory analysis of tabular datasets, by providing a graphical interface 
that allows to quickly visualize arbitrary combinations of variable pairs in the data, inspect the effects of confounding factors, and rank 
explanatory variables by the strength of their association with any outcome variable of interest. For more information about this project, 
take a look at its <a href="https://fathom.info/mirador" target="_top">homepage</a>.</p>

<p>During the past year, however, I was not able to work on Mirador as much as I wanted. Fortunately, I recently had a chance to come 
back to it, and a new release, 1.4.2, is finally available for download. Although it does not introduce any changes or improvements to its UI or 
statistics module, it brings some important tweaks and fixes. First of all, Mirador now starts up with a small launch screen providing info about 
the project, a link to the home page, and a couple of buttons, one to choose a dataset to work with, and the other to quit:</p>

<center><img src="mirador-1.4.2-welcome-screen.png" alt="Mirador 1.4.2 launch screen"></center>

<p>This simple launch screen could be expanded later on, with a few additional options (e.g.: a list of recently opened files).</p>

<p>More importantly, the previous Windows version of Mirador was broken, as reported by <a href="https://github.com/mirador/mirador/issues/59" target="_top">some users</a>. 
Release 1.4.2 solves this issue, while also introducing support for HiDPI displays:</p>

<p><img src="mirador-1.4.2-windows.png" alt="Mirador 1.4.2 on Windows"></p>

<p>Another first, this release now includes 32 and 64 bit packages for Linux:</p>

<p><img src="mirador-1.4.2-linux.png" alt="Mirador 1.4.2 on Linux"></p>

<p>All things considered, Mirador does not try to be a a ‚Äúbig data‚Äù or ‚Äúbusiness analytics‚Äù tool, but rather, a platform to experiment with some ideas about visual exploration 
of complex datasets (by complex, I mean containing arbitrary combinations of numerical and categorical data), and about using measures of statistical association to guide 
the data analysis while minimizing biases due to the search process itself. To give it a try, download the latest installation or zip packages for 
<a href="https://github.com/mirador/mirador/releases/tag/latest-windows" target="_top">Windows</a>, <a href="https://github.com/mirador/mirador/releases/tag/latest-macos" target="_top">macOS</a>, or 
<a href="https://github.com/mirador/mirador/releases/tag/latest-linux" target="_top">Linux</a>, or check the <a href="https://github.com/mirador/mirador/wiki" target="_top">Wiki</a> for details on 
how to build Mirador from source.</p></div>

<p class="date">
<a href="http://andrescolubri.net/visualization,/mirador,/updates,/fixes,/tools/2017/09/18/mirador-update.html">September 18, 2017 05:00 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="/news/#2017-09-18T14:30:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> ‚Äî <a href="https://gstreamer.freedesktop.org/news/#2017-09-18T14:30:00Z">GStreamer 1.12.3 stable release</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is pleased to announce the third bugfix release in the
stable 1.12 release series of your favourite cross-platform multimedia framework!
</p><p>
This release only contains bugfixes and it should be safe to update from
1.12.x.
</p><p>
See <a href="https://gstreamer.freedesktop.org/releases/1.12/#1.12.3">/releases/1.12/</a>
for the full release notes.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be available shortly.
</p><p>
Check out the release notes for
<a href="https://gstreamer.freedesktop.org/releases/gstreamer/1.12.3.html">GStreamer core</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-base/1.12.3.html">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-good/1.12.3.html">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-ugly/1.12.3.html">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-plugins-bad/1.12.3.html">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-libav/1.12.3.html">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-rtsp-server/1.12.3.html">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-python/1.12.3.html">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-editing-services/1.12.3.html">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/releases/gst-validate/1.12.3.html">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/releases/gstreamer-vaapi/1.12.3.html">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/releases/gst-omx/1.12.3.html">gst-omx</a>,
or download tarballs for
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.12.3.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.12.3.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.12.3.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.12.3.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.12.3.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.12.3.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.12.3.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.12.3.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.12.3.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.12.3.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.12.3.tar.xz">gstreamer-vaapi</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.12.3.tar.xz">gst-omx</a>.
        </p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2017-09-18T14:30:00Z">September 18, 2017 02:30 PM</a>
</p>
</div>
</div>


</div>

</div>
</div>

<div id="sidebar">

<div id="about">
<p>
Planet GStreamer is powered by <a href="http://www.planetplanet.org/">Planet</a> and hosted by <a href="http://www.freedesktop.org/">freedesktop.org</a>
</p>
<p>
<a href="http://gstreamer.net/">GStreamer Home</a>
</p>
<p>
<a href="http://www.catb.org/hacker-emblem/"><img class="button" src="hacker.png" alt="[Hacker]" width="80" height="15"></a>
<a href="http://www.planetplanet.org/"><img class="button" src="planet.png" alt="[Planet]" width="80" height="15"></a>
</p>
</div>

<div id="freshness">
<p>
<em>Last updated: January 04, 2018 05:18 AM. All times are UTC.</em>
</p>
</div>

<div id="feeds">
<h2>Feeds:</h2>
<p>
Planet GStreamer has aggregated feeds available as
<a href="https://gstreamer.freedesktop.org/planet/atom.xml">Atom 1.0</a>,
<a href="https://gstreamer.freedesktop.org/planet/rss10.xml">RSS 1.0</a>, and
<a href="https://gstreamer.freedesktop.org/planet/rss20.xml">RSS 2.0</a>
and subscription lists in
<a href="https://gstreamer.freedesktop.org/planet/foafroll.xml">FOAF</a> and
<a href="https://gstreamer.freedesktop.org/planet/opml.xml">OPML</a>.
</p>
</div>

<div id="subscriptions">
<h2>Subscriptions:</h2>
<ul>
<li>
<a href="http://abock.org/" class="message" title="no activity in 90 days">Aaron Bockover</a> <a href="https://abock.org/feed" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://genuinepulse.blogspot.com/" class="message" title="http status 401">Andre Dieb Martins</a> <a href="http://genuinepulse.blogspot.com/feeds/posts/default/-/gstreamer" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://wingolog.org/" class="message" title="no activity in 90 days">Andy Wingo</a> <a href="http://wingolog.org/feed/atom/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://arunraghavan.net/" class="message" title="no activity in 90 days">Arun Raghavan</a> <a href="https://arunraghavan.net/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://hadess.net/" title="/b…ës Ààtj…õÃÉ no Ààse  Å…ë/  (hadess) | News">Bastien Nocera</a> <a href="http://www.hadess.net/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/company/" class="message" title="no activity in 90 days">Benjamin Otte</a> <a href="http://www.advogato.org/person/company/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://dotsony.blogspot.com/" class="message" title="no activity in 90 days">Brandon Lewis</a> <a href="http://dotsony.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.oracle.com/yippi/feed/entries/rss" class="message" title="404: not found">Brian Cameron</a> <a href="http://blogs.oracle.com/yippi/feed/entries/rss" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> <a href="https://blogs.gnome.org/uraeus/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://schleef.org/" class="message" title="internal server error">David Schleef</a> <a href="http://schleef.org/blog/feeds/atom/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.gnome.org/portal/edwardrv" class="message" title="no activity in 90 days">Edward Hervey</a> <a href="http://blogs.gnome.org/edwardrv/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://eocanha.org/blog" title="GStreamer ‚Äì Happy coding">Enrique Oca√±a Gonz√°lez</a> <a href="http://eocanha.org/blog/category/gstreamer/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/omega/" class="message" title="no activity in 90 days">Erik Walthinsen</a> <a href="http://www.advogato.org/person/omega/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://felipec.wordpress.com/" class="message" title="410: gone">Felipe Contreras</a> <a href="http://felipec.wordpress.com/category/planet/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://fcarvalho.blogspot.com/" class="message" title="no activity in 90 days">Flavio Oliveira</a> <a href="http://fcarvalho.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.flumotion.net/news/" class="message" title="no activity in 90 days">Flumotion</a> <a href="http://www.flumotion.net/news/rss-1.0.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> <a href="https://gstreamer.freedesktop.org/news/rss-1.0.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://gstreamer.freedesktop.org/" class="message" title="no activity in 90 days">GStreamer Newsletter</a> <a href="https://gstreamer.freedesktop.org/news/status-rss-1.0.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://gkiagia.wordpress.com/" title="GStreamer ‚Äì Gkiagia‚Äôs Blog">George Kiagiadakis</a> <a href="https://gkiagia.wordpress.com/category/gstreamer-2/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.desmottes.be/?feed/en/rss2" class="message" title="no activity in 90 days">Guillaume Desmottes</a> <a href="https://blog.desmottes.be/?feed/en/rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://guij.emont.org/blog/category/geekeries/" class="message" title="no activity in 90 days">Guillaume Emont</a> <a href="http://guij.emont.org/blog/category/geekeries/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://andrescolubri.net/feed.xml" class="message" title="no activity in 90 days">Gustavo Orrillo</a> <a href="http://andrescolubri.net/feed.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://blogs.igalia.com/zzoon/" class="message" title="404: not found">Hyunjun Ko</a> <a href="https://blogs.igalia.com/zzoon/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/peaceandlove/" class="message" title="no activity in 90 days">Iain Holmes</a> <a href="http://www.advogato.org/person/peaceandlove/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://masher.homeip.net/~jan/diary/" class="message" title="no activity in 90 days">Jan Schmidt</a> <a href="http://noraisin.net/diary/?feed=rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://fortintam.com/blog" class="message" title="no activity in 90 days">Jean-Fran√ßois Fortin Tam</a> <a href="http://fortintam.com/blog/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/jdahlin/" class="message" title="no activity in 90 days">Johan Dahlin</a> <a href="http://www.advogato.org/person/jdahlin/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.jokosher.org/" class="message" title="internal server error">Jokosher News</a> <a href="http://www.jokosher.org/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://laszlopandy.com/" class="message" title="no activity in 90 days">Laszlo Pandy</a> <a href="http://laszlopandy.com/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/lmjohns3/" class="message" title="no activity in 90 days">Leif Johnson</a> <a href="http://www.advogato.org/person/lmjohns3/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://oxcsnicho-stamp.blogspot.com/" class="message" title="no activity in 90 days">Lin YANG</a> <a href="http://oxcsnicho-stamp.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://mathieuduponchelle.blogspot.com/" class="message" title="no activity in 90 days">Mathieu Duponchelle</a> <a href="http://mathieuduponchelle.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.mikeasoft.com/" title="Michael Sheldon's Stuff">Michael Sheldon</a> <a href="http://blog.mikeasoft.com/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://mikesmith.wordpress.com/" class="message" title="no activity in 90 days">Michael Smith</a> <a href="https://mikesmith.wordpress.com/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://macslow.thepimp.net/" class="message" title="internal server error">Mirco M√ºller</a> <a href="http://macslow.thepimp.net/?feed=rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://ndufresne.ca/" class="message" title="no activity in 90 days">Nicolas Dufresne</a> <a href="http://ndufresne.ca/category/planet/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.nirbheek.in/" class="message" title="no activity in 90 days">Nirbheek Chauhan</a> <a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.tester.ca/" class="message" title="no activity in 90 days">Olivier Crete</a> <a href="http://ocrete.ca/category/english/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://base-art.net/" class="message" title="no activity in 90 days">Phil Normand</a> <a href="http://base-art.net/Articles/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/phkhal/" class="message" title="no activity in 90 days">Philippe Khalaf</a> <a href="http://www.advogato.org/person/phkhal/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/rillian/" class="message" title="no activity in 90 days">Ralph Giles</a> <a href="http://www.advogato.org/person/rillian/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://heisenbugs.blogspot.com/" class="message" title="no activity in 90 days">Reynaldo Verdejo</a> <a href="http://heisenbugs.blogspot.com/feeds/posts/default/-/FreeDesktop" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://richard-spiers.blogspot.com/" class="message" title="no activity in 90 days">Richard Spiers</a> <a href="http://richard-spiers.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://ramcq.net/" class="message" title="no activity in 90 days">Robert McQueen</a> <a href="http://robot101.net/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://gstmediaservices.blogspot.com/" class="message" title="no activity in 90 days">Roberto Fag√°</a> <a href="http://gstmediaservices.blogspot.com/atom.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://relekandcode.blogspot.com/" class="message" title="no activity in 90 days">Roland Elek</a> <a href="http://relekandcode.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://coaxion.net/blog" title="coaxion.net ‚Äì slomo's blog">Sebastian Dr√∂ge</a> <a href="https://coaxion.net/blog/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://k-d-w.org/" title="Sebastian P√∂lsterl's blog">Sebastian P√∂lsterl</a> <a href="https://k-d-w.org/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://sjoerd.luon.net/tags/gnome/" class="message" title="no activity in 90 days">Sjoerd Simons</a> <a href="http://sjoerd.luon.net/tags/gnome/index.atom" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/ensonic/" class="message" title="no activity in 90 days">Stefan Kost</a> <a href="http://www.advogato.org/person/ensonic/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.thiagoss.com/" class="message" title="no activity in 90 days">Thiago Santos</a> <a href="http://blog.thiagoss.com/tag/gstreamer/rss/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.gnome.org/tsaunier" class="message" title="no activity in 90 days">Thibault Saunier</a> <a href="http://blogs.gnome.org/tsaunier/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://thomas.apestaart.org/log" class="message" title="no activity in 90 days">Thomas Vander Stichele</a> <a href="http://thomas.apestaart.org/log/?feed=rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://blogs.igalia.com/vjaquez" title="gstreamer ‚Äì Herostratus‚Äô legacy">V√≠ctor J√°quez</a> <a href="https://blogs.igalia.com/vjaquez/tag/gstreamer/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://blogs.igalia.com/xrcalvar" class="message" title="no activity in 90 days">Xabier Rodr√≠guez Calvar</a> <a href="https://blogs.igalia.com/xrcalvar/category/planets/planet-gstreamer/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://zaheer.merali.org/" class="message" title="no activity in 90 days">Zaheer Abbas Merali</a> <a href="http://zaheer.merali.org/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/zeenix/" class="message" title="no activity in 90 days">Zeeshan Ali</a> <a href="http://www.advogato.org/person/zeenix/rss.xml" title="subscribe">(feed)</a>
</li>
</ul>
</div>

<div id="planetarium">
<h2>Planetarium:</h2>
<ul>
<li><a href="http://www.planetapache.org/">Planet Apache</a></li>
<li><a href="http://classpath.wildebeest.org/planet/">Planet Classpath</a></li>
<li><a href="http://planet.debian.net/">Planet Debian</a></li>
<li><a href="http://planet.debian.org.hk/">Planet Debian HK</a></li>
<li><a href="http://planet.freedesktop.org/">Planet freedesktop.org</a></li>
<li><a href="http://planet.gnome.org/">Planet GNOME</a></li>
<li><a href="http://gnome.or.kr/pgk/">Planet GNOME Korea</a></li>
<li><a href="http://planetjava.org/">Planet Java.org</a></li>
<li><a href="http://planet.perl.org/">Planet Perl</a></li>
<li><a href="http://www.planetsuse.org/">Planet SuSE</a></li>
<li><a href="http://planet.twistedmatrix.com/">Planet Twisted</a></li>
<li><a href="http://planet.arslinux.com/">Planet Ars Linux</a></li>
<li><a href="http://www.planetkde.org/">Planet KDE</a></li>
<li><a href="http://fossplanet.osdir.com/">FOSS Planet</a></li>
<li><a href="http://live.linuxchix.org/">LinuxChix Live</a></li>
<li><a href="http://kerneltrap.org/hackers/linux">Linux @ KernelTrap</a></li>
<li><a href="http://www.go-mono.com/monologue/">Mono</a></li>
<li><a href="http://www.planet-php.net/">PHP</a></li>
<li><a href="http://planetrdf.com/">RDF</a></li>
<li><a href="http://xfce.org/blog/">XFCE</a></li>
<li><a href="http://advogato.org/recentlog.html?thresh=4">Advogato</a></li>
</ul>
</div>

</div>




</body>
</html>
