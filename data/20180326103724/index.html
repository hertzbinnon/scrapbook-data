<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

<title>Planet GStreamer</title>

<meta name="generator" content="Planet/2.0 +http://www.planetplanet.org">

<link rel="shortcut icon" href="favicon.png">
<link rel="alternate" href="http://gstreamer.freedesktop.org/planet/atom.xml" title="" type="application/atom+xml">

<link media="all" href="index.css" type="text/css" rel="stylesheet">
</head>
<body>

<div id="banner">
<h1>Planet GStreamer</h1>
</div>

<div id="entries">
<div class="daygroup">
<h2 class="date">March 24, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="tag:blogger.com,1999:blog-701969077517001201.post-3703786293135563494">
<h3><a href="http://blog.nirbheek.in/" title="Nirbheek’s Rantings">Nirbheek Chauhan</a> — <a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html">Latency in Digital Audio</a></h3>
<div class="entry">
<div class="content">
<div dir="ltr">We've come a long way since <a href="https://en.wikipedia.org/wiki/Invention_of_the_telephone" target="_top">Alexander Graham Bell</a>, and everything's turned digital.<br><br>Compared to analog audio, <a href="https://en.wikipedia.org/wiki/Digital_signal_processing" target="_top">digital audio processing </a>is extremely versatile, is much easier to design and implement than  analog processing, and also adds effectively zero noise along the way. With rising computing power and dropping costs, every operating system has had drivers, engines, and libraries to record, process, playback, transmit, and store audio for over 20 years.<br><br><div>Today we'll talk about the some of the differences between analog and digital audio, and how the widespread use of digital audio adds a new challenge: <i>latency</i>.</div><br><h2>Analog vs Digital</h2><div><br></div><div><b>Analog data</b> flows like water through an empty pipe. You open the tap, and the time it takes for the first drop of water to reach you is the latency. When analog audio is transmitted through, say, an <a href="https://en.wikipedia.org/wiki/RCA_connector" target="_top">RCA cable</a>, the transmission happens at the speed of electricity and your latency is:<code></code><br><br><div><img alt="wire length/speed of electricity" src="analog-latency.svg"></div><br>This number is ridiculously small<span class="st">—</span>especially when compared to the speed of sound. An electrical signal takes 0.001 milliseconds to travel 300 metres (984 feet). Sound takes 874 milliseconds (almost a second).<br><br>All analog effects and filters obey similar equations. If you're using, say, an analog pedal with an electric guitar, the signal is transformed continuously by  an electrical circuit, so the latency is a function of the wire length (plus capacitors/transistors/etc), and is almost always negligible.<br><br><b>Digital audio</b> is transmitted in "packets" (buffers) of a particular size, like a <a href="https://en.wikipedia.org/wiki/Bucket_brigade" target="_top">bucket brigade</a>, but at the speed of electricity. Since the real world is analog, this means to record audio, you must use an <a href="https://en.wikipedia.org/wiki/Analog-to-digital_converter" target="_top">Analog-Digital Converter</a>. The <abbr title="Analog-Digital Converter">ADC</abbr> <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)" target="_top">quantizes</a> <a href="https://wiki.xiph.org/Videos/A_Digital_Media_Primer_For_Geeks" target="_top">the signal</a> into digital measurements (samples), packs multiple samples into  a buffer, and sends it forward. This means your latency is now: </div><br><div><img alt="(wire length/speed of electricity) + buffer size" src="digital-latency.svg"></div><div><br>We saw above that the first part is insignificant, what about the second part?<br><br>Latency is measured in time, but buffer size is measured in bytes. For <a href="https://en.wikipedia.org/wiki/Audio_bit_depth" target="_top">16-bit integer audio</a>, each measurement (sample) is stored as a 16-bit integer, which is 2 bytes. That's the theoretical lower limit on the buffer size. The <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Sampling_rate" target="_top">sample rate</a> defines how often measurements are made, and these days, is usually 48KHz. This means each sample contains 0.021ms of audio. To go lower, you need to increase the sample rate to 96KHz or 192KHz.<br><br>However, when general-purpose computers are involved, the buffer size is almost never lower than 32 bytes, and is usually 128 bytes or larger. For 16-bit integer audio at 48KHz, a 32 byte buffer is 0.67ms, and a 128 byte buffer is 2.67ms. This is our buffer size and hence the base latency while recording (or playing) digital audio.<br><br>Digital effects operate on individual buffers, and will add an additional amount of latency depending on the delay added by the CPU processing required by the effect. Such effects may also add latency if the algorithm used requires that, but that's the same with analog effects.<br><br><h2>The Digital Age</h2><br>So everyone's using digital. But isn't 2.67ms a lot of additional latency?<br><br>It might seem that way till you think about it in real-world terms. Sound travels less than a meter (3 feet) in that time, and that sort of delay is completely unnoticeable by humans<span class="st">—</span>otherwise we'd notice people's lips moving before we heard their words.<br><br>In fact, 2.67ms is too small for the majority of audio applications!<br><br>To process such small buffer sizes, you'd have to wake the CPU up <abbr title="1000 / 2.67">375 times a second</abbr>, just for audio. This is highly inefficient, and wastes a lot of power. You really don't want that on your phone or your laptop, and is completely unnecessary in most cases anyway. <br><br>For instance, your music player will usually use a buffer size of ~200ms, which is just <i>5</i> CPU wakeups per second. Note that this doesn't mean that you will hear sound 200ms after hitting "play". The audio player will just send 200ms of audio to the sound card at once, and playback will begin immediately.<br><br>Of course, you can't do that with live playback such as video calls<span class="st">—y</span>ou can't "read-ahead" data you don't have. You'd have to invent a time machine first. As a result, apps that use real-time communication have to use smaller buffer sizes because that directly affects the latency of live playback.<br><br>That brings us back to efficiency. These apps also need to conserve power, and 2.67ms buffers are really wasteful. Most consumer apps that require low latency use 10-15ms buffers, and that's good enough for things like voice/video calling, video games, notification sounds, and so on.<br><br><h2>Ultra Low Latency</h2><br>There's one category left: musicians, sound engineers, and other folk that work in the pro-audio business. For them, 10ms of latency is much too high!<br><br>You usually can't notice a 10ms delay between an event and the sound for it, but when making music, you <i>can</i> hear it when two instruments are out-of-sync by 10ms or if the sound for an instrument you're playing is delayed. Instruments such as drum snare are more susceptible to this problem than others, which is why the <a href="https://en.wikipedia.org/wiki/Stage_monitor_system" target="_top">stage monitors</a> used in live concerts must not add any latency.<br><br>The standard in the music business is to use buffers that are 5ms or lower, down to the 0.67ms number that we talked about  above.<br><br>Power consumption is absolutely no concern, and the real problems are the accumulation of small amounts of latencies everywhere in your stack, and ensuring that you're able to read buffers from the hardware or write buffers to the hardware fast enough.<br><br>Let's say you're using an app on your computer to apply digital effects to a guitar that you're playing. This involves capturing audio from the line-in port, sending it to the application for processing, and playing it from the sound card to your amp.<br><br>The latency while capturing and outputting audio are both multiples of the buffer size, so it adds up very quickly. The effects app itself will also add a variable amount of latency, and at 2.67ms buffer sizes you will find yourself quickly approaching a 10ms latency from line-in to amp-out. The only way to lower this is to use a smaller buffer size, which is precisely what pro-audio hardware and software enables.<br><br>The second problem is that of CPU scheduling. You need to ensure that the threads that are fetching/sending audio data to the hardware and processing the audio have the highest priority, so that nothing else will steal CPU-time away from them and cause glitching due to buffers arriving late.<br><br>This gets harder as you lower the buffer size because the audio stack has to do more work for each bit of audio. The fact that we're doing this on a general-purpose operating system makes it even harder, and requires implementing <a href="https://en.wikipedia.org/wiki/Real-time_computing" target="_top">real-time scheduling</a> features across several layers. But that's a story for another time!<br><br>I hope you found this dive into digital audio interesting! My next post <span>will be</span> is about my journey in <a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html">implementing ultra low latency capture and render on Windows</a> in the <a href="https://msdn.microsoft.com/library/windows/desktop/dd371455.aspx" target="_top">WASAPI</a> plugin for <a href="https://en.wikipedia.org/wiki/GStreamer" target="_top">GStreamer</a>. This was already possible on Linux with the JACK GStreamer plugin and on macOS with the CoreAudio GStreamer plugin, so it will be interesting to see how the same problems are solved on Windows. Tune in!</div></div></div>

<p class="date">
<a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html">by Nirbheek (noreply@blogger.com) at March 24, 2018 01:18 PM</a>
</p>
</div>
</div>




<div class="entrygroup" id="tag:blogger.com,1999:blog-701969077517001201.post-1566574286640095569">
<h3><a href="http://blog.nirbheek.in/" title="Nirbheek’s Rantings">Nirbheek Chauhan</a> — <a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html">Low-latency audio on Windows with GStreamer</a></h3>
<div class="entry">
<div class="content">
<div dir="ltr">Digital audio is so ubiquitous that we rarely stop to think or wonder  how the gears turn underneath our all-pervasive apps for entertainment. Today we'll look at one specific piece of the machinery: latency.<br><br>Let's say you're making a video of someone's birthday party with an app on  your phone. Once the recording starts, you don't care when the app starts writing it to disk<span class="st">—</span>as long as everything is there in the end.<br><br>However, if you're having a Skype call with your friend, it matters a <i>whole lot</i>  how long it takes for the video to reach the other end and vice versa.  It's impossible to have a conversation if the lag (latency) is too  high.<br><br>The difference is, do you need real-time feedback or not?<br><br>Other examples, in order of increasingly stricter latency requirements are: live video streaming, security cameras, augmented reality games such as <a href="https://en.wikipedia.org/wiki/Pok%C3%A9mon_Go" target="_top">Pokémon Go</a>, multiplayer video games in general, audio effects apps for live music recording, and many many more.<br><br>“But Nirbheek”, you might ask, “why doesn't everyone always ‘immediately’ send/store/show whatever is recorded? Why do people have to worry about latency?” and that's a great question!<br><br>To understand that, checkout my previous blog post, <a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html" target="_top">Latency in Digital Audio</a>. It's also a good primer on analog vs digital audio!<br><br><h2>Low latency on consumer operating systems</h2><div><br></div><div>Each operating system has its own set of application APIs for audio, and each has a lower bind on the achievable latency:</div><div><br></div><ul><li>Linux has <a href="https://www.alsa-project.org/main/index.php/ALSA_Library_API" target="_top">alsa-lib</a> (old), <a href="https://en.wikipedia.org/wiki/Pulseaudio" target="_top">Pulseaudio</a> (standard), <a href="https://en.wikipedia.org/wiki/JACK_Audio_Connection_Kit" target="_top">JACK</a> (pro-audio), and <a href="https://pipewire.org/" target="_top">Pipewire</a> (<a href="https://blogs.gnome.org/uraeus/2018/01/26/an-update-on-pipewire-the-multimedia-revolution-an-update/" target="_top">under development</a>)</li><li>macOS and iOS have <a href="https://en.wikipedia.org/wiki/Core_Audio" target="_top">CoreAudio</a> (standard, pro-audio)</li><li>Android has <a href="https://source.android.com/devices/audio/" target="_top">AudioFlinger</a> (Java API, android.media), <a href="https://en.wikipedia.org/wiki/OpenSL_ES" target="_top">OpenSL ES</a> (C/C++ API), and <a href="https://source.android.com/devices/audio/aaudio" target="_top">AAudio</a> (C/C++ API, new, pro-audio)</li><li>Windows has <a href="https://en.wikipedia.org/wiki/Directsound" target="_top">DirectSound</a> (deprecated), <a href="https://en.wikipedia.org/wiki/Technical_features_new_to_Windows_Vista#Audio_stack_architecture" target="_top">WASAPI</a> (standard), and <a href="https://en.wikipedia.org/wiki/Audio_Stream_Input/Output" target="_top">ASIO</a> (proprietary, old, pro-audio).</li><li>BSDs still use <a href="https://en.wikipedia.org/wiki/Open_Sound_System">OSS</a></li></ul><div><br></div><div>GStreamer already has plugins for almost all of these<a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#gst-plugins">¹</a> (plus others that aren't listed here), and on Windows, GStreamer has been using the DirectSound API by default for  audio capture and output since the very beginning.<br><br>However, the DirectSound API was deprecated in Windows XP, and with Vista, it was removed and replaced with an emulation layer on top of the newly-released WASAPI. As a result, the plugin can't be configured to have less than 200ms of latency, which makes it unsuitable for all the low-latency use-cases mentioned above. The DirectSound API is quite crufty and unnecessarily complex anyway.<br><br>GStreamer is rarely used in video games, but it is widely used for live streaming, audio/video calls, and other real-time applications. Worse, the WASAPI GStreamer plugins were effectively untouched and unused since the initial implementation in 2008 and were completely broken<a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#gst-windows">²</a>.<br><br>This left no way to achieve low-latency audio capture or playback on Windows using GStreamer.<br><br>The situation became particularly dire when GStreamer added a new <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html">implementation of the WebRTC spec</a> in this <a href="https://gstreamer.freedesktop.org/releases/1.14/">release cycle</a>. People that try it out on Windows were going to see much higher latencies than they should.<br><br>Luckily, I rewrote most of the WASAPI plugin code in January and February, and it should now work well on all versions of Windows from Vista to 10! You can get <a href="https://gstreamer.freedesktop.org/data/pkg/windows/1.14.0.1/">binary installers for GStreamer</a> or <a href="https://gstreamer.freedesktop.org/documentation/installing/building-from-source-using-cerbero.html">build it from source</a>.<br><br><h2>Shared and Exclusive WASAPI</h2><br>WASAPI allows applications to open sound devices in two modes: <i>shared</i> and <i>exclusive</i>. As the name suggests, <i>shared</i> mode allows multiple applications to output to (or capture from) an audio device at the same time, whereas <i>exclusive</i> mode does not.<br><br>Almost all applications should open audio devices in shared mode. It would be quite disastrous if your YouTube videos played without sound because Spotify decided to open your speakers in exclusive mode.<br><br>In shared mode, the audio engine has to resample and mix audio streams from all the applications that want to output to that device. This increases latency because it must maintain its own audio ringbuffer for doing all this, from which audio buffers will be periodically written out to the audio device.<br><br>In theory, hardware mixing could be used if the sound card supports it, but very few sound cards implement that now since it's so cheap to do in software. On Windows, only high-end audio interfaces used for professional audio implement this.<br><br>Another option is to allocate your audio engine buffers directly in the sound card's memory with DMA, but that complicates the implementation and relies on good drivers from hardware manufacturers. Microsoft has tried similar approaches in the past with DirectSound and been burned by it, so it's not a route they took with WASAPI<a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#ms-audio-history">³</a>.<br><br>On the other hand, some applications know they will be the only ones using a device, and for them all this machinery is a hindrance. This is why <i>exclusive</i> mode exists. In this mode, if the audio driver is implemented correctly, the application's buffers will be directly written out to the sound card, which will yield the lowest possible latency.<br><br><h2>Audio latency with WASAPI</h2><br>So what kind of latencies <i>can</i> we get with WASAPI?<br><br>That depends on the <i>device period</i> that is being used. The term <i>device period</i> is a fancy way of saying <i>buffer size</i>; specifically the buffer size that is used in each call to your application that fetches audio data.<br><br>This is the same period with which audio data will be written out to the actual device, so it is the major contributor of latency in the entire machinery.<i></i><br><br>If you're using the <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/dd370865">AudioClient</a> interface in WASAPI to initialize your streams, the default period is 10ms. This means the theoretical <i>minimum</i> latency you can get in <i>shared mode</i> would be 10ms (audio engine) + 10ms (driver) = 20ms. In practice, it'll be somewhat higher due to various inefficiencies in the subsystem.<br><br>When using <i>exclusive mode</i>, there's no engine latency, so the same number goes down to ~10ms.<br><br>These numbers are decent for most use-cases, but like I explained in my <a href="http://blog.nirbheek.in/2018/03/latency-in-digital-audio.html">previous blog post</a>, this is totally insufficient for pro-audio use-cases such as applying live effects to music recordings. You really need latencies that are lower than 10ms there.<br><br><h2>Ultra-low latency with WASAPI</h2><br>Starting with Windows 10, WASAPI removed most of its  aforementioned inefficiencies, and introduced a new interface: <a href="https://msdn.microsoft.com/library/windows/desktop/dn911487">AudioClient3</a>. If you initialize your streams with this interface, and if your audio driver is implemented correctly, you can configure a device period of just <i>2.67ms</i> at 48KHz.<br><br>The best part is that this is the period not just in exclusive mode but <i>also in shared mode</i>, which brings WASAPI almost at-par with JACK and CoreAudio  <br><br>So that was the good news. Did I mention there's bad news too? Well, now you know.<br><br>The first bit is that these numbers are only achievable if you use Microsoft's implementation of the Intel HD Audio standard for consumer drivers. This is fine; you follow <a href="https://blogs.msdn.microsoft.com/matthew_van_eerde/2010/08/23/troubleshooting-how-to-install-the-microsoft-hd-audio-class-driver/">some badly-documented steps</a> and it turns out fine.<br><br>Then you realize that if you want to use something more high-end than an Intel HD Audio sound card, unless you use <a href="http://www.motu.com/newsitems/windows-wave-rt-support-is-now-shipping">one of the rare</a> pro-audio interfaces that have drivers that use the new <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/audio/understanding-the-wavert-port-driver">WaveRT</a> driver model instead of the old <a href="https://msdn.microsoft.com/en-us/library/windows/hardware/ff538767">WaveCyclic</a> model, you still see 10ms device periods.<br><br>It seems the pro-audio industry made the decision to stick with ASIO since it already provides &lt;5ms latency. They don't care that the API is proprietary, and that most applications can't actually use it because of that. All the apps that are used in the pro-audio world already work with it.<br><br>The strange part is that all this information is nowhere on the Internet and seems to lie solely in the minds of the Windows audio driver cabals across the US and Europe. It's surprising and frustrating for someone used to working in the open to see such counterproductive information asymmetry, and <a href="https://github.com/kinetiknz/cubeb/issues/324">I'm not the only one</a>.<br><br>This is where I plug open-source and talk about how Linux has had ultra-low latencies for years since all the audio drivers are open-source, follow the same <a href="https://www.kernel.org/doc/html/v4.10/sound/kernel-api/index.html">ALSA driver model</a><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#alsa-kernel">⁴</a>, and are constantly improved. JACK is probably the most well-known low-latency audio engine in existence, and was born on Linux. People are even using Pulseaudio these days to work with &lt;5ms latencies.<br><br>But this blog post is about Windows and WASAPI, so let's get back on track.<br><br>To be fair, Microsoft is not to blame here. Decades ago they made the decision of not working more closely with the companies that write drivers for their standard hardware components, and they're still paying the price for it. Blue screens of death were the most user-visible consequences, but the current audio situation is an indication that losing control of your platform has more dire consequences.<br><br>There is one more bit of bad news. In my testing, I wasn't able to get glitch-free <i>capture</i> of audio in the source element using the AudioClient3 interface at the minimum configurable latency in shared mode, even with <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/sys/wasapi/gstwasapiutil.c#n980">critical thread priorities</a> unless there was nothing else running on the machine.<br><br>As a result, this feature is disabled by default on the source element. This is unfortunate, but not a great loss since the same device period is achievable in exclusive mode without glitches.<br><br><h2>Measuring WASAPI latencies</h2><br>Now that we're back from our detour, the executive summary is that the GStreamer WASAPI <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-wasapisrc.html#gst-plugins-bad-plugins-wasapisrc.description">source</a> and <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-wasapisink.html#gst-plugins-bad-plugins-wasapisink.description">sink</a> elements now use the latest recommended WASAPI interfaces. You should test them out and see how well they work for you!<br><br>By default, a device is opened in shared mode with a conservative latency setting. To force the stream into the lowest latency possible, set <i>low-latency=true</i>. If you're on Windows 10 and want to force-enable/disable the use of the AudioClient3 interface, toggle the <i>use-audioclient3</i> property.<br><br>To open a device in exclusive mode, set <i>exclusive=true</i>. This will ignore the <i>low-latency</i> and <i>use-audioclient3</i> properties since they only apply to shared mode streams. When a device is opened in exclusive mode, the stream will always be configured for the lowest possible latency by WASAPI.<br><br>To measure the actual latency in each configuration, you can use the new <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad/html/gst-plugins-bad-plugins-audiolatency.html#gst-plugins-bad-plugins-audiolatency.description">audiolatency</a> plugin that I wrote to get hard numbers for the total end-to-end latency including the latency added by the GStreamer audio ringbuffers in the source and sink elements, the WASAPI audio engine (capture and render), the audio driver, and so on.<br><br>I look forward to hearing what your numbers are on Windows 7, 8.1, and 10 in all these configurations! ;)<br><br><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="gst-plugins"></a><br><span>1. The only ones missing are AAudio because it's very new and ASIO which is a proprietary API with licensing requirements.</span><br><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="gst-windows"></a><br><span>2. It's no secret that although lots of people use GStreamer on Windows, the majority of GStreamer developers work on Linux and macOS. As a result the Windows plugins haven't always gotten a lot of love. It doesn't help that <a href="https://gstreamer.freedesktop.org/documentation/installing/building-from-source-using-cerbero.html">building GStreamer on Windows</a> can be a daunting task . This is actually one of the major reasons why we're moving to Meson, but I've already <a href="http://blog.nirbheek.in/2016/05/gstreamer-and-meson-new-hope.html">written about that elsewhere</a>!</span><br><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="ms-audio-history"></a><br><span>3. My knowledge about the history of the decisions behind the Windows Audio API is spotty, so corrections and expansions on this are most welcome!</span><br><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" name="alsa-kernel"></a><br><span>4. The ALSA drivers in the Linux kernel should not be confused with the ALSA userspace library.</span></div></div></div>

<p class="date">
<a href="http://blog.nirbheek.in/2018/03/low-latency-audio-on-windows-with.html">by Nirbheek (noreply@blogger.com) at March 24, 2018 06:09 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">March 20, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=586" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net – slomo's blog">Sebastian Dröge</a> — <a href="https://coaxion.net/blog/2018/03/gstreamer-rust-bindings-0-11-plugin-writing-infrastructure-0-2-release/">GStreamer Rust bindings 0.11 / plugin writing infrastructure 0.2 release</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dröge)" width="80" height="80">
<p>Following the <a href="https://gstreamer.freedesktop.org/releases/1.14/" rel="noopener" target="_top">GStreamer 1.14 release</a> and the new <a href="http://gtk-rs.org/blog/2018/03/17/new-release.html" rel="noopener" target="_top">round of gtk-rs releases</a>, there are also new releases for the <a href="https://github.com/sdroege/gstreamer-rs" rel="noopener" target="_top">GStreamer Rust bindings</a> (0.11) and the <a href="https://github.com/sdroege/gst-plugin-rs/" rel="noopener" target="_top">plugin writing infrastructure</a> (0.2).</p>
<p>Thanks also to all the contributors for making these releases happen and adding lots of valuable changes and API additions.</p>
<h4>GStreamer Rust Bindings</h4>
<p>The main changes in the Rust bindings were the update to GStreamer 1.14 (which brings in quite some new API, like <i>GstPromise</i>), a couple of API additions (<i>GstBufferPool</i> specifically) and the addition of the <i>GstRtspServer</i> and <i>GstPbutils</i> crates. The former allows writing a full <a href="https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol" rel="noopener" target="_top">RTSP</a> server in a <a href="https://github.com/sdroege/gstreamer-rs/blob/0.11/examples/src/bin/rtsp-server.rs" rel="noopener" target="_top">couple of lines of code</a> (with lots of potential for customizations), the latter provides access to the <i>GstDiscoverer</i> helper object that allows inspecting files and streams for their container format, codecs, tags and all kinds of other metadata.</p>
<p>The <i>GstPbutils</i> crate will also get other features added in the near future, like <a href="https://github.com/sdroege/gstreamer-rs/pull/96" rel="noopener" target="_top">encoding profile bindings</a> to allow using the <i>encodebin</i> GStreamer element (a helper element for automatically selecting/configuring encoders and muxers) from Rust.</p>
<p>But the biggest changes in my opinion is some refactoring that was done to the <i>Event</i>, <i>Message</i> and <i>Query</i> APIs. Previously you would have to use a view on a newly created query to be able to use the type-specific functions on it</p>
<p></p><pre class="crayon-plain-tag">let mut q = gst::Query::new_position(gst::Format::Time);
if pipeline.query(q.get_mut().unwrap()) {
    match q.view() {
        QueryView::Position(ref p) =&gt; Some(p.get_result()),
        _ =&gt; None,
    }
} else {
    None
}</pre><p> </p>
<p>Now you can directly use the type-specific functions on a newly created query</p><pre class="crayon-plain-tag">let mut q = gst::Query::new_position(gst::Format::Time);
if pipeline.query(&amp;mut q) {
    Some(q.get_result())
} else {
    None
}</pre><p> </p>
<p>In addition, the views can now dereference directly to the event/message/query itself and provide access to their API, which simplifies some code even more.</p>
<h4>Plugin Writing Infrastructure</h4>
<p>While the plugin writing infrastructure did not see that many changes apart from a couple of bugfixes and updating to the new versions of everything else, this does not mean that development on it stalled. Quite the opposite. The existing code works very well already and there was just no need for adding anything new for the projects I and others did on top of it, most of the required API additions were in the GStreamer bindings.</p>
<p>So the status here is the same as last time, get started writing GStreamer plugins in Rust. It works well!</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2018/03/gstreamer-rust-bindings-0-11-plugin-writing-infrastructure-0-2-release/">by slomo at March 20, 2018 11:42 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">March 19, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2018-03-19T20:00:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> — <a href="https://gstreamer.freedesktop.org/news/#2018-03-19T20:00:00Z">GStreamer 1.14.0 new major stable release</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is proud to announce a new major feature release of
your favourite cross-platform multimedia framework!
</p><p>
The 1.14 release series adds new features on top of the previous 1.12 series
and is part of the API and ABI-stable 1.x release series of the GStreamer
multimedia framework.
</p><p>
  <b>Highlights:</b>
  </p><ul>
  <li>WebRTC support: real-time audio/video streaming to and from web browsers</li>
  <li>Experimental support for the next-gen royalty-free AV1 video codec</li>
  <li>Video4Linux: encoding support, stable element names and faster device probing</li>
  <li>Support for the Secure Reliable Transport (SRT) video streaming protocol</li>
  <li>RTP Forward Error Correction (FEC) support (ULPFEC)</li>
  <li>RTSP 2.0 support in <tt>rtspsrc</tt> and gst-rtsp-server</li>
  <li>ONVIF audio backchannel support in gst-rtsp-server and <tt>rtspsrc</tt></li>
  <li><tt>playbin3</tt> gapless playback and pre-buffering support</li>
  <li><tt>tee</tt>, our stream splitter/duplication element, now does allocation query
     aggregation which is important for efficient data handling and zero-copy</li>
  <li>QuickTime muxer has a new prefill recording mode that allows file import
      in Adobe Premiere and FinalCut Pro while the file is still being written.</li>
  <li><tt>rtpjitterbuffer</tt> fast-start mode and timestamp offset adjustment smoothing</li>
  <li><tt>souphttpsrc</tt> connection sharing, which allows for connection reuse, cookie sharing, etc.</li>
  <li><tt>nvdec</tt>: new plugin for hardware-accelerated video decoding using the NVIDIA NVDEC API</li>
  <li>Adaptive DASH trick play support</li>
  <li><tt>ipcpipeline</tt>: new plugin that allows splitting a pipeline across
      multiple processes</li>
  <li>Major <tt>gobject-introspection</tt> annotation improvements for large parts of the library API</li>
  <li>GStreamer C# bindings have been revived and seen many updates and fixes</li>
  <li>The externally-maintained GStreamer Rust bindings have many
      usability improvements and cover most of the API now</li>
  </ul>
<p></p><p>
Full release notes can be found <a href="https://gstreamer.freedesktop.org/releases/1.14/">here</a>.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided in the next days.
</p><p>
You can download release tarballs directly here: 
<a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.14.0.tar.xz">gstreamer</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.14.0.tar.xz">gst-plugins-base</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.14.0.tar.xz">gst-plugins-good</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.14.0.tar.xz">gst-plugins-ugly</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.14.0.tar.xz">gst-plugins-bad</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.14.0.tar.xz">gst-libav</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.14.0.tar.xz">gst-rtsp-server</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.14.0.tar.xz">gst-python</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.14.0.tar.xz">gst-editing-services</a>,
<a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.14.0.tar.xz">gst-validate</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.14.0.tar.xz">gstreamer-vaapi</a>,
<a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.14.0.tar.xz">gstreamer-sharp</a>, or
<a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.14.0.tar.xz">gst-omx</a>.
        </p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2018-03-19T20:00:00Z">March 19, 2018 08:00 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="tag:base-art.net,2018-03-19:/Articles/gstreamers-playbin3-overview-for-application-developers/">
<h3><a href="http://base-art.net/" title="Base-Art - Philippe Normand">Phil Normand</a> — <a href="https://base-art.net/Articles/gstreamers-playbin3-overview-for-application-developers/">GStreamer’s playbin3 overview for application&nbsp;developers</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="philn.png" alt="(Phil Normand)" width="85" height="115">
<p>Multimedia applications based on GStreamer usually handle playback with the
<a class="reference external" href="https://gstreamer.freedesktop.org/documentation/tutorials/playback/playbin-usage.html">playbin</a> element. I recently added support for <a class="reference external" href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-plugins/html/gst-plugins-base-plugins-playbin3.html">playbin3</a> in WebKit. This post
aims to document the changes needed on application side to support this new
generation flavour of&nbsp;playbin.</p>
<p>So, first of, why is it named playbin3 anyway? The GStreamer …</p></div>

<p class="date">
<a href="https://base-art.net/Articles/gstreamers-playbin3-overview-for-application-developers/">by Philippe Normand at March 19, 2018 07:13 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">March 18, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="tag:base-art.net,2018-03-18:/Articles/moving-to-pelican/">
<h3><a href="http://base-art.net/" title="Base-Art - Philippe Normand">Phil Normand</a> — <a href="https://base-art.net/Articles/moving-to-pelican/">Moving to&nbsp;Pelican</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="philn.png" alt="(Phil Normand)" width="85" height="115">
<p>Time for a change! Almost 10 years ago I was starting to hack on a
Blog engine with two friends, it was called <a class="reference external" href="http://github.com/philn/alinea">Alinea</a> and it powered
this website for a long time. Back then hacking on your own Blog
engine was the pre-requirement to host your blog :) But nowadays …</p></div>

<p class="date">
<a href="https://base-art.net/Articles/moving-to-pelican/">by Philippe Normand at March 18, 2018 09:18 AM</a>
</p>
</div>
</div>




<div class="entrygroup" id="tag:base-art.net,2018-03-18:/Articles/the-gnome-shell-gajim-extension-maintenance/">
<h3><a href="http://base-art.net/" title="Base-Art - Philippe Normand">Phil Normand</a> — <a href="https://base-art.net/Articles/the-gnome-shell-gajim-extension-maintenance/">The GNOME-Shell Gajim extension&nbsp;maintenance</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="philn.png" alt="(Phil Normand)" width="85" height="115">
<p>Back in January 2011 I wrote a <span class="caps">GNOME</span>-Shell <a class="reference external" href="https://extensions.gnome.org/extension/565/gajim-im-integration/">extension</a> allowing Gajim users to
carry on with their chats using the Empathy infrastructure and <span class="caps">UI</span> present in the
Shell. For some time the extension was also part of the official
gnome-shell-extensions module and then I had to move it to …</p></div>

<p class="date">
<a href="https://base-art.net/Articles/the-gnome-shell-gajim-extension-maintenance/">by Philippe Normand at March 18, 2018 09:18 AM</a>
</p>
</div>
</div>




<div class="entrygroup" id="tag:base-art.net,2018-03-18:/Articles/web-engines-hackfest-2014/">
<h3><a href="http://base-art.net/" title="Base-Art - Philippe Normand">Phil Normand</a> — <a href="https://base-art.net/Articles/web-engines-hackfest-2014/">Web Engines Hackfest&nbsp;2014</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="philn.png" alt="(Phil Normand)" width="85" height="115">
<p>Last week I attended the <a class="reference external" href="http://www.webengineshackfest.org/">Web Engines Hackfest</a>. The
event was sponsored by Igalia (also hosting the event), Adobe and&nbsp;Collabora.</p>
<p>As usual I spent most of the time working on the WebKitGTK+ GStreamer
backend and <a class="reference external" href="https://coaxion.net/">Sebastian Dröge</a> kindly joined and helped out quite a
bit, make sure to read …</p></div>

<p class="date">
<a href="https://base-art.net/Articles/web-engines-hackfest-2014/">by Philippe Normand at March 18, 2018 09:18 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">March 12, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2018-03-12T23:00:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> — <a href="https://gstreamer.freedesktop.org/news/#2018-03-12T23:00:00Z">GStreamer 1.13.91 pre-release (1.14 rc2)</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is pleased to announce the second and hopefully last
release candidate for the upcoming stable 1.14 release series.
</p><p>
The 1.14 release series adds new features on top of the current stable 1.12
series and is part of the API and ABI-stable 1.x release series of the
GStreamer multimedia framework.
</p><p>
The 1.13.91 pre-release is for testing and development purposes in the lead-up
to the stable 1.14 series which is now feature frozen and scheduled for release
soon.
</p><p>
Full release notes can be found on the <a href="https://gstreamer.freedesktop.org/releases/1.14/">1.14 release notes page</a>,
highlighting all the new features, bugfixes, performance optimizations and
other important changes.
</p><p>
Packagers: please note that quite a few plugins and libraries have moved
between modules, so please take extra care and make sure inter-module
version dependencies are such that users can only upgrade all modules in
one go, instead of seeing a mix of 1.13 and 1.12 on their system.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided shortly.
</p><p>
Release tarballs can be downloaded directly here:
</p><ul>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.13.91.tar.xz">gstreamer-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.13.91.tar.xz">gst-plugins-base-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.13.91.tar.xz">gst-plugins-good-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.13.91.tar.xz">gst-plugins-ugly-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.13.91.tar.xz">gst-plugins-bad-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.13.91.tar.xz">gst-libav-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.13.91.tar.xz">gst-rtsp-server-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.13.91.tar.xz">gst-python-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.13.91.tar.xz">gst-editing-services-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.13.91.tar.xz">gst-validate-1.13.91.tar.xz</a>,</li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.13.91.tar.xz">gstreamer-vaapi-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.13.91.tar.xz">gst-omx-1.13.91.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-sharp/gstreamer-sharp-1.13.91.tar.xz">gstreamer-sharp-1.13.91.tar.xz</a></li>
</ul>
        <p></p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2018-03-12T23:00:00Z">March 12, 2018 11:00 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">March 03, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2018-03-03T23:00:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> — <a href="https://gstreamer.freedesktop.org/news/#2018-03-03T23:00:00Z">GStreamer 1.13.90 pre-release (1.14 rc1)</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is pleased to announce the first release candidate for the
upcoming stable 1.14 release series.
</p><p>
The 1.14 release series adds new features on top of the current stable 1.12
series and is part of the API and ABI-stable 1.x release series of the
GStreamer multimedia framework.
</p><p>
The 1.13.90 pre-release is for testing and development purposes in the lead-up
to the stable 1.14 series which is now feature frozen and scheduled for release
soon.
</p><p>
Full release notes will be provided in the near future, highlighting all the
new features, bugfixes, performance optimizations and other important changes.
</p><p>
Packagers: please note that quite a few plugins and libraries have moved
between modules, so please take extra care and make sure inter-module
version dependencies are such that users can only upgrade all modules in
one go, instead of seeing a mix of 1.13 and 1.12 on their system.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided shortly.
</p><p>
Release tarballs can be downloaded directly here:
</p><ul>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.13.90.tar.xz">gstreamer-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.13.90.tar.xz">gst-plugins-base-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.13.90.tar.xz">gst-plugins-good-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.13.90.tar.xz">gst-plugins-ugly-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.13.90.tar.xz">gst-plugins-bad-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.13.90.tar.xz">gst-libav-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.13.90.tar.xz">gst-rtsp-server-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.13.90.tar.xz">gst-python-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.13.90.tar.xz">gst-editing-services-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.13.90.tar.xz">gst-validate-1.13.90.tar.xz</a>,</li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.13.90.tar.xz">gstreamer-vaapi-1.13.90.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.13.90.tar.xz">gst-omx-1.13.90.tar.xz</a></li>
</ul>
        <p></p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2018-03-03T23:00:00Z">March 03, 2018 11:00 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">February 27, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="tag:blogger.com,1999:blog-701969077517001201.post-5212108155320993264">
<h3><a href="http://blog.nirbheek.in/" title="Nirbheek’s Rantings">Nirbheek Chauhan</a> — <a href="http://blog.nirbheek.in/2018/02/decoupling-gstreamer-pipelines.html">Decoupling GStreamer Pipelines</a></h3>
<div class="entry">
<div class="content">
<div dir="ltr"><span><i>This post is best read with some prior familiarity with <a href="https://gstreamer.freedesktop.org/" target="_top">GStreamer</a> pipelines. If you want to learn more about that, a good place to start is the <a href="https://twitter.com/thaytan/status/956366764543111169" target="_top">tutorial Jan presented at LCA 2018</a>.</i></span><br><br><h2>Elevator Pitch</h2><br>GStreamer was designed with modularity, pluggability, and ease of use in mind, and the structure was somewhat inspired by UNIX pipes. With GStreamer, you start with an idea of what your dataflow will look like, and the pipeline  will map that quite closely.<br><br>This is true whether you're working with a simple and static pipeline:<br><br><code>source ! transform ! sink</code><br><br>Or if you need  complex and dynamic pipelines with varying rates of data flow:<br><br><div class="separator"><a href="https://i.imgur.com/GJC4y2y.png"><img src="foo.png" width="400" height="37" border="0"></a></div><br>The inherent pluggability of the system allows for quick prototyping and makes a lot of changes simpler than they would be in other systems.<br><br>At the same time, to achieve efficient multimedia processing, one must avoid onerous copying of data, excessive threading, or additional latency. Other features necessary are  varying rates of playback, seeking, branching, mixing, non-linear data flow, timing, and much more, but let's keep it simple for now.<br><br><h2>Modular Multimedia Processing</h2><br>A naive way to implement this would be to have one thread (or process) for each node, and use shared memory or message-passing. This can achieve high throughput if you use the right APIs for zerocopy message-passing, but because of a lack of realtime guarantees on all consumer operating systems, the latency will be jittery and much harder to achieve.<br><br>So how does GStreamer solve these problems?<br><br>Let's take a look at a simple pipeline to try and understand. We generate a sine wave, encode it with <a href="https://opus-codec.org/" target="_top">Opus</a>, mux it into an Ogg container, and write it to disk.<br><br><code><br>$ gst-launch-1.0 -e audiotestsrc ! opusenc ! oggmux ! filesink location=out.ogg<br></code><br><br>How does data make it from one end of this pipeline to the other in GStreamer? The answer lies in <i>source pads</i>, <i>sink pads</i> and the <a href="https://gstreamer.freedesktop.org/documentation/plugin-development/basics/chainfn.html" target="_top">chain function</a>.<br><br>In this pipeline, the <code>audiotestsrc</code> element has one <i>source pad</i>. <code>opusenc</code> and <code>oggmux</code> have one <i>source pad</i> and one <i>sink pad</i> each, and <code>filesink</code> only has a <i>sink pad</i>. Buffers always move from source pads to sink pads. All elements that receive buffers (with sink pads) must implement a <i>chain function</i> to handle each buffer.<br><br>Zooming in a bit more, to output buffers, an element will call <code>gst_pad_push()</code> on its <i>source pad</i>. This function will figure out what the corresponding <i>sink pad</i> is, and call the chain function of that element with a pointer to the buffer that was pushed earlier. This chain function can then apply a transformation to the buffer and push it (or a new buffer) onward with <code>gst_pad_push()</code> again.<br><br>The net effect of this is that all buffer handling from one end of this pipeline to the other happens <b>in one series of chained  function calls</b>. This is a really important detail that allows GStreamer to be efficient by default.<br><br><h2>Pipeline Multithreading</h2><br>Of course, sometimes you <i>want</i> to decouple parts of the pipeline, and that brings us to the simplest mechanism for doing so: the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-plugins/html/gstreamer-plugins-queue.html#gstreamer-plugins-queue.description" target="_top"><code>queue</code> element</a>. The most basic use-case for this element is to ensure that the downstream of your pipeline <a href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/multithreading-and-pad-availability.html" target="_top">runs in a new thread</a>.<br><br>In some applications, you want even greater decoupling of parts of your pipeline. For instance, if you're reading data from the network, you don't want a network error to bring down our entire pipeline, or if you're working with a hotpluggable device, device removal should be recoverable without needing to restart the pipeline.<br><br>There are various&nbsp; mechanisms to achieve such decoupling: <a href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/short-cutting-the-pipeline.html" target="_top"><code>appsrc</code>/<code>appsink</code></a>, <code>fdsrc</code>/<code>fdsink</code>, <code>shmsrc</code>/<code>shmsink</code>, <a href="https://www.collabora.com/news-and-blog/blog/2017/11/17/ipcpipeline-splitting-a-gstreamer-pipeline-into-multiple-processes/" target="_top"><code>ipcpipeline</code></a>, etc.&nbsp; However, each of those have their own limitations and complexities. In particular, <a href="https://gstreamer.freedesktop.org/documentation/application-development/basics/data.html#events" target="_top">events</a>, <a href="https://gstreamer.freedesktop.org/documentation/design/negotiation.html" target="_top">negotiation</a>, and <a href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/time-management.html" target="_top">synchronization</a> usually need to be handled or serialized manually at the boundary.<br><br><h2>Seamless Pipeline Decoupling</h2><br>We recently merged a new plugin that makes this job much simpler: <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/commit/?id=3f7e29d5b32f20dff75a58186533e40bb0ed4081" target="_top">gstproxy</a>. Essentially, you insert a <code>proxysink</code> element when you want to send data outside your pipeline, and use a <code>proxysrc</code> element to push that data into a different pipeline in the same process.<br><br>The interesting thing about this plugin is that <i>everything</i> is proxied, not just buffers. Events, queries, and hence caps negotiation all happen seamlessly. This is particularly useful when you want to do dynamic reconfiguration of your pipeline, and want the decoupled parts to reconfigure automatically.<br><br>Say you have a pipeline like this:<br><br><code><br>pulsesrc ! opusenc ! oggmux ! souphttpclientsink<br></code><br><br>Where the <code>souphttpclientsink</code> element is doing a <code>PUT</code> to a remote HTTP server. If the server suddenly closes the connection, you want to be able to immediately reconnect to the same server or a different one without interrupting the recording. One way to do this, would be to use <code>appsrc</code> and <code>appsink</code> to split it into two pipelines:<br><br><code><br>pulsesrc ! opusenc ! oggmux ! appsink<br><br>appsrc ! souphttpclientsink<br></code><br><br>Now you need to write code to handle buffers that are received on the <code>appsink</code> and then manually push those into <code>appsrc</code>. With the <code>proxy</code> plugin, you split your pipeline like before:<br><br><code><br>pulsesrc ! opusenc ! oggmux ! proxysink<br><br>proxysrc ! souphttpclientsink<br></code><br><br>Next, we connect the <code>proxysrc</code> and <code>proxysink</code> elements, and gstreamer will automatically push buffers from the first pipeline to the second one.<br><br><code>g_object_set (psrc, "proxysink", psink, NULL);</code><br><br><code>proxysink</code> also contains a <code>queue</code>, so the second pipeline will always run in a separate thread.<br><br>Another option is the <a href="https://gstreamer.freedesktop.org/documentation/plugins.html"><code>inter</code> plugin</a>. If you use a pair of <code>interaudiosink/interaudiosrc</code> elements, buffers will be automatically moved between pipelines, but those only support raw audio or video, and drop events and queries at the boundary. The <code>proxy</code> elements push pointers to buffers without copying, and they do not care what the contents of the buffers are.<br><br>This example was a trivial one, but with more complex pipelines, you usually have bins that automatically reconfigure themselves according to the events and caps sent by upstream elements; f.ex <code>decodebin</code> and <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html"><code>webrtcbin</code></a>. This metadata about the buffers is lost when using <code>appsrc</code>/<code>appsink</code>, and similar elements, but is transparently proxied by the <code>proxy</code> elements.<br><br>The <code>ipcpipeline</code> elements also forward buffers, events, queries, etc (not zerocopy, but could be), but they are much more complicated since they were built for splitting pipelines across multiple processes, and are most often used in a security-sensitive context.<br><br>The <code>proxy</code> elements only work when all the split pipelines are within the same process, are much simpler and as a result, more efficient. They should be used when you want graceful recovery from element errors, and your elements are not a vector for security attacks.<br><br>For more details on how to use them, checkout the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/gst/proxy/gstproxysrc.c?id=HEAD#n22" target="_top">documentation and example</a>! The online docs will be generated from that when we're closer to the release of GStreamer 1.14. There are a few caveats, but a number of projects are already using it with great success.</div></div>

<p class="date">
<a href="http://blog.nirbheek.in/2018/02/decoupling-gstreamer-pipelines.html">by Nirbheek (noreply@blogger.com) at February 27, 2018 09:57 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">February 25, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://fortintam.com/blog/?p=3558" lang="en-US">
<h3><a href="http://fortintam.com/blog" title="J.F. Fortin Tam">Jean-François Fortin Tam</a> — <a href="http://fortintam.com/blog/2018/02/25/journey-towards-a-reliable-linux-workstation/">The Longest Debugging—The journey towards a reliable Linux workstation</a></h3>
<div class="entry">
<div class="content">
<p>I have this curse where I keep finding heisenbugs not only in the software I use, but also in hardware… The difference being that, unlike software misbehavior, hardware issues take me <em>months</em> to figure out. But hey, they say I’m a persistent bastard.</p>
<p><img class="wp-image-3636 aligncenter" src="hardware-debugging-meme-500x392.jpg" alt="" width="300" height="235"></p>
<p>This blog post is mainly a tale about computer hardware, which is a bit unusual on this blog, but it ties back into software and Linux graphics troubleshooting, so there should be something interesting for everybody. Enjoy a solid 20 minutes of reading my walk through the inferno, hopefully you’ll find something insightful or funny in my unreasonably persistent path to hardware salvation. Anyway, the reading time doesn’t seem that long when it took me <em>10 months</em> to write this post over 37 revisions <img src="1f609.png" alt="😉" class="wp-smiley"><span id="more-3558"></span></p>
<p>This divine comedy is divided in a série of chants:</p>
<ol>
<li><em>“Prologue: Beware of Hardware”</em> (two short case studies of strange hardware bugs from years past)</li>
<li><em>“I once thought 4GB of RAM ought to be enough for anybody”</em> (a tale of modern web browsing, and of operating system kernels gone rogue)</li>
<li><em>“Overwhelming power: the new Kusanagi”</em> (this is where today’s hardware story begins)</li>
<li><em>“The quest for stability begins”</em> (my descent into Hell)</li>
<li><em>“Finding the culprit”</em> (meeting with Virgilio)</li>
<li><em>“Reworking my workstation”</em> (salvation)</li>
<li>Epilogue</li>
</ol>
<h1>Prologue: Beware of Hardware</h1>
<p><img class="alignnone size-full wp-image-3796" src="tears_of_steel_frame_01_2a.jpg" alt="" width="1280" height="533"></p>
<p>A bit over ten years ago (jeez, we’re old) <a href="https://www.linkedin.com/in/jeffschroeder/" target="_top" rel="noopener">herr Schroeder</a> gave me this advice: <em>“Never skimp on RAM or power supplies, because troubleshooting issues that involve these is so damned hard”</em> (I’m loosely quoting from memory). Sage advice, but somehow all the hardware heisenbugs I encountered so far turned out not to be in those particular components. For example:</p>
<ul>
<li><small>Some years ago, I spent months trying to figure out why my aunt’s brand new computer running Centos would experience random kernel panics. Whenever she would bring the computer to my office I would be unable to reproduce the issue no matter how hard I tried to “break it”, and when it went back home the issue would start reappearing again (I saw it with my own eyes). So I tested for peripherals combinations, software combinations, network combinations, tried torture-testing the thing with various benchmarks (including repeatedly opening and closing hundreds of tabs at once, heating up the system)… until I realized that the key to the problem was the fact that the computer was being physically transported across the two locations. Eventually I found the cause: a faulty SATA cable connected to the SSD!</small></li>
<li><small>Last year, it took me months (again) to figure out why my father’s desktop computer had started randomly freezing (the applications on the screen, and the mouse cursor, would just lock up out of the blue). Here again I did stress tests, tried systematically reverting and bisecting recent software updates to figure out what had “broken” the system—after all, the computer hadn’t moved at all, it could only be due to some software regression, right? Until I bothered to open the case and found <a href="https://en.wikipedia.org/wiki/Capacitor_plague" target="_top" rel="noopener">capacitor plague</a>.</small></li>
</ul>
<a href="http://fortintam.com/blog/wp-content/uploads/2016-05-30-capacitor-plague.jpg"><img class="wp-image-3622 size-large" src="2016-05-30-capacitor-plague-1024x539.jpg" alt="" width="525" height="276"></a>Capacitor plague on my parents’ computer. I went to an electronics hardware store, bought three or four capacitors for a few cents each and replaced the blown capacitors on the motherboard. Since then, the computer is running like a champ.
<p>Well, today’s blog post is another story about taking months to find the solution to a problem—except this time it was worse. Much, much worse. Grab yourself a beer in the fridge.</p>
<hr>
<h1>I once thought, <em>“4 GB of RAM ought to be enough for anybody”</em></h1>
<p>These days, if you’re a marketeer or hardcore project manager, it seems 5-8 gigabytes of RAM is no longer enough to do anything serious in parallel to web browsing&nbsp;on a GNU+Linux system. See also:</p>
<ul>
<li><a href="http://idlewords.com/talks/website_obesity.htm" target="_top" rel="noopener">the website obesity crisis</a>,</li>
<li>the <a href="https://josephg.com/blog/electron-is-flash-for-the-desktop/" target="_top" rel="noopener">disease of web apps being used as replacements for native desktop applications</a>,</li>
<li>my <a href="http://fortintam.com/blog/2010/09/19/free-my-memory/">seven-years-old rant on Firefox</a>—which, given that Firefox is more “globally memory-efficient” than Chrome/Chromium these days, is in big part being fixed this year as we now have <a href="https://blog.mozilla.org/addons/2016/04/11/the-why-of-electrolysis/" target="_top" rel="noopener">Electrolysis</a> and <a href="https://metafluff.com/2017/07/21/i-am-a-tab-hoarder/" target="_top" rel="noopener">Quantum Flow startup times</a> (with Firefox 55, my startup time went from 1 minute and 13 seconds to… 5.8 seconds).</li>
</ul>
<p>Since I have a lot of tabs open in parallel to my research or graphics work, I tend to run into <a href="https://en.wikipedia.org/wiki/Out_of_memory">OoM</a> conditions <em>all the time</em>, no matter which web browser I use. Thanks, web 2.0!</p>
<img class="size-full wp-image-931" src="metal-gear-rex.jpg" alt="" width="600" height="427">Pictured: a modern web browser.
<p>The issue wouldn’t be so bad if the Linux kernel actually did its job, but it doesn’t: when your RAM is full, the Linux kernel will just start trashing your hard disk for no good reason, and <em>if you’re lucky</em> after 30 minutes it <em>might</em> figure out “Oh, that web browser process thing that grew at hundreds of megabytes per minute or tried to allocate 20 times the amount of available RAM, maybe that’s the one I should kill instantly?”… Or, to put things succintly:</p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en"><a href="https://twitter.com/mairin?ref_src=twsrc%5Etfw">@mairin</a> I wish the OOM killer ignored small fry and chased down the real outlaws. Like that Firefox bastard that terrorizes the town.</p>
<p>— Jeff Fortin Tam (@nekohayo) <a href="https://twitter.com/nekohayo/status/460975304157048834?ref_src=twsrc%5Etfw">April 29, 2014</a></p></blockquote>
<p></p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en">Number of times I've seen OOM handled correctly ⇒ 0</p>
<p>— Christian Hergert (@hergertme) <a href="https://twitter.com/hergertme/status/704607950829068288?ref_src=twsrc%5Etfw">March 1, 2016</a></p></blockquote>
<p></p>
<p>This is in addition to having our desktop environments slow to a crawl whenever there is I/O (hard disk) trashing going on:</p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en">: My computer hardlocks due to IO craziness once or twice a week &amp; I lose data every week or two (when forcefully rebooting) too. <img src="1f622.png" alt="😢" class="wp-smiley"></p>
<p>— Garrett LeSage (@garrett) <a href="https://twitter.com/garrett/status/740577649668546560?ref_src=twsrc%5Etfw">June 8, 2016</a></p></blockquote>
<p></p>
<p><!-- My computer hardlocks due to IO craziness once or twice a week &amp; I lose data every week or two (when forcefully rebooting) too. &#x1f622; — Garrett LeSage (@garrett) June 8, 2016 -->I suspect there is only a handful of us crazies (“power users”) acutely aware of these issues. Not everybody experiences or notices this, but just like video tearing in X.org or the general <a href="https://bugzilla.gnome.org/show_bug.cgi?id=642652#c178">framerate dropping over time</a> in GNOME Shell, it’s the kind of thing where “once you become aware of its existence, you cannot un-see it”.</p>
<p>Overall, the Linux kernel is still pretty subpar (I’m being polite here) for “workstation” desktop usecases. It’s so, <em>so incredibly bad</em> at handling memory and I/O. Throughout the years, it has been the #1 component of my system that regularly made me want to throw furniture across the room as I hit “out of memory” conditions that destroyed my productivity. <em>All I wanted was to do design+photography work in parallel to tons of online research!</em></p>
<hr>
<h1>Overwhelming power:<br>
the new Kusanagi</h1>
<p>Hence, after a decade of waiting for the Linux kernel to get its act together, I gave up and decided to nuke the problem from orbit by replacing my “perfectly good” (but maxed out) computer—<em>Kusanagi—</em>by a workstation so stupidly powerful that I would not even be affected by the kernel’s lackluster resources management anymore.</p>
<p>Since this is GNU+Linux we’re talking about, I would just have to transplant its cyberbrain (hard drives and SSDs, containing the “ghost”) into the new “shell”—a trivial operation, no need to reinstall the operating system or anything.</p>
<p><img class="alignnone size-full wp-image-3799" src="batou-connecting-the-tank-and-kusanagi.jpg" alt="" width="1280" height="688"></p>
<p>So, at the end of 2015, I bought this completely overpowered 2nd-hand cyborg shell, for about 370 Canadian yen:</p>
<p><img class="alignnone size-full wp-image-3800" src="newborn-kusanagi.jpg" alt="" width="1280" height="688"></p>
<p>Er, I mean:</p>
<p><img class="alignnone size-full wp-image-3625" src="dell_precision-t3500.jpg" alt="" width="1024" height="768"></p>
<p>…a second-hand Dell “Precision T3500” workstation maxed out with <em>24 gigabytes</em> of RAM and an 8-cores Xeon processor, lazily named “Kusanagi&nbsp;2.0”<em>.</em> While it was made in 2009, it was a <a href="https://en.wikipedia.org/wiki/Workstation" target="_top" rel="noopener">workstation</a>-class machine, and remains completely overkill even today, in <del>2016</del> <del>2017</del> 2018.<em><br>
</em></p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2015-12-05-12.42.48.jpg"><img class="alignnone size-full wp-image-3623" src="2015-12-05-12.42.48.jpg" alt="" width="1000" height="659"></a></p>
<p>This would let me kill two birds with one catapult boulder:</p>
<ul>
<li>Accomplish more parallel work, and be more productive in my daily work—I was utterly sick of “having to think about RAM” or experiencing debilitating lockups. I want to think about <em>business problems,</em> not the boundaries of my tools.</li>
<li>Accomplish more complex work, such as completing <a href="http://fortintam.com/blog/2017/06/11/painting-two-old-friends-tintin-vs-sephiroth/">a special painting project where I needed at least 10 gigabytes of RAM to work with</a> (Kusanagi 1.0 “only” had 5 GB).</li>
</ul>
<p>I did not expect to see an application-level performance difference compared to my previous quad-core Inspiron 530n, but there definitely is one—I was shocked at how ridiculously faster my “new” computer turned out to be, with the Xeon and DDR3 RAM (vs DDR2). With Firefox (before the Quantum days) or GTG, you could feel applications launching and responding noticeably quicker. Neat!</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2015-12-15-15.16.46.jpg"><img class="alignnone size-large wp-image-3624" src="2015-12-15-15.16.46-1024x714.jpg" alt="" width="525" height="366"></a></p>
<h2>Basic care and tweaks</h2>
<p>First, the machine needed some cleanup (click to enlarge):</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2015-12-04-15.20.23.jpg"><img class="alignnone size-large wp-image-3627" src="2015-12-04-15.20.23-1024x636.jpg" alt="" width="525" height="326"></a></p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2015-12-04-15.20.11.jpg"><img class="alignnone wp-image-3626 size-large" src="2015-12-04-15.20.11-1024x474.jpg" alt="" width="525" height="243"></a></p>
<p>Then, I removed the metal faceplate in the front and custom-built an air intake dust filter—using stockings, some L-shaped metal enclosed in a plastic frame to create the filter’s support system, and some hooks to hold it in place.</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2016-05-08-14.48.56.jpg"><img class="alignnone size-large wp-image-3630" src="2016-05-08-14.48.56-1024x606.jpg" alt="" width="525" height="311"></a></p>
<p>It now had a sober, unassuming “serious business” look, which I rather liked for its simplicity (and the lack of flashiness makes it less appealing to thieves):</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2016-05-09-14.38.45.jpg"><img class="alignnone size-large wp-image-3631" src="2016-05-09-14.38.45-972x1024.jpg" alt="" width="525" height="553"></a></p>
<p>Here is the resulting battlestation:</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2016-11-20-21.00.23.jpg"><img class="alignnone wp-image-3629 size-full" src="2016-11-20-21.00.23.jpg" alt="" width="1000" height="893"></a></p>
<p>With this “new” super powerful computer, I was ready to be immediately productive, right? Well, <em>almost. </em></p>
<h1>The quest for stability begins<br>
(descent into Hell)</h1>
<p><img class="alignnone size-full wp-image-3789" src="devil-may-cry-5-dante.jpg" alt="" width="1000" height="561"></p>
<p>There turned out to be just <em>one</em> problem with the new workhorse: after a few weeks of use, I realized I kept running into a strange issue where <a href="https://bugs.freedesktop.org/show_bug.cgi?id=93341" target="_top" rel="noopener">the video card would randomly die on me and the Linux kernel would panic</a>, freezing the whole machine.</p>
<p>It would happen at any time: often when doing something that stresses the GPU (like watching videos or using an OpenGL/WebGL application), but also (a bit more rarely) when not doing anything in particular; I could be just sitting and staring at my desktop when suddenly the monitor would turn off and I would get the same errors in dmesg:</p>
<pre>radeon 0000:02:00.0: ring 0 stalled for more than 10252msec
radeon 0000:02:00.0: GPU lockup (current fence id 0x00000000006c9132 last fence id 0x00000000006c928b on ring 0)
radeon 0000:02:00.0: failed to get a new IB (-35)
[drm:radeon_cs_ioctl [radeon]] *ERROR* Failed to get ib !
BUG: unable to handle kernel paging request at ffffc90404239ffc
IP: [&lt;ffffffffa013736a&gt;] radeon_ring_backup+0xda/0x190 [radeon]
PGD 6068a8067 PUD 0 
Oops: 0000 [#1] SMP</pre>
<p>It was infuriating. I had this superb, powerful machine… that I couldn’t use except for the lightest tasks (like basic web surfing, email, office work), and still had to watch out for potential data loss as a result of unpredictable crashes.</p>
<p>Thus began a painful, expensive, nearly endless quest to figure out why my graphics card was randomly crashing. <em>Note: if you don’t care about the “investigation” educational part, Ctrl+F “Finding the culprit” to skip to the next part. Otherwise, read on, my geeky friend.</em></p>
<hr>
<p>Potential causes I suspected:</p>
<ol>
<li>Linux “radeonsi” driver bug/regression (“hey, it worked before December 2015!”): unsure, but presumed.</li>
<li>Some sort of SNAFU somewhere else in the stack in my Linux distribution (<a href="https://getfedora.com/" target="_top" rel="noopener">Fedora</a>), ie a distro-specific bug: unsure.</li>
<li>RAM errors: did memtests, no problems there.</li>
<li>Capacitor plague on the motherboard: nope, it all looked good to the naked eye.</li>
<li>Underpowered or failing power supply</li>
<li>Linux-incompatible motherboard or BIOS</li>
<li>Incompatible motherboard-and-GPU combination</li>
</ol>
<p>For #1 and 2, I placed my hopes on the (then upcoming) Wayland-based Fedora 25 (“gotta wait for nov/december 2016”): turned out to not be the solution. Still randomly crashing.</p>
<p>Hypotheses #6 and 7 would be dealbreakers, where it would mean I would have wasted my money on the workstation as I would have to replace it again. My geek honor was not ready to accept that scenario.</p>
<p>All along, I was trying to figure out how to trigger the bug, by:</p>
<ul>
<li>trying to overload the system (be it the graphics card or CPU or I/O, with games/compiling/webGL demos/video playback/etc.)</li>
<li>using a different graphics card (another older radeon, or a nVidia card which was impossible to get working drivers for)</li>
<li>putting the card back into my older computer (and indeed the issue didn’t seem to happen there)</li>
<li>transplanting (exchanging) power supplies</li>
<li>spreading the load across multiple power supplies (“What if one power supply is not enough to power the Xeon <em>and</em> the Radeon?”)</li>
</ul>
<p>It was absolute hell to debug/reproduce. It would sometimes crash within 10 minutes, an hour, or sometimes work for 2-3 days straight without issues, which made me question if it even worked 100.00% reliably on Kusanagi 1.0, muddying up the waters and making me waste countless week-ends and question my sanity. A true hardware heisenbug.</p>
<a href="http://fortintam.com/blog/wp-content/uploads/2016-07-01-19.04.45.jpg"><img class="size-large wp-image-3633" src="2016-07-01-19.04.45-785x1024.jpg" alt="" width="525" height="685"></a>Running multiple simultaneous WebGL demos for days on end
<a href="http://fortintam.com/blog/wp-content/uploads/2016-07-01-multiple-web-benchmarks.jpg"><img class="size-large wp-image-3816" src="2016-07-01-multiple-web-benchmarks-1024x695.jpg" alt="" width="525" height="356"></a>Some of the WebGL and WebRTC stress tests being run
<p>Throughout these tests, I was considering the possibility that Dell’s Foxconn power supply had received too much abuse from its previous owners, which&nbsp; seemed plausible (but can you <em>really</em> be sure?) as the graphics card wouldn’t crash the kernel when it was inside “Kusanagi 1.0” or when Kusanagi 1.0’s power supply was connected siamese-style with a power link across to Kusanagi 2.0:</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2016-07-02-10.35.41.jpg"><img class="alignnone size-large wp-image-3634" src="2016-07-02-10.35.41-1024x518.jpg" alt="" width="525" height="266"></a></p>
<p>This seemed like a good hypothesis, ergo #YOLO, I ordered a brand new power supply, the eVGA <em>Supernova G2</em> series (the best I could find on the market in late 2016—I spent way too much time researching and reading reviews) and… nope, it didn’t solve the issue.</p>
<p><img class="alignnone size-full wp-image-1962" src="rage-guy.jpg" alt="" width="418" height="357"></p>
<p>125$ wasted for the sake of the experiment (it was not economically feasible to return the power supply, so I thought I might as well keep it). At least I could say I got an extremely good silent power supply that should last me a decade or more (considering my other branded power supply from 2003 still works today).</p>
<p>I also tested Windows on this machine to be sure, and no matter what I tried to do to stress the system, it wouldn’t crash, an observation that threw me back to the “it’s a bug somewhere in the Linux graphics stack” theory.</p>
<p>Okay, so the computer crashes <em>only</em> on Linux. Randomly. Counterproductively. At that point I had become quite disheartened with the Linux graphics stack that was causing me such grief for months on end, and I was looking at my options:</p>
<ul>
<li>Go back to Kusanagi 1.0 (and lose the copious amounts of RAM).</li>
<li>Run Windows: unbearable. I can’t stand this piece of crap, I feel handicapped everytime I use it.</li>
<li>Make a hackintosh out of it and run Mac OS: pain in the ass, and handicapping as well.</li>
<li>Buy nVidia and make it work. <em>Ha ha ha. No.</em></li>
<li>Resell Kusanagi 2.0, rebuild a 600-1000$ brand new Intel-only DIY workstation (Dell prices workstations at 2.5-3.5k$!) retrofitted in an old Mac Pro case.</li>
<li>Wait some months/years for <a href="https://en.wikipedia.org/wiki/Free_and_open-source_graphics_device_driver#amdgpu" target="_top" rel="noopener">AMDGPU</a> to replace radeonsi as the One and Only driver, and “hope” it is unaffected: I don’t believe in magic.</li>
<li>“Just don’t stress the system”, use the computer for menial tasks: a huge waste.</li>
</ul>
<hr>
<h1>Finding the culprit</h1>
<p><img class="alignnone size-full wp-image-3790" src="dante-vs-vergil.jpg" alt="" width="903" height="562"></p>
<p>Eventually, after having spent weeks testing with games, videoconferencing, video playback and WebGL demos, I started truly <em>torturing</em> the graphics card with “furmark” and realized the issue occurred when the card was overheating to a really high temperature (113 degrees Celsius), but <em>only</em> on Linux. Finally, a way to reproduce the issue reliably! And so I wrote into <a href="https://bugs.freedesktop.org/show_bug.cgi?id=93341" target="_top" rel="noopener">the bug report</a>:</p>
<blockquote><p>My understanding is that on Radeons (well, at least the Radeon HD 7770), there is an emergency mechanism in the hardware (or firmware/microcode maybe) that activates self-throttling of performances when the GPU reaches a critical temperature. Normally, the video driver is supposed to handle this state change gracefully, however the radeonsi/radeon/amdgpu driver on Linux does not, so the kernel panics because the driver went belly up.</p></blockquote>
<p><img class="wp-image-3641 alignright" src="one-does-not-simply-say-well-it-worked-on-my-machine-500x295.jpg" alt="" width="351" height="207">“Duh! Just get better cooling!” might sound like the solution, but technically it still is a software/driver issue: the radeonsi driver on Linux does not handle the event where the hardware force-throttles itself. In Windows, breaching the 110-113 degrees Celsius limit results in the video driver simply dropping frames massively, continuing to function at reduced performance (ie: going from 40-60 fps to 10-15 fps on one of my benchmarks). The system never crashes. The Linux driver <em>should</em> handle such scenarios gracefully just as well as the Windows driver. At least, that’s the theory.</p>
<p>In practice, it would be quicker for me to solve my cooling problem than to wait for a driver bugfix. Besides, my graphics card would thank me (and provide better performance).</p>
<p>But wait, I’ve had the GPU since 2012, so <strong>why didn’t I encounter this with my previous computer,</strong> prior to December 2015? Because Kusanagi 2.0’s case has a different airflow and cooling behavior from Kusanagi 1.0. So now you’re asking, <strong>why didn’t I realize this during my weeks of benchmarking</strong> then? Because it was very hard to get consistent crashes (the more I tried to investigate it, the less sense it made), due to these factors:</p>
<ul>
<li>The Radeon 7770 I have is an “open air” cooling system which spreads the heat into the case (not a “blower” fan that exhausts outside the case), which means that for the bug to occur, the whole system has to reach a temperature plateau which might require specific CPU and GPU interaction or ambient room temperature conditions;</li>
<li>I was sometimes testing with the case closed, sometimes with the case open (when trying different power supplies configurations), which threw off my results.</li>
</ul>
<p>Anyway. At least, now I knew the cause, and therefore had a basic workaround: just keep the computer’s case open, where the heat would evacuate naturally and the graphics card would never reach the critical temperature, preventing the issue. But this looks silly and lets the dust in, so I set out to find the “proper” solution to extract the heat without needing to keep the case open.</p>
<p><img class="alignnone size-full wp-image-3813" src="hhcib-topgear-2.png" alt="" width="560" height="315"></p>
<h1>Reworking my workstation’s thermal design beyond what Dell intended it to be</h1>
<p>I had the following restrictions for the solution I wanted to find:</p>
<ol>
<li>Reasonably cheap. Otherwise I might as well just cut my losses and build a brand new machine.</li>
<li>No replacing my “perfectly good” and well-tested Radeon 7770: I didn’t want to go back into the “let’s wait for Free/Open drivers to be developed for your card” cycle again. Also, see point #1.</li>
<li>Super quiet. I’m an absolute maniac when it comes to having “silent” computers. I like to hear myself <em>think.</em> Therefore, the solution had to not only be efficient at exchanging the air between the case and the room, it also had to be nearly inaudible.</li>
<li>No drilling/cutting of the case if at all possible (I don’t like irreversible mods, given how much trial-and-error is involved here).</li>
</ol>
<p>The big challenge would be to devise something compatible with the T3500’s proprietary case and airflow design (2x120mm intake fans in a suspended cage in the front pushing air into a fanless CPU heatsink, and 2x80mm exhaust grilles at the bottom at the back):</p>
<a href="http://fortintam.com/blog/wp-content/uploads/original-Dell-T3500-thermal-design-airflow.png"><img class="wp-image-3646 size-full" src="original-dell-t3500-thermal-design-airflow.png" alt="" width="948" height="600"></a>The Dell Precision T3500 moves air from front to back through the bottom half of the case, while the graphics card (which sucks air from the top and spreads it onto the card and everywhere into the case) is in the upper half of the case, with the power supply that does not extract anywhere near enough air from the GPU’s hotspot, and the GPU’s board blocking air exchange with the bottom half of the case—that clearly wasn’t going to work “as designed”.
<p>I considered the following possibilities:</p>
<ul>
<li>Just replace the thermal compound on the graphics card by Arctic Silver 5 (and wait 200 hours for it to cure), and try various fan combinations to see the impact on temperatures and time-to-crash.</li>
<li>Put an additional exhaust fan or two in the back in the lower half: nope, didn’t work: the hot air pocket remained in the upper half (and the power supply’s fans were not moving enough air to compensate it, either).</li>
<li>Put some small turbine <a href="https://www.amazon.com/StarTech-com-Expansion-Exhaust-Connector-FANCASE/dp/B0000510SS/" target="_top" rel="nofollow noopener">expansion slot fan</a> in the back: that would certainly be ineffective (since the card is an open design instead of a blower) and very noisy.</li>
<li>Leaving the case open and building a giant filter all over it.</li>
<li>Cutting out a hole in the case’s door to have an exhaust fan on the side, extracting the air from the GPU’s area: last resort only.</li>
<li>Watercooling the GPU, or replacing the GPU’s OEM air cooler by some aftermarket “blower” cooler… but that’s spending another pile of money that could get near the cost of a new graphics card, so it seemed beyond reason.</li>
</ul>
<a href="http://fortintam.com/blog/wp-content/uploads/2017-04-14-10.03.51.jpg"><img class="wp-image-3649 size-large" src="2017-04-14-10.03.51-1024x612.jpg" alt="" width="525" height="314"></a>Taking apart the GPU’s stock cooling system
<a href="http://fortintam.com/blog/wp-content/uploads/2017-04-14-10.24.06.jpg"><img class="wp-image-3650 size-large" src="2017-04-14-10.24.06-1024x576.jpg" alt="" width="525" height="295"></a>The GPU’s die after cleaning it up
<a href="http://fortintam.com/blog/wp-content/uploads/2017-04-14-10.27.37.jpg"><img class="wp-image-3651 size-large" src="2017-04-14-10.27.37-1024x576.jpg" alt="" width="525" height="295"></a>Putting new thermal compound on the GPU
<p>Replacing the GPU’s thermal compound didn’t really improve things. There was a slight improvement, but not nearly enough.</p>
<p>Luckily, one day my pal Youness said, “I have an all-in-one watercooler that is gathering dust inside a decommissioned PC. I might as well just give it to you”, to which I replied, “Sure—wait, what did you just say?”</p>
<h2>A waterball’s chance in Hell</h2>
<p>I took the watercooler and proceeded to design the airflow around it. It had a very big radiator and fan (120mm), and a fairly short pair of flexible tubes, meaning I couldn’t place the radiator at the back because the exhaust grilles were too small (80mm) and the tubes would be too short to go around the bulky graphics card.</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-04-25-12.48.25.jpg"><img class="alignnone size-large wp-image-3652" src="2017-04-25-12.48.25-1024x576.jpg" alt="" width="525" height="295"></a></p>
<p>So I had to reverse the case’s airflow: it would exhaust from the front, intake from the back, with the air passing through the CPU’s radiator while being pulled by the watercooler’s fan acting as a case exhaust fan. Essentially, two radiators from two different devices being cooled by one big fan—better hope your CPU heat doesn’t significantly affect the GPU’s radiator (luckily, it didn’t)!</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/kusanagi-2.0-Dell-T3500-thermal-airflow-design-take-2-indexed.png"><img class="alignnone size-full wp-image-3817" src="kusanagi-2.0-dell-t3500-thermal-airflow-design-take-2-indexed.png" alt="" width="948" height="600"></a></p>
<p>The watercooler was a <em>CPU</em> watercooler, not actually meant to be installed on a GPU: it didn’t have compatible mounting brackets, and the contact surface was <em>immense</em> compared to the thumb-sized GPU die. So I used the “<a href="http://www.overclock.net/t/1203636/official-amd-ati-gpu-mod-club-aka-the-red-mod" target="_top" rel="nofollow noopener">red mod</a>” technique to fit it onto the GPU with <a href="https://en.wikipedia.org/wiki/Cable_tie" target="_top" rel="noopener">zip ties</a>. Serious business:</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-08-27-14.33.10.jpg"><img class="alignnone size-large wp-image-3819" src="2017-08-27-14.33.10-1024x576.jpg" alt="" width="525" height="295"></a></p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-08-27-14.30.01.jpg"><img class="alignnone size-large wp-image-3820" src="2017-08-27-14.30.01-1024x685.jpg" alt="" width="525" height="351"></a></p>
<p>I tested the new setup, and it seemed to work wonders: no matter what I did, the computer was now unable to overheat, and the watercooler’s radiator fan acting as the main case exhaust was sufficient to keep both the GPU and the CPU cool, even if both are being heavily loaded simultaneously for hours on end.</p>
<p>To complete the set up, I gave myself a treat and replaced the (also rattling) fan from that second-hand watercooler by a Noctua NF-P12, and added two Noctua redux NF-R8 intake fans to facilitate airflow to the CPU radiator.&nbsp; Result: a computer that can handle any workload and stay whisper quiet.</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-08-27-14.29.36.jpg"><img class="alignnone size-large wp-image-3821" src="2017-08-27-14.29.36-1024x691.jpg" alt="" width="525" height="354"></a></p>
<p>I just had to make a new custom intake filter with a wireframe instead of solid frame, to be installed on the back of the computer, which was much less elegant than the previous front intake filter due to the odd space and shape constraints on the back of the computer:</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-06-16-16.13.01.jpg"><img class="alignnone size-medium wp-image-3653" src="2017-06-16-16.13.01-500x282.jpg" alt="" width="500" height="282"></a></p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-06-30-20.29.35.jpg"><img class="alignnone size-medium wp-image-3654" src="2017-06-30-20.29.35-500x282.jpg" alt="" width="500" height="282"></a></p>
<p>As for the front exhaust, I screwed the watercooler’s radiator onto the front grille, and sealed everything with electric tape so that the ventilation holes would match the radiator exactly, keeping a direct airflow. It now looked like this (any resemblance with Frankenstein is purely accidental):</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-06-01-17.27.35.jpg"><img class="alignnone size-large wp-image-3656" src="2017-06-01-17.27.35-1024x882.jpg" alt="" width="525" height="452"></a></p>
<p><strong>It all worked wonders… <em>until it didn’t.</em></strong> My modification worked for <em>exactly</em> three months, until I moved the computer a bit and the watercooler’s previously temporary rattling “air bubbles” noise became permanent, no matter how I shook or oriented the case. To top it all, while trying to solve the air bubbles issue, the watercooler’s block had now come loose from the GPU die and I would have to re-apply thermal compound and redo the whole zip tie setup (tightening the zip ties is fairly difficult, you need two people for that).</p>
<p>Welcome back to hell.</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/doom-4-cover-art-loli.jpg"><img class="alignnone size-large wp-image-3807" src="doom-4-cover-art-loli-1024x576.jpg" alt="" width="525" height="295"></a></p>
<h2>Turning Hell upside down</h2>
<p>At that point in time, a new possible approach came to my mind: rip out the watercooler, revert to an “open” air-cooled GPU, and <strong>find a way to reorient the computer case itself</strong>. I figured that if the computer was just positioned differently, to let the GPU’s rising hot air escape “naturally” from the top of the computer case (instead of having the intake and exhaust fans move presumably cold air at the bottom of the case and letting the GPU sit in a stagnant air pocket above), it might make a difference. I was ready to try anything at this point.</p>
<p>To test my theory, and since I thought it was probably necessary to have the exhaust directed straight upwards (and the intake below the case), I devised a pretty silly scheme to create “legs” for the back of my computer to stand on (remember, the back is where the air intake fan now was, as well as the connectivity cabling). So I literally “bricked” my computer:</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-08-28-11.07.18.jpg"><img class="alignnone size-large wp-image-3804" src="2017-08-28-11.07.18-1024x680.jpg" alt="" width="525" height="349"></a></p>
<p>The only other way would have been to have the computer suspended on steel wires, but that would be a big stress for my desk and it would also be highly impractical for servicing the machine.</p>
<p>Test results showed that my “bricked computer” now had the best “theoretical” airflow design indeed: with the intake at the bottom and the exhaust at the top, the computer was now completely immune to overheating, even with the case closed! Hurrah!</p>
<p>However, the whole set-up was a bit flimsy and looked quite silly—a 17 kg (38 lbs) computer standing in equilibrium by the edges of its chassis on <em>two bricks on top of a towel!</em> Seriously Anakin, look at this:</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/2017-08-28-11.07.01.jpg"><img class="alignnone size-large wp-image-3803" src="2017-08-28-11.07.01-1024x943.jpg" alt="" width="525" height="483"></a></p>
<p>So I wondered: would it still work if I <strong>flip the entire computer case upside-down from its normal orientation,</strong> with both intake and exhaust fans being located in the top portion and moving air horizontally? I had doubts—after all, the power supply’s exhaust fan in the previous configurations had never been sufficient to get rid of the GPU’s hot air pocket, so would this really work any better?</p>
<p><a href="http://fortintam.com/blog/wp-content/uploads/kusanagi-2.0-Dell-T3500-thermal-airflow-design-take-3-indexed.png"><img class="alignnone size-full wp-image-3818" src="kusanagi-2.0-dell-t3500-thermal-airflow-design-take-3-indexed.png" alt="" width="948" height="600"></a></p>
<p>Turns out that it did.</p>
<ul>
<li>The GPU remains cool in all my stress tests, as the rising heat is still correctly getting evacuated by new “horizontal air corridor” at the top of the case.</li>
<li>The Xeon CPU stays cool under normal working conditions; only under heavy CPU load will its temperature rise (fairly high, up to 80-85 Celsius, due to the recycled GPU air and the absence of fans mounted directly on the Xeon’s heatsink), though it never reaches “critical” thermal limits. <small>I thought of building an “air duct” system to force more air to pass through the CPU heatsink for the rare occasions when I’m pegging the CPU for extended periods of time, but you know what? <em>Screw that—</em>it works “well enough” (and running hot is what workstation-grade Xeons were <em>designed</em> for anyway).</small></li>
</ul>
<p>As you can see, the final solution turned out to be quite trivial. So simple, in fact, I can’t believe it took me so long to find it—and that’s not for lack of online research, discussion with fellow geeks, or thinking hard about the problem as a certified geek.</p>
<h1>Epilogue</h1>
<p><img class="alignnone size-full wp-image-3791" src="devil-may-cry-dmc-fanart.jpeg" alt="" width="1024" height="442"></p>
<p>The story ends as I have achieved workstation nirvana (it really is my favorite computer now, making any work enjoyable), after spending <del>18</del> <strong>21</strong> months to troubleshoot and fix what I thought was a software issue, then a hardware issue, then “a little bit of both” <img src="1f613.png" alt="😓" class="wp-smiley"></p>
<p>I ended up spending a grand total of 555 C$ on the whole setup (370$ for the computer, 125$ for the unecessary power supply, 60$ for top-of-the-line fans), but that is still quite inexpensive (a brand new silent computer with that kind of power would run in the thousands of dollars). Just ignore the “opportunity cost” of my hourly rate when it comes to the time I spent on this!</p>
<p>That said, I learned a lot in the process, and that’s priceless. I hope this troubleshooting tale can help others too—or that you at least had a good laugh at my persistence in repurposing a legacy system into an unstoppable silent powerhouse that crushes most machines we see out there even today.</p>
<p>Hmm? What is it you’re saying? All mainstream CPUs made since the original Pentium have now been found to be vulnerable to a <a href="https://spectreattack.com/" target="_top" rel="noopener">fundamental architectural flaw</a> and we need to get brand new CPUs designed as of 2018? Well,</p>
<p><img class="alignnone size-full wp-image-3797" src="cartman-damnit.gif" alt="" width="220" height="165"></p>
<p>P.s.: feel free to retweet <a href="https://twitter.com/nekohayo/status/967800437524123648" target="_top" rel="noopener">this</a> ;)</p></div>

<p class="date">
<a href="http://fortintam.com/blog/2018/02/25/journey-towards-a-reliable-linux-workstation/">by Jeff at February 25, 2018 04:17 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="https://k-d-w.org/107 at https://k-d-w.org" lang="en">
<h3><a href="https://k-d-w.org/rss.xml" title="Sebastian Pölsterl's blog - Where he blogs about his latest hacking adventures.">Sebastian Pölsterl</a> — <a href="https://k-d-w.org/node/107">Convolutional Autoencoder as TensorFlow estimator</a></h3>
<div class="entry">
<div class="content">
<span class="field field-name-title field-formatter-string field-type-string field-label-hidden">Convolutional Autoencoder as TensorFlow estimator</span>
<div class="clearfix text-formatted field field-node--body field-formatter-text-default field-name-body field-type-text-with-summary field-label-hidden has-single"><div class="field__items"><div class="field__item"><div class="tex2jax_process"><p>In my previous <a href="https://k-d-w.org/node/103">post</a>, I explained how to implement autoencoders as TensorFlow <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span>. I thought it would be nice to add convolutional autoencoders in addition to the existing fully-connected autoencoder. So that's what I did. Moreover, I added the option to extract the low-dimensional encoding of the encoder and visualize it in TensorBoard.</p>
<p>The complete source code is available at <a href="https://github.com/sebp/tf_autoencoder">https://github.com/sebp/tf_autoencoder</a>.</p>
<h2>Why convolutions?</h2>
<p>For the fully-connected autoencoder, we reshaped each 28x28 image to a 784-dimensional feature vector. Next, we assigned a separate weight to each edge connecting one of 784 pixels to one of 128 neurons of the first hidden layer, which amounts to 100,352 weights (excluding biases) that need to be learned during training. For the last layer of the decoder, we need another 100,352 weights to reconstruct the full-size image. Considering that the whole autoencoder consists of 222,384 weights, it is obvious that these two layers dominate other layers by a large margin. When using higher resolution images, this imbalance becomes even more dramatic.</p>
<!--break--><p>
Convolutional layers allow us to significantly reduce the number of weights by sharing weights across multiple edges connecting pixels in the input image to the first hidden layer. A convolutional layer takes a small matrix of weights, let's say 3x3, and slides it across the whole image as shown in the animation below (courtesy of <a href="https://github.com/vdumoulin/conv_arithmetic">Vincent Dumoulin and Francesco Visin</a>):</p>
<div align="center">
<img src="same_padding_no_strides.gif" alt="Convolution with padding and no strides" width="300"></div>
<p>The blue tiles represent the pixels of the input image, and the green tiles represent the output of the convolutional layer after multiplying each 3x3 patch in the input image with the 3x3 weight matrix and summing the result. This operation is called a convolution. To obtain an output – called <em>feature map</em> – of the same size as the input, we need to add a margin of 1 pixel (white tiles). Typical implementations of convolutions use the value of the closest true pixel for padded pixels. Having just a single 3x3 weight matrix in each layer is quite restrictive, thus we apply multiple convolutions on the same input, each with it's own weight matrix. The big advantage is that the number of weights does not depend on the size of the input image, as it was the case for fully-connected layers, instead it is determined by the number of filters/kernels and their respective size (3x3 in the example above).</p>
<p>In the case of MNIST, inputs are $28 \times 28 \times 1$ gray-scale images with a single color channel, and the output of the first convolutional layer has as many "color" channels (feature maps) as there are filters, for example 16. The second convolutional layer will perform the same operation as the first layer, but on an "image" (or tensor to be more precise) of $28 \times 28 \times 16$.</p>
<h2>Encoder</h2>
<p>In the fully-connected autoencoder, we used layers with decreasing complexity by gradually decreasing the number of hidden units. When using convolutional layers in the encoder, we can reduce the complexity by lowering the number of filters or the resolution of the output. Two common approaches exist for down-scaling: 1) pooling values in a small window, 2) using convolutions with strides. The former introduces no additional weights, we simply compute the maximum/minimum/average over a small window – typically 2x2, which reduces width and height by a factor of 2. Note that the pooling operation is applied to each channel independently, thus the number of channels is not altered. Alternatively, a specific form of convolutional layer can be used, as depicted below:</p>
<div align="center">
<img src="padding_strides.gif" width="300"></div>
<p>The difference to the standard convolution from above is that the weight matrix is moving by 2 instead 1 pixel, thus halving height and weight.</p>
<p>In TensorFlow, the encoder following the first approach (using max pooling) becomes:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> conv_encoder<span class="br0">(</span>inputs<span class="sy0">,</span> num_filters<span class="sy0">,</span> scope<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span>scope<span class="sy0">,</span> <span class="st0">'encoder'</span><span class="sy0">,</span> <span class="br0">[</span>inputs<span class="br0">]</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">for</span> layer_id<span class="sy0">,</span> num_outputs <span class="kw1">in</span> <span class="kw2">enumerate</span><span class="br0">(</span>num_filters<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span><span class="st0">'block{}'</span>.<span class="me1">format</span><span class="br0">(</span>layer_id<span class="br0">)</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">repeat</span><span class="br0">(</span>net<span class="sy0">,</span> <span class="nu0">2</span><span class="sy0">,</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d</span><span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_outputs<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kernel_size<span class="sy0">=</span><span class="nu0">3</span><span class="sy0">,</span> stride<span class="sy0">=</span><span class="nu0">1</span><span class="sy0">,</span> padding<span class="sy0">=</span><span class="st0">"SAME"</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">max_pool2d</span><span class="br0">(</span>net<span class="sy0">,</span> kernel_size<span class="sy0">=</span><span class="nu0">2</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">identity</span><span class="br0">(</span>net<span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'output'</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div><br>
where <span class="geshifilter"><code class="text geshifilter-text">num_filters</code></span> is a list number of filters (in decreasing order), and we use a block of two convolutional layers before reducing the spatial resolution via max pooling. For the second approach, the <span class="geshifilter"><code class="text geshifilter-text">max_pool2d</code></span> layer is replaced by<br><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d</span><span class="br0">(</span>net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_outputs<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;kernel_size<span class="sy0">=</span><span class="nu0">3</span><span class="sy0">,</span> stride<span class="sy0">=</span><span class="nu0">2</span><span class="sy0">,</span> padding<span class="sy0">=</span><span class="st0">"SAME"</span><span class="br0">)</span></pre></div></div><br>
where <span class="geshifilter"><code class="text geshifilter-text">stride=2</code></span> tells TensorFlow to slide the weight matrix by 2 pixels. In contrast to max pooling, adding another convolutional layer introduces additional weights when downscaling the image.<br><br><h2>Decoder</h2>
<p>In the decoder, we need to reverse the operations of the encoder and up-scale the image from the low-dimensional embedding of the encoder to its original size. In particular, we need to increase the spatial resolution to 28x28 and reduce the number of channels to 1. For up-scaling, we use a so called transposed convolution with stride 2, which performs the operation depicted in the animation below:</p>
<div align="center">
<img src="padding_strides_transposed.gif" width="300"></div>
<p>Whereas using stride 2 in the conventional convolutional layer had the effect of sliding the weight matrix by 2 pixels, here, stride determines the dilation factor for the input feature map. For stride 2, a 1 pixel margin is introduced around each pixel. Thus, the input is up-scaled (weight and height double) and the convolution is applied, leading to a feature map with higher spatial resolution than the input. As before, we can apply this operation multiple times, to obtain a multi-channel output.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> conv_decoder<span class="br0">(</span>inputs<span class="sy0">,</span> num_filters<span class="sy0">,</span> output_shape<span class="sy0">,</span> scope<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span>scope<span class="sy0">,</span> <span class="st0">'decoder'</span><span class="sy0">,</span> <span class="br0">[</span>inputs<span class="br0">]</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">for</span> layer_id<span class="sy0">,</span> num_outputs <span class="kw1">in</span> <span class="kw2">enumerate</span><span class="br0">(</span>num_filters<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span><span class="st0">'block_{}'</span>.<span class="me1">format</span><span class="br0">(</span>layer_id<span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values<span class="sy0">=</span><span class="br0">(</span>net<span class="sy0">,</span><span class="br0">)</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d_transpose</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kernel_size<span class="sy0">=</span><span class="nu0">3</span><span class="sy0">,</span> stride<span class="sy0">=</span><span class="nu0">2</span><span class="sy0">,</span> padding<span class="sy0">=</span><span class="st0">'SAME'</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">with</span> tf.<span class="me1">variable_scope</span><span class="br0">(</span><span class="st0">'linear'</span><span class="sy0">,</span> values<span class="sy0">=</span><span class="br0">(</span>net<span class="sy0">,</span><span class="br0">)</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d_transpose</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> <span class="nu0">1</span><span class="sy0">,</span> activation_fn<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div>
<p>where this time <span class="geshifilter"><code class="text geshifilter-text">num_filters</code></span> is in decreasing order. The final transposed convolution is used to obtain a single-channel image (without non-linearity) and will be passed to the same loss function used in the fully-connected autoencoder.</p>
<p>There's one important aspect, we haven't considered yet. When the encoder takes an image of size 28x28 and outputs a low-dimensional feature map of size 4x4, which get's up-scaled three times, we end up with a reconstructed image of size 32x32, which is larger than the input image. We can simply solve this problem by cropping 2 pixels off each side of the image.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">shape <span class="sy0">=</span> tf.<span class="me1">shape</span><span class="br0">(</span>net<span class="br0">)</span>.<span class="me1">as_list</span><span class="br0">(</span><span class="br0">)</span>
output <span class="sy0">=</span> net<span class="br0">[</span>:<span class="sy0">,</span> <span class="nu0">2</span>:shape<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span> - <span class="nu0">2</span><span class="sy0">,</span> <span class="nu0">2</span>:shape<span class="br0">[</span><span class="nu0">2</span><span class="br0">]</span> - <span class="nu0">2</span><span class="sy0">,</span> :<span class="br0">]</span></pre></div></div><br>
&nbsp;
<h2>The model</h2>
<p>It is now straight-forward to construct the autoencoder model</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> conv_autoencoder<span class="br0">(</span>inputs<span class="sy0">,</span> num_filters<span class="sy0">,</span> activation_fn<span class="sy0">,</span> weight_decay<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; weights_init <span class="sy0">=</span> slim.<span class="me1">initializers</span>.<span class="me1">variance_scaling_initializer</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">if</span> weight_decay <span class="kw1">is</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp; &nbsp; <span class="kw1">else</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_reg <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">l2_regularizer</span><span class="br0">(</span>weight_decay<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">with</span> slim.<span class="me1">arg_scope</span><span class="br0">(</span><span class="br0">[</span>tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d</span><span class="sy0">,</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">conv2d_transpose</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_initializer<span class="sy0">=</span>weights_init<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer<span class="sy0">=</span>weights_reg<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span>activation_fn<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">reshape</span><span class="br0">(</span>inputs<span class="sy0">,</span> <span class="br0">[</span>-<span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">1</span><span class="br0">]</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> conv_encoder<span class="br0">(</span>net<span class="sy0">,</span> num_filters<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> conv_decoder<span class="br0">(</span>net<span class="sy0">,</span> num_filters<span class="br0">[</span>::-<span class="nu0">1</span><span class="br0">]</span><span class="sy0">,</span> <span class="br0">[</span>-<span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">28</span><span class="sy0">,</span> <span class="nu0">1</span><span class="br0">]</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">reshape</span><span class="br0">(</span>net<span class="sy0">,</span> <span class="br0">[</span>-<span class="nu0">1</span><span class="sy0">,</span> <span class="nu0">28</span> * <span class="nu0">28</span><span class="br0">]</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div>
<p>As before, the output is the input to <span class="geshifilter"><code class="text geshifilter-text">tf.losses.sigmoid_cross_entropy</code></span>, which is the loss function we want to minimize.</p>
<p>A convolutional autoencoder with 16 and two times 8 filters in the encoder and decoder has a mere 7873 weights and achieves a similar performance than the fully-connected auto-encoder with 222,384 weights (128, 64, and 32 nodes in encoder and decoder). The video below shows ten reconstructed images from the test data and their corresponding groundtruth after each epoch of training:<br>Your browser does not support the video tag.<br><br></p>
<h2>Visualizing the embedding</h2>
<p>Thanks to TensorBoard, we can also interactively visualize the low-dimensional embedding of our images, which looks something like the image below (click to see a larger version).</p>
<div align="center">
<a href="https://k-d-w.org/uploads/images/autoencoder/embedding.png"><img src="embedding_thumb.png"></a>
</div>
<p>There are some clusters that are relatively homogeneous, like the left one, which is predominantly composed of 1s, or the red cluster composed of 2s. On the other hand, the low-dimensional embedding struggles to distinguish between 5s and 3s. If we wanted to classify images, the low-dimensional representation would likely not yield great results. Of course, one could make the autoencoder deeper or increase the size of the low-dimensional embedding, which I encourage you to explore.</p>
<h2>Convolutions and data format</h2>
<p>If you are running the code on a GPU, there is a technical detail related to how convolutions are implemented and how images are represented in memory. In the code above, I assumed that the <em>last dimension</em> corresponds to the color channel, which is of size 1 for the input and corresponds to the number of feature maps otherwise. Thus, convolutions would operate on 4D Tensors of size $\text{batch size} \times \text{height} \times \text{width} \times \text{channels}$. This is TensorFlow's default format. Unfortunately, NVIDIA's cuDNN routines are optimized for a different data format, where the channel dimension comes before the spatial dimensions, i.e., tensors are of the format $\text{batch size} \times \text{channels} \times \text{height} \times \text{width}$. After reordering dimensions, you have to call <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d">tf.contrib.layers.conv2d</a> with the argument <span class="geshifilter"><code class="text geshifilter-text">data_format="NCHW"</code></span>, instead of the default <span class="geshifilter"><code class="text geshifilter-text">data_format="NHWC"</code></span>. The speed-up can be substantial, on a p2xlarge AWS instance, this increased the training speed from 27 iterations per second to 40 iterations per second. In my code, you just have to change <a href="https://github.com/sebp/tf_autoencoder/blob/master/tf_autoencoder/layers.py#L243">this line</a> to use the alternative data format.</p>
<p>I hope my code provides a starting point for convolutional autoencoders in TensorFlow. If you want to learn more about convolutional neural networks, check out the links at the bottom.</p>
<h2>References</h2>
<ul><li><a href="https://arxiv.org/abs/1512.07108">Recent Advances in Convolutional Neural Networks</a></li>
<li><a href="https://github.com/vdumoulin/conv_arithmetic">Technical report on convolution arithmetic</a></li>
<li><a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d">An Introduction to different Types of Convolutions in Deep Learning</a></li>
</ul></div></div></div>
</div>
<span class="field field-name-uid field-formatter-author field-type-entity-reference field-label-hidden"><span>sebp</span></span>
<span class="field field-name-created field-formatter-timestamp field-type-created field-label-hidden">Sun, 02/25/2018 - 16:07</span>
<a name="comments" id="comments"></a>
  <h1 class="begin-comments">Comments</h1></div>

<p class="date">
<a href="https://k-d-w.org/node/107">by sebp at February 25, 2018 03:07 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">February 21, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=567" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net – slomo's blog">Sebastian Dröge</a> — <a href="https://coaxion.net/blog/2018/02/how-to-write-gstreamer-elements-in-rust-part-2-a-raw-audio-sine-wave-source/">How to write GStreamer Elements in Rust Part 2: A raw audio sine wave source</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dröge)" width="80" height="80">
<p>A bit later than anticipated, this is now part two of the blog post series about writing <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> elements in <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a>. Part one can be found <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/" rel="noopener" target="_top">here</a>, and I’ll assume that everything written there is known already.</p>
<p>In this part, a raw audio sine wave source element is going to be written. It will be similar to the one Mathieu was writing in his <a href="https://mathieuduponchelle.github.io/2018-02-01-Python-Elements.html" rel="noopener" target="_top">blog post</a> about writing such a GStreamer element in Python. Various details will be different though, but more about that later.</p>
<p>The final code can be found <a href="https://github.com/sdroege/gst-plugin-rs/blob/0.1/gst-plugin-tutorial/src/sinesrc.rs" rel="noopener" target="_top">here</a>.</p>
<h3 id="toc">Table of Contents</h3>
<ol>
<li><a href="https://coaxion.net/blog/feed/#boilerplate">Boilerplate</a></li>
<li><a href="https://coaxion.net/blog/feed/#caps">Caps Negotiation</a></li>
<li><a href="https://coaxion.net/blog/feed/#query-handling">Query Handling</a></li>
<li><a href="https://coaxion.net/blog/feed/#buffer-creation">Buffer Creation</a></li>
<li><a href="https://coaxion.net/blog/feed/#live">(Pseudo) Live Mode</a></li>
<li><a href="https://coaxion.net/blog/feed/#unlock">Unlocking</a></li>
<li><a href="https://coaxion.net/blog/feed/#seeking">Seeking</a></li>
</ol>
<h3 id="boilerplate">Boilerplate</h3>
<p>The first part here will be all the boilerplate required to set up the element. You can safely <a href="https://coaxion.net/blog/feed/#caps">skip</a> this if you remember all this from the previous blog post.</p>
<p>Our sine wave element is going to produce raw audio, with a number of channels and any possible sample rate with both 32 bit and 64 bit floating point samples. It will produce a simple sine wave with a configurable frequency, volume/mute and number of samples per audio buffer. In addition it will be possible to configure the element in (pseudo) live mode, meaning that it will only produce data in real-time according to the pipeline clock. And it will be possible to seek to any time/sample position on our source element. It will basically be a more simply version of the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-plugins/html/gst-plugins-base-plugins-audiotestsrc.html" rel="noopener" target="_top">audiotestsrc</a> element from gst-plugins-base.</p>
<p>So let’s get started with all the boilerplate. This time our element will be based on the <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseSrc.html" rel="noopener" target="_top">BaseSrc</a> base class instead of <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseTransform.html" rel="noopener" target="_top">BaseTransform</a>.</p>
<p></p><pre class="crayon-plain-tag">use glib;
use gst;
use gst::prelude::*;
use gst_base::prelude::*;
use gst_audio;

use byte_slice_cast::*;

use gst_plugin::properties::*;
use gst_plugin::object::*;
use gst_plugin::element::*;
use gst_plugin::base_src::*;

use std::{i32, u32};
use std::sync::Mutex;
use std::ops::Rem;

use num_traits::float::Float;
use num_traits::cast::NumCast;

// Default values of properties
const DEFAULT_SAMPLES_PER_BUFFER: u32 = 1024;
const DEFAULT_FREQ: u32 = 440;
const DEFAULT_VOLUME: f64 = 0.8;
const DEFAULT_MUTE: bool = false;
const DEFAULT_IS_LIVE: bool = false;

// Property value storage
#[derive(Debug, Clone, Copy)]
struct Settings {
    samples_per_buffer: u32,
    freq: u32,
    volume: f64,
    mute: bool,
    is_live: bool,
}

impl Default for Settings {
    fn default() -&gt; Self {
        Settings {
            samples_per_buffer: DEFAULT_SAMPLES_PER_BUFFER,
            freq: DEFAULT_FREQ,
            volume: DEFAULT_VOLUME,
            mute: DEFAULT_MUTE,
            is_live: DEFAULT_IS_LIVE,
        }
    }
}

// Metadata for the properties
static PROPERTIES: [Property; 5] = [
    Property::UInt(
        "samples-per-buffer",
        "Samples Per Buffer",
        "Number of samples per output buffer",
        (1, u32::MAX),
        DEFAULT_SAMPLES_PER_BUFFER,
        PropertyMutability::ReadWrite,
    ),
    Property::UInt(
        "freq",
        "Frequency",
        "Frequency",
        (1, u32::MAX),
        DEFAULT_FREQ,
        PropertyMutability::ReadWrite,
    ),
    Property::Double(
        "volume",
        "Volume",
        "Output volume",
        (0.0, 10.0),
        DEFAULT_VOLUME,
        PropertyMutability::ReadWrite,
    ),
    Property::Boolean(
        "mute",
        "Mute",
        "Mute",
        DEFAULT_MUTE,
        PropertyMutability::ReadWrite,
    ),
    Property::Boolean(
        "is-live",
        "Is Live",
        "(Pseudo) live output",
        DEFAULT_IS_LIVE,
        PropertyMutability::ReadWrite,
    ),
];

// Stream-specific state, i.e. audio format configuration
// and sample offset
struct State {
    info: Option,
    sample_offset: u64,
    sample_stop: Option,
    accumulator: f64,
}

impl Default for State {
    fn default() -&gt; State {
        State {
            info: None,
            sample_offset: 0,
            sample_stop: None,
            accumulator: 0.0,
        }
    }
}

// Struct containing all the element data
struct SineSrc {
    cat: gst::DebugCategory,
    settings: Mutex,
    state: Mutex,
}

impl SineSrc {
    // Called when a new instance is to be created
    fn new(element: &amp;BaseSrc) -&gt; Box&gt; {
        // Initialize live-ness and notify the base class that
        // we'd like to operate in Time format
        element.set_live(DEFAULT_IS_LIVE);
        element.set_format(gst::Format::Time);

        Box::new(Self {
            cat: gst::DebugCategory::new(
                "rssinesrc",
                gst::DebugColorFlags::empty(),
                "Rust Sine Wave Source",
            ),
            settings: Mutex::new(Default::default()),
            state: Mutex::new(Default::default()),
        })
    }

    // Called exactly once when registering the type. Used for
    // setting up metadata for all instances, e.g. the name and
    // classification and the pad templates with their caps.
    //
    // Actual instances can create pads based on those pad templates
    // with a subset of the caps given here. In case of basesrc,
    // a "src" and "sink" pad template are required here and the base class
    // will automatically instantiate pads for them.
    //
    // Our element here can output f32 and f64
    fn class_init(klass: &amp;mut BaseSrcClass) {
        klass.set_metadata(
            "Sine Wave Source",
            "Source/Audio",
            "Creates a sine wave",
            "Sebastian Dröge ",
        );

        // On the src pad, we can produce F32/F64 with any sample rate
        // and any number of channels
        let caps = gst::Caps::new_simple(
            "audio/x-raw",
            &amp;[
                (
                    "format",
                    &amp;gst::List::new(&amp;[
                        &amp;gst_audio::AUDIO_FORMAT_F32.to_string(),
                        &amp;gst_audio::AUDIO_FORMAT_F64.to_string(),
                    ]),
                ),
                ("layout", &amp;"interleaved"),
                ("rate", &amp;gst::IntRange::::new(1, i32::MAX)),
                ("channels", &amp;gst::IntRange::::new(1, i32::MAX)),
            ],
        );
        // The src pad template must be named "src" for basesrc
        // and specific a pad that is always there
        let src_pad_template = gst::PadTemplate::new(
            "src",
            gst::PadDirection::Src,
            gst::PadPresence::Always,
            &amp;caps,
        );
        klass.add_pad_template(src_pad_template);

        // Install all our properties
        klass.install_properties(&amp;PROPERTIES);
    }
}

impl ObjectImpl for SineSrc {
    // Called whenever a value of a property is changed. It can be called
    // at any time from any thread.
    fn set_property(&amp;self, obj: &amp;glib::Object, id: u32, value: &amp;glib::Value) {
        let prop = &amp;PROPERTIES[id as usize];
        let element = obj.clone().downcast::().unwrap();

        match *prop {
            Property::UInt("samples-per-buffer", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let samples_per_buffer = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing samples-per-buffer from {} to {}",
                    settings.samples_per_buffer,
                    samples_per_buffer
                );
                settings.samples_per_buffer = samples_per_buffer;
                drop(settings);

                let _ =
                    element.post_message(&amp;gst::Message::new_latency().src(Some(&amp;element)).build());
            }
            Property::UInt("freq", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let freq = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing freq from {} to {}",
                    settings.freq,
                    freq
                );
                settings.freq = freq;
            }
            Property::Double("volume", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let volume = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing volume from {} to {}",
                    settings.volume,
                    volume
                );
                settings.volume = volume;
            }
            Property::Boolean("mute", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let mute = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing mute from {} to {}",
                    settings.mute,
                    mute
                );
                settings.mute = mute;
            }
            Property::Boolean("is-live", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let is_live = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing is-live from {} to {}",
                    settings.is_live,
                    is_live
                );
                settings.is_live = is_live;
            }
            _ =&gt; unimplemented!(),
        }
    }

    // Called whenever a value of a property is read. It can be called
    // at any time from any thread.
    fn get_property(&amp;self, _obj: &amp;glib::Object, id: u32) -&gt; Result {
        let prop = &amp;PROPERTIES[id as usize];

        match *prop {
            Property::UInt("samples-per-buffer", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.samples_per_buffer.to_value())
            }
            Property::UInt("freq", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.freq.to_value())
            }
            Property::Double("volume", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.volume.to_value())
            }
            Property::Boolean("mute", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.mute.to_value())
            }
            Property::Boolean("is-live", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.is_live.to_value())
            }
            _ =&gt; unimplemented!(),
        }
    }
}

// Virtual methods of gst::Element. We override none
impl ElementImpl for SineSrc { }

impl BaseSrcImpl for SineSrc {
    // Called when starting, so we can initialize all stream-related state to its defaults
    fn start(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // Reset state
        *self.state.lock().unwrap() = Default::default();

        gst_info!(self.cat, obj: element, "Started");

        true
    }

    // Called when shutting down the element so we can release all stream-related state
    fn stop(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // Reset state
        *self.state.lock().unwrap() = Default::default();

        gst_info!(self.cat, obj: element, "Stopped");

        true
    }
}

struct SineSrcStatic;

// The basic trait for registering the type: This returns a name for the type and registers the
// instance and class initializations functions with the type system, thus hooking everything
// together.
impl ImplTypeStatic for SineSrcStatic {
    fn get_name(&amp;self) -&gt; &amp;str {
        "SineSrc"
    }

    fn new(&amp;self, element: &amp;BaseSrc) -&gt; Box&gt; {
        SineSrc::new(element)
    }

    fn class_init(&amp;self, klass: &amp;mut BaseSrcClass) {
        SineSrc::class_init(klass);
    }
}

// Registers the type for our element, and then registers in GStreamer under
// the name "sinesrc" for being able to instantiate it via e.g.
// gst::ElementFactory::make().
pub fn register(plugin: &amp;gst::Plugin) {
    let type_ = register_type(SineSrcStatic);
    gst::Element::register(plugin, "rssinesrc", 0, type_);
}</pre><p></p>
<p>If any of this needs explanation, please see the <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/" rel="noopener" target="_top">previous</a> blog post and the comments in the code. The explanation for all the structs fields and what they’re good for will follow in the next sections.</p>
<p>With all of the above and a small addition to <i>src/lib.rs</i> this should compile now.</p>
<p></p><pre class="crayon-plain-tag">mod sinesrc;
[...]

fn plugin_init(plugin: &amp;gst::Plugin) -&gt; bool {
    [...]
    sinesrc::register(plugin);
    true
}</pre><p></p>
<p>Also a couple of new crates have to be added to <i>Cargo.toml</i> and <i>src/lib.rs</i>, but you best check the code in the <a href="https://github.com/sdroege/gst-plugin-rs/tree/0.1/gst-plugin-tutorial" rel="noopener" target="_top">repository</a> for details.</p>
<h3 id="caps">Caps Negotiation</h3>
<p>The first part that we have to implement, just like last time, is caps negotiation. We already notified the base class about any caps that we can potentially handle via the caps in the pad template in <i>class_init</i> but there are still two more steps of behaviour left that we have to implement.</p>
<p>First of all, we need to get notified whenever the caps that our source is configured for are changing. This will happen once in the very beginning and then whenever the pipeline topology or state changes and new caps would be more optimal for the new situation. This notification happens via the <i>BaseTransform::set_caps</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn set_caps(&amp;self, element: &amp;BaseSrc, caps: &amp;gst::CapsRef) -&gt; bool {
        use std::f64::consts::PI;

        let info = match gst_audio::AudioInfo::from_caps(caps) {
            None =&gt; return false,
            Some(info) =&gt; info,
        };

        gst_debug!(self.cat, obj: element, "Configuring for caps {}", caps);

        element.set_blocksize(info.bpf() * (*self.settings.lock().unwrap()).samples_per_buffer);

        let settings = *self.settings.lock().unwrap();
        let mut state = self.state.lock().unwrap();

        // If we have no caps yet, any old sample_offset and sample_stop will be
        // in nanoseconds
        let old_rate = match state.info {
            Some(ref info) =&gt; info.rate() as u64,
            None =&gt; gst::SECOND_VAL,
        };

        // Update sample offset and accumulator based on the previous values and the
        // sample rate change, if any
        let old_sample_offset = state.sample_offset;
        let sample_offset = old_sample_offset
            .mul_div_floor(info.rate() as u64, old_rate)
            .unwrap();

        let old_sample_stop = state.sample_stop;
        let sample_stop =
            old_sample_stop.map(|v| v.mul_div_floor(info.rate() as u64, old_rate).unwrap());

        let accumulator =
            (sample_offset as f64).rem(2.0 * PI * (settings.freq as f64) / (info.rate() as f64));

        *state = State {
            info: Some(info),
            sample_offset: sample_offset,
            sample_stop: sample_stop,
            accumulator: accumulator,
        };

        drop(state);

        let _ = element.post_message(&amp;gst::Message::new_latency().src(Some(element)).build());

        true
    }</pre><p></p>
<p>In here we parse the caps into a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_audio/struct.AudioInfo.html" rel="noopener" target="_top"><i>AudioInfo</i></a> and then store that in our internal state, while updating various fields. We tell the base class about the number of bytes each buffer is usually going to hold, and update our current sample position, the stop sample position (when a seek with stop position happens, we need to know when to stop) and our accumulator. This happens by scaling both positions by the old and new sample rate. If we don’t have an old sample rate, we assume nanoseconds (this will make more sense once seeking is implemented). The scaling is done with the help of the <a href="https://crates.io/crates/muldiv" rel="noopener" target="_top"><i>muldiv</i></a> crate, which implements scaling of integer types by a fraction with protection against overflows by doing up to 128 bit integer arithmetic for intermediate values.</p>
<p>The accumulator is the updated based on the current phase of the sine wave at the current sample position.</p>
<p>As a last step we post a new <i>LATENCY</i> message on the bus whenever the sample rate has changed. Our latency (in live mode) is going to be the duration of a single buffer, but more about that later.</p>
<p>BaseSrc is by default already selecting possible caps for us, if there are multiple options. However these defaults might not be (and often are not) ideal and we should override the default behaviour slightly. This is done in the <i>BaseSrc::fixate</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn fixate(&amp;self, element: &amp;BaseSrc, caps: gst::Caps) -&gt; gst::Caps {
        // Fixate the caps. BaseSrc will do some fixation for us, but
        // as we allow any rate between 1 and MAX it would fixate to 1. 1Hz
        // is generally not a useful sample rate.
        //
        // We fixate to the closest integer value to 48kHz that is possible
        // here, and for good measure also decide that the closest value to 1
        // channel is good.
        let mut caps = gst::Caps::truncate(caps);
        {
            let caps = caps.make_mut();
            let s = caps.get_mut_structure(0).unwrap();
            s.fixate_field_nearest_int("rate", 48_000);
            s.fixate_field_nearest_int("channels", 1);
        }

        // Let BaseSrc fixate anything else for us. We could've alternatively have
        // called Caps::fixate() here
        element.parent_fixate(caps)
    }</pre><p></p>
<p>Here we take the caps that are passed in, truncate them (i.e. remove all but the very first <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/structure/struct.Structure.html" rel="noopener" target="_top"><i>Structure</i></a>) and then manually fixate the sample rate to the closest value to 48kHz. By default, caps fixation would result in the lowest possible sample rate but this is usually not desired.</p>
<p>For good measure, we also fixate the number of channels to the closest value to 1, but this would already be the default behaviour anyway. And then chain up to the parent class’ implementation of <i>fixate</i>, which for now basically does the same as <i>Caps::fixate()</i>. After this, the caps are fixated, i.e. there is only a single <i>Structure</i> left and all fields have concrete values (no ranges or sets).</p>
<h3 id="query-handling">Query Handling</h3>
<p>As our source element will work by generating a new audio buffer from a specific offset, and especially works in <i>Time</i> format, we want to notify downstream elements that we don’t want to run in <i>Pull</i> mode, only in <i>Push</i> mode. In addition would prefer sequential reading. However we still allow seeking later. For a source that does not know about <i>Time</i>, e.g. a file source, the format would be configured as <i>Bytes</i>. Other values than <i>Time</i> and <i>Bytes</i> generally don’t make any sense.</p>
<p>The main difference here is that otherwise the base class would ask us to produce data for arbitrary <i>Byte</i> offsets, and we would have to produce data for that. While possible in our case, it’s a bit annoying and for other audio sources it’s not easily possible at all.</p>
<p>Downstream elements will try to query this very information from us, so we now have to override the default query handling of <i>BaseSrc</i> and handle the <i>SCHEDULING</i> query differently. Later we will also handle other queries differently.</p>
<p></p><pre class="crayon-plain-tag">fn query(&amp;self, element: &amp;BaseSrc, query: &amp;mut gst::QueryRef) -&gt; bool {
        use gst::QueryView;

        match query.view_mut() {
            // We only work in Push mode. In Pull mode, create() could be called with
            // arbitrary offsets and we would have to produce for that specific offset
            QueryView::Scheduling(ref mut q) =&gt; {
                q.set(gst::SchedulingFlags::SEQUENTIAL, 1, -1, 0);
                q.add_scheduling_modes(&amp;[gst::PadMode::Push]);
                return true;
            }
            _ =&gt; (),
        }
        BaseSrcBase::parent_query(element, query)
    }</pre><p></p>
<p>To handle the <i>SCHEDULING</i> query specifically, we first have to match on a view (mutable because we want to modify the view) of the query check the type of the query. If it indeed is a scheduling query, we can set the <i>SEQUENTIAL</i> flag and specify that we handle only <i>Push</i> mode, then return <i>true</i> directly as we handled the query already.</p>
<p>In all other cases we fall back to the parent class’ implementation of the <i>query</i> virtual method.</p>
<h3 id="buffer-creation">Buffer Creation</h3>
<p>Now we have everything in place for a working element, apart from the virtual method to actually generate the raw audio buffers with the sine wave. From a high-level <i>BaseSrc</i> works by calling the <i>create</i> virtual method over and over again to let the subclass produce a buffer until it returns an error or signals the end of the stream.</p>
<p>Let’s first talk about how to generate the sine wave samples themselves. As we want to operate on 32 bit and 64 bit floating point numbers, we implement a generic function for generating samples and storing them in a mutable byte slice. This is done with the help of the <a href="https://crates.io/crates/num-traits" rel="noopener" target="_top"><i>num_traits</i></a> crate, which provides all kinds of useful traits for abstracting over numeric types. In our case we only need the <a href="https://docs.rs/num-traits/0.2.0/num_traits/float/trait.Float.html" rel="noopener" target="_top"><i>Float</i></a> and <a href="https://docs.rs/num-traits/0.2.0/num_traits/cast/trait.NumCast.html" rel="noopener" target="_top"><i>NumCast</i></a> traits.</p>
<p>Instead of writing a generic implementation with those traits, it would also be possible to do the same with a simple macro that generates a function for both types. Which approach is nicer is a matter of taste in the end, the compiler output should be equivalent for both cases.</p>
<p></p><pre class="crayon-plain-tag">fn process(
        data: &amp;mut [u8],
        accumulator_ref: &amp;mut f64,
        freq: u32,
        rate: u32,
        channels: u32,
        vol: f64,
    ) {
        use std::f64::consts::PI;

        // Reinterpret our byte-slice as a slice containing elements of the type
        // we're interested in. GStreamer requires for raw audio that the alignment
        // of memory is correct, so this will never ever fail unless there is an
        // actual bug elsewhere.
        let data = data.as_mut_slice_of::().unwrap();

        // Convert all our parameters to the target type for calculations
        let vol: F = NumCast::from(vol).unwrap();
        let freq = freq as f64;
        let rate = rate as f64;
        let two_pi = 2.0 * PI;

        // We're carrying a accumulator with up to 2pi around instead of working
        // on the sample offset. High sample offsets cause too much inaccuracy when
        // converted to floating point numbers and then iterated over in 1-steps
        let mut accumulator = *accumulator_ref;
        let step = two_pi * freq / rate;

        for chunk in data.chunks_mut(channels as usize) {
            let value = vol * F::sin(NumCast::from(accumulator).unwrap());
            for sample in chunk {
                *sample = value;
            }

            accumulator += step;
            if accumulator &gt;= two_pi {
                accumulator -= two_pi;
            }
        }

        *accumulator_ref = accumulator;
    }</pre><p></p>
<p>This function takes the mutable byte slice from our buffer as argument, as well as the current value of the accumulator and the relevant settings for generating the sine wave.</p>
<p>As a first step, we “cast” the byte slice to one of the target type (f32 or f64) with the help of the <a href="https://crates.io/crates/byte-slice-cast" rel="noopener" target="_top"><i>byte_slice_cast</i></a> crate. This ensures that alignment and sizes are all matching and returns a mutable slice of our target type if successful. In case of GStreamer, the buffer alignment is guaranteed to be big enough for our types here and we allocate the buffer of a correct size later.</p>
<p>Now we convert all the parameters to the types we will use later, and store them together with the current accumulator value in local variables. Then we iterate over the whole floating point number slice in chunks with all channels, and fill each channel with the current value of our sine wave.</p>
<p>The sine wave itself is calculated by <i>val = volume * sin(2 * PI * frequency * (i + accumulator) / rate)</i>, but we actually calculate it by simply increasing the accumulator by <i>2 * PI * frequency / rate</i> for every sample instead of doing the multiplication for each sample. We also make sure that the accumulator always stays between <i>0</i> and <i>2 * PI</i> to prevent any inaccuracies from floating point numbers to affect our produced samples.</p>
<p>Now that this is done, we need to implement the <i>BaseSrc::create</i> virtual method for actually allocating the buffer, setting timestamps and other metadata and it and calling our above function.</p>
<p></p><pre class="crayon-plain-tag">fn create(
        &amp;self,
        element: &amp;BaseSrc,
        _offset: u64,
        _length: u32,
    ) -&gt; Result {
        // Keep a local copy of the values of all our properties at this very moment. This
        // ensures that the mutex is never locked for long and the application wouldn't
        // have to block until this function returns when getting/setting property values
        let settings = *self.settings.lock().unwrap();

        // Get a locked reference to our state, i.e. the input and output AudioInfo
        let mut state = self.state.lock().unwrap();
        let info = match state.info {
            None =&gt; {
                gst_element_error!(element, gst::CoreError::Negotiation, ["Have no caps yet"]);
                return Err(gst::FlowReturn::NotNegotiated);
            }
            Some(ref info) =&gt; info.clone(),
        };

        // If a stop position is set (from a seek), only produce samples up to that
        // point but at most samples_per_buffer samples per buffer
        let n_samples = if let Some(sample_stop) = state.sample_stop {
            if sample_stop = state.sample_offset {
                gst_log!(self.cat, obj: element, "At EOS");
                return Err(gst::FlowReturn::Eos);
            }

            sample_stop - state.sample_offset
        } else {
            settings.samples_per_buffer as u64
        };

        // Allocate a new buffer of the required size, update the metadata with the
        // current timestamp and duration and then fill it according to the current
        // caps
        let mut buffer =
            gst::Buffer::with_size((n_samples as usize) * (info.bpf() as usize)).unwrap();
        {
            let buffer = buffer.get_mut().unwrap();

            // Calculate the current timestamp (PTS) and the next one,
            // and calculate the duration from the difference instead of
            // simply the number of samples to prevent rounding errors
            let pts = state
                .sample_offset
                .mul_div_floor(gst::SECOND_VAL, info.rate() as u64)
                .unwrap()
                .into();
            let next_pts: gst::ClockTime = (state.sample_offset + n_samples)
                .mul_div_floor(gst::SECOND_VAL, info.rate() as u64)
                .unwrap()
                .into();
            buffer.set_pts(pts);
            buffer.set_duration(next_pts - pts);

            // Map the buffer writable and create the actual samples
            let mut map = buffer.map_writable().unwrap();
            let data = map.as_mut_slice();

            if info.format() == gst_audio::AUDIO_FORMAT_F32 {
                Self::process::(
                    data,
                    &amp;mut state.accumulator,
                    settings.freq,
                    info.rate(),
                    info.channels(),
                    settings.volume,
                );
            } else {
                Self::process::(
                    data,
                    &amp;mut state.accumulator,
                    settings.freq,
                    info.rate(),
                    info.channels(),
                    settings.volume,
                );
            }
        }
        state.sample_offset += n_samples;
        drop(state);

        gst_debug!(self.cat, obj: element, "Produced buffer {:?}", buffer);

        Ok(buffer)
    }</pre><p></p>
<p>Just like last time, we start with creating a copy of our properties (settings) and keeping a mutex guard of the internal state around. If the internal state has no <i>AudioInfo</i> yet, we error out. This would mean that no caps were negotiated yet, which is something we can’t handle and is not really possible in our case.</p>
<p>Next we calculate how many samples we have to generate. If a sample stop position was set by a seek event, we have to generate samples up to at most that point. Otherwise we create at most the number of samples per buffer that were set via the property. Then we allocate a buffer of the corresponding size, with the help of the <i>bpf</i> field of the <i>AudioInfo</i>, and then set its metadata and fill the samples.</p>
<p>The metadata that is set is the timestamp (PTS), and the duration. The duration is calculated from the difference of the following buffer’s timestamp and the current buffer’s. By this we ensure that rounding errors are not causing the next buffer’s timestamp to have a different timestamp than the sum of the current’s and its duration. While this would not be much of a problem in GStreamer (inaccurate and jitterish timestamps are handled just fine), we can prevent it here and do so.</p>
<p>Afterwards we call our previously defined function on the writably mapped buffer and fill it with the sample values.</p>
<p>With all this, the element should already work just fine in any GStreamer-based application, for example <i>gst-launch-1.0</i>. Don’t forget to set the <i>GST_PLUGIN_PATH</i> environment variable correctly like last time. Before running this, make sure to turn down the volume of your speakers/headphones a bit.</p>
<p></p><pre class="crayon-plain-tag">export GST_PLUGIN_PATH=`pwd`/target/debug
gst-launch-1.0 rssinesrc freq=440 volume=0.9 ! audioconvert ! autoaudiosink</pre><p></p>
<p>You should hear a 440Hz sine wave now.</p>
<h3 id="live">(Pseudo) Live Mode</h3>
<p>Many audio (and video) sources can actually only produce data in real-time and data is produced according to some clock. So far our source element can produce data as fast as downstream is consuming data, but we optionally can change that. We simulate a live source here now by waiting on the pipeline clock, but with a real live source you would only ever be able to have the data in real-time without any need to wait on a clock. And usually that data is produced according to a different clock than the pipeline clock, in which case translation between the two clocks is needed but we ignore this aspect for now. For details check the <a href="https://gstreamer.freedesktop.org/documentation/application-development/advanced/clocks.html" rel="noopener" target="_top">GStreamer documentation</a>.</p>
<p>For working in live mode, we have to add a few different parts in various places. First of all, we implement waiting on the clock in the <i>create</i> function.</p>
<p></p><pre class="crayon-plain-tag">fn create(...
        [...]
        state.sample_offset += n_samples;
        drop(state);

        // If we're live, we are waiting until the time of the last sample in our buffer has
        // arrived. This is the very reason why we have to report that much latency.
        // A real live-source would of course only allow us to have the data available after
        // that latency, e.g. when capturing from a microphone, and no waiting from our side
        // would be necessary..
        //
        // Waiting happens based on the pipeline clock, which means that a real live source
        // with its own clock would require various translations between the two clocks.
        // This is out of scope for the tutorial though.
        if element.is_live() {
            let clock = match element.get_clock() {
                None =&gt; return Ok(buffer),
                Some(clock) =&gt; clock,
            };

            let segment = element
                .get_segment()
                .downcast::()
                .unwrap();
            let base_time = element.get_base_time();
            let running_time = segment.to_running_time(buffer.get_pts() + buffer.get_duration());

            // The last sample's clock time is the base time of the element plus the
            // running time of the last sample
            let wait_until = running_time + base_time;
            if wait_until.is_none() {
                return Ok(buffer);
            }

            let id = clock.new_single_shot_id(wait_until).unwrap();

            gst_log!(
                self.cat,
                obj: element,
                "Waiting until {}, now {}",
                wait_until,
                clock.get_time()
            );
            let (res, jitter) = id.wait();
            gst_log!(
                self.cat,
                obj: element,
                "Waited res {:?} jitter {}",
                res,
                jitter
            );
        }

        gst_debug!(self.cat, obj: element, "Produced buffer {:?}", buffer);

        Ok(buffer)
    }</pre><p></p>
<p>To be able to wait on the clock, we first of all need to calculate the clock time until when we want to wait. In our case that will be the clock time right after the end of the last sample in the buffer we just produced. Simply because you can’t capture a sample before it was produced.</p>
<p>We calculate the running time from the PTS and duration of the buffer with the help of the currently configured segment and then add the base time of the element on this to get the clock time as result. Please check the <a href="https://gstreamer.freedesktop.org/documentation/application-development/advanced/clocks.html" rel="noopener" target="_top">GStreamer documentation</a> for details, but in short the running time of a pipeline is the time since the start of the pipeline (or the last reset of the running time) and the running time of a buffer can be calculated from its PTS and the segment, which provides the information to translate between the two. The base time is the clock time when the pipeline went to the <i>Playing</i> state, so just an offset.</p>
<p>Next we wait and then return the buffer as before.</p>
<p>Now we also have to tell the base class that we’re running in live mode now. This is done by calling <i>set_live(true)</i> on the base class before changing the element state from <i>Ready</i> to <i>Paused</i>. For this we override the <i>Element::change_state</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">impl ElementImpl for SineSrc {
    fn change_state(
        &amp;self,
        element: &amp;BaseSrc,
        transition: gst::StateChange,
    ) -&gt; gst::StateChangeReturn {
        // Configure live'ness once here just before starting the source
        match transition {
            gst::StateChange::ReadyToPaused =&gt; {
                element.set_live(self.settings.lock().unwrap().is_live);
            }
            _ =&gt; (),
        }

        element.parent_change_state(transition)
    }
}</pre><p></p>
<p>And as a last step, we also need to notify downstream elements about our <a href="https://gstreamer.freedesktop.org/documentation/application-development/advanced/clocks.html#latency" rel="noopener" target="_top">latency</a>. Live elements always have to report their latency so that synchronization can work correctly. As the clock time of each buffer is equal to the time when it was created, all buffers would otherwise arrive late in the sinks (they would appear as if they should’ve been played already at the time when they were created). So all the sinks will have to compensate for the latency that it took from capturing to the sink, and they have to do that in a coordinated way (otherwise audio and video would be out of sync if both have different latencies). For this the pipeline is querying each sink for the latency on its own branch, and then configures a global latency on all sinks according to that.</p>
<p>This querying is done with the <i>LATENCY</i> query, which we will now also have to handle.</p>
<p></p><pre class="crayon-plain-tag">fn query(&amp;self, element: &amp;BaseSrc, query: &amp;mut gst::QueryRef) -&gt; bool {
        use gst::QueryView;

        match query.view_mut() {
            // We only work in Push mode. In Pull mode, create() could be called with
            // arbitrary offsets and we would have to produce for that specific offset
            QueryView::Scheduling(ref mut q) =&gt; {
                [...]
            }
            // In Live mode we will have a latency equal to the number of samples in each buffer.
            // We can't output samples before they were produced, and the last sample of a buffer
            // is produced that much after the beginning, leading to this latency calculation
            QueryView::Latency(ref mut q) =&gt; {
                let settings = *self.settings.lock().unwrap();
                let state = self.state.lock().unwrap();

                if let Some(ref info) = state.info {
                    let latency = gst::SECOND
                        .mul_div_floor(settings.samples_per_buffer as u64, info.rate() as u64)
                        .unwrap();
                    gst_debug!(self.cat, obj: element, "Returning latency {}", latency);
                    q.set(settings.is_live, latency, gst::CLOCK_TIME_NONE);
                    return true;
                } else {
                    return false;
                }
            }
            _ =&gt; (),
        }
        BaseSrcBase::parent_query(element, query)
    }</pre><p></p>
<p>The latency that we report is the duration of a single audio buffer, because we’re simulating a real live source here. A real live source won’t be able to output the buffer before the last sample of it is captured, and the difference between when the first and last sample were captured is exactly the latency that we add here. Other elements further downstream that introduce further latency would then add their own latency on top of this.</p>
<p>Inside the latency query we also signal that we are indeed a live source, and additionally how much buffering we can do (in our case, infinite) until data would be lost. The last part is important if e.g. the video branch has a higher latency, causing the audio sink to have to wait some additional time (so that audio and video stay in sync), which would then require the whole audio branch to buffer some data. As we have an artificial live source, we can always generate data for the next time but a real live source would only have a limited buffer and if no data is read and forwarded once that runs full, data would get lost.</p>
<p>You can test this again with e.g. <i>gst-launch-1.0</i> by setting the <i>is-live</i> property to true. It should write in the output now that the pipeline is live.</p>
<p>In Mathieu’s blog post this was implemented without explicit waiting and the usage of the <i>get_times</i> virtual method, but as this is only really useful for pseudo live sources like this one I decided to explain how waiting on the clock can be achieved correctly and even more important how that relates to the next section.</p>
<h3 id="unlock">Unlocking</h3>
<p>With the addition of the live mode, the <i>create</i> function is now blocking and waiting on the clock for some time. This is suboptimal as for example a (flushing) seek would have to wait now until the clock waiting is done, or when shutting down the application would have to wait.</p>
<p>To prevent this, all waiting/blocking in GStreamer streaming threads should be interruptible/cancellable when requested. And for example the <i>ClockID</i> that we got from the clock for waiting can be cancelled by calling <i>unschedule()</i> on it. We only have to do it from the right place and keep it accessible. The right place is the <i>BaseSrc::unlock</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">struct ClockWait {
    clock_id: Option,
    flushing: bool,
}

struct SineSrc {
    cat: gst::DebugCategory,
    settings: Mutex,
    state: Mutex,
    clock_wait: Mutex,
}

[...]

    fn unlock(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // This should unblock the create() function ASAP, so we
        // just unschedule the clock it here, if any.
        gst_debug!(self.cat, obj: element, "Unlocking");
        let mut clock_wait = self.clock_wait.lock().unwrap();
        if let Some(clock_id) = clock_wait.clock_id.take() {
            clock_id.unschedule();
        }
        clock_wait.flushing = true;

        true
    }</pre><p></p>
<p>We store the clock ID in our struct, together with a boolean to signal whether we’re supposed to flush already or not. And then inside <i>unlock</i> unschedule the clock ID and set this boolean flag to true.</p>
<p>Once everything is unlocked, we need to reset things again so that data flow can happen in the future. This is done in the <i>unlock_stop</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn unlock_stop(&amp;self, element: &amp;BaseSrc) -&gt; bool {
        // This signals that unlocking is done, so we can reset
        // all values again.
        gst_debug!(self.cat, obj: element, "Unlock stop");
        let mut clock_wait = self.clock_wait.lock().unwrap();
        clock_wait.flushing = false;

        true
    }</pre><p></p>
<p>To make sure that this struct is always initialized correctly, we also call <i>unlock</i> from <i>stop</i>, and <i>unlock_stop</i> from <i>start</i>.</p>
<p>Now as a last step, we need to actually make use of the new struct we added around the code where we wait for the clock.</p>
<p></p><pre class="crayon-plain-tag">// Store the clock ID in our struct unless we're flushing anyway.
            // This allows to asynchronously cancel the waiting from unlock()
            // so that we immediately stop waiting on e.g. shutdown.
            let mut clock_wait = self.clock_wait.lock().unwrap();
            if clock_wait.flushing {
                gst_debug!(self.cat, obj: element, "Flushing");
                return Err(gst::FlowReturn::Flushing);
            }

            let id = clock.new_single_shot_id(wait_until).unwrap();
            clock_wait.clock_id = Some(id.clone());
            drop(clock_wait);

            gst_log!(
                self.cat,
                obj: element,
                "Waiting until {}, now {}",
                wait_until,
                clock.get_time()
            );
            let (res, jitter) = id.wait();
            gst_log!(
                self.cat,
                obj: element,
                "Waited res {:?} jitter {}",
                res,
                jitter
            );
            self.clock_wait.lock().unwrap().clock_id.take();

            // If the clock ID was unscheduled, unlock() was called
            // and we should return Flushing immediately.
            if res == gst::ClockReturn::Unscheduled {
                gst_debug!(self.cat, obj: element, "Flushing");
                return Err(gst::FlowReturn::Flushing);
            }</pre><p></p>
<p>The important part in this code is that we first have to check if we are already supposed to unlock, before even starting to wait. Otherwise we would start waiting without anybody ever being able to unlock. Then we need to store the clock id in the struct and make sure to drop the mutex guard so that the <i>unlock</i> function can take it again for unscheduling the clock ID. And once waiting is done, we need to remove the clock id from the struct again and in case of <i>ClockReturn::Unscheduled</i> we directly return <i>FlowReturn::Flushing</i> instead of the error.</p>
<p>Similarly when using other blocking APIs it is important that they are woken up in a similar way when <i>unlock</i> is called. Otherwise the application developer’s and thus user experience will be far from ideal.</p>
<h3 id="seeking">Seeking</h3>
<p>As a last feature we implement seeking on our source element. In our case that only means that we have to update the <i>sample_offset</i> and <i>sample_stop</i> fields accordingly, other sources might have to do more work than that.</p>
<p>Seeking is implemented in the <i>BaseSrc::do_seek</i> virtual method, and signalling whether we can actually seek in the <i>is_seekable</i> virtual method.</p>
<p></p><pre class="crayon-plain-tag">fn is_seekable(&amp;self, _element: &amp;BaseSrc) -&gt; bool {
        true
    }

    fn do_seek(&amp;self, element: &amp;BaseSrc, segment: &amp;mut gst::Segment) -&gt; bool {
        // Handle seeking here. For Time and Default (sample offset) seeks we can
        // do something and have to update our sample offset and accumulator accordingly.
        //
        // Also we should remember the stop time (so we can stop at that point), and if
        // reverse playback is requested. These values will all be used during buffer creation
        // and for calculating the timestamps, etc.

        if segment.get_rate()  0.0 {
            gst_error!(self.cat, obj: element, "Reverse playback not supported");
            return false;
        }

        let settings = *self.settings.lock().unwrap();
        let mut state = self.state.lock().unwrap();

        // We store sample_offset and sample_stop in nanoseconds if we
        // don't know any sample rate yet. It will be converted correctly
        // once a sample rate is known.
        let rate = match state.info {
            None =&gt; gst::SECOND_VAL,
            Some(ref info) =&gt; info.rate() as u64,
        };

        if let Some(segment) = segment.downcast_ref::() {
            use std::f64::consts::PI;

            let sample_offset = segment
                .get_start()
                .unwrap()
                .mul_div_floor(rate, gst::SECOND_VAL)
                .unwrap();

            let sample_stop = segment
                .get_stop()
                .map(|v| v.mul_div_floor(rate, gst::SECOND_VAL).unwrap());

            let accumulator =
                (sample_offset as f64).rem(2.0 * PI * (settings.freq as f64) / (rate as f64));

            gst_debug!(
                self.cat,
                obj: element,
                "Seeked to {}-{:?} (accum: {}) for segment {:?}",
                sample_offset,
                sample_stop,
                accumulator,
                segment
            );

            *state = State {
                info: state.info.clone(),
                sample_offset: sample_offset,
                sample_stop: sample_stop,
                accumulator: accumulator,
            };

            true
        } else if let Some(segment) = segment.downcast_ref::() {
            use std::f64::consts::PI;

            if state.info.is_none() {
                gst_error!(
                    self.cat,
                    obj: element,
                    "Can only seek in Default format if sample rate is known"
                );
                return false;
            }

            let sample_offset = segment.get_start().unwrap();
            let sample_stop = segment.get_stop().0;

            let accumulator =
                (sample_offset as f64).rem(2.0 * PI * (settings.freq as f64) / (rate as f64));

            gst_debug!(
                self.cat,
                obj: element,
                "Seeked to {}-{:?} (accum: {}) for segment {:?}",
                sample_offset,
                sample_stop,
                accumulator,
                segment
            );

            *state = State {
                info: state.info.clone(),
                sample_offset: sample_offset,
                sample_stop: sample_stop,
                accumulator: accumulator,
            };

            true
        } else {
            gst_error!(
                self.cat,
                obj: element,
                "Can't seek in format {:?}",
                segment.get_format()
            );

            false
        }
    }</pre><p></p>
<p>Currently no support for reverse playback is implemented here, that is left as an exercise for the reader. So as a first step we check if the segment has a negative rate, in which case we just fail and return false.</p>
<p>Afterwards we again take a copy of the settings, keep a mutable mutex guard of our state and then start handling the actual seek.</p>
<p>If no caps are known yet, i.e. the <i>AudioInfo</i> is <i>None</i>, we assume a rate of 1 billion. That is, we just store the time in nanoseconds for now and let the <i>set_caps</i> function take care of that (which we already implemented accordingly) once the sample rate is known.</p>
<p>Then, if a <i>Time</i> seek is performed, we convert the segment start and stop position from time to sample offsets and save them. And then update the accumulator in a similar way as in the <i>set_caps</i> function. If a seek is in <i>Default</i> format (i.e. sample offsets for raw audio), we just have to store the values and update the accumulator but only do so if the sample rate is known already. A sample offset seek does not make any sense until the sample rate is known, so we just fail here to prevent unexpected surprises later.</p>
<p>Try the following pipeline for testing seeking. You should be able to seek the current time drawn over the video, and with the left/right cursor key you can seek. Also this shows that we create a quite nice sine wave.</p>
<p></p><pre class="crayon-plain-tag">gst-launch-1.0 rssinesrc ! audioconvert ! monoscope ! timeoverlay ! navseek ! glimagesink</pre><p></p>
<p>And with that all features are implemented in our sine wave raw audio source.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2018/02/how-to-write-gstreamer-elements-in-rust-part-2-a-raw-audio-sine-wave-source/">by slomo at February 21, 2018 02:05 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="https://arunraghavan.net/?p=2421" lang="en-US">
<h3><a href="http://arunraghavan.net/" title="Arun Raghavan">Arun Raghavan</a> — <a href="https://arunraghavan.net/2018/02/applicative-functors-for-fun-and-parsing/">Applicative Functors for Fun and Parsing</a></h3>
<div class="entry">
<div class="content">
<p><em>PSA: This post has a bunch of Haskell code, but I’m going to try to make it more broadly accessible. Let’s see how that goes.</em></p>

<p>I’ve been proceeding apace with my 3rd year in <a href="https://abhinavsarkar.net/">Abhinav’s</a> Haskell classes at <a href="https://nilenso.com/">Nilenso</a>, and we just got done with the section on Applicative Functors. I’m at that point when I finally “get” it, so I thought I’d document the process, and maybe capture my a-ha moment of Applicatives.</p>

<p>I should point out that the ideas and approach in this post are all based on Abhinav’s class material (and I’ve found them really effective in understanding the underlying concepts). Many thanks are due to him, and any lack of clarity you find ahead is in my own understanding.</p>

<h3>Functors and Applicatives</h3>

<p>Functors represent a type or a context on which we can meaningfully apply (map) a function. The <code>Functor</code> typeclass is pretty straightforward:</p>

<p></p><pre class="crayon-plain-tag">class Functor f where
  fmap :: (a -&gt; b) -&gt; f a -&gt; f b</pre><p></p>

<p>Easy enough. <code>fmap</code> takes a function that transforms something of type <code>a</code> to type <code>b</code> and a value of type <code>a</code> in a context <code>f</code>. It produces a value of type <code>b</code> in the same context.</p>

<p>The <code>Applicative</code> typeclass adds two things to <code>Functor</code>. Firstly, it gives us a means of <em>putting things inside a context</em> (also called lifting). The second is to <em>apply a function within a context</em>.</p>

<p></p><pre class="crayon-plain-tag">class Functor f =&gt; Applicative f where
  pure :: a -&gt; f a
  (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b</pre><p></p>

<p>We can see <code>pure</code> <em>lifts</em> a given value into a context. The apply function (<code>&lt;*&gt;</code>) intuitively looks like <code>fmap</code>, with the difference that the function is within a context. This becomes key when we remember that Haskell functions are curried (and can thus be partially applied). This would then allow us to write something like:</p>

<p></p><pre class="crayon-plain-tag">maybeAdd :: Maybe Int -&gt; Maybe Int -&gt; Maybe Int
maybeAdd ma mb = pure (+) &lt;*&gt; ma &lt;*&gt; mb</pre><p></p>

<p>This function takes two numbers in the <code>Maybe</code> context (that is, they either exist, or are <code>Nothing</code>), and adds them. The result will be the sum if both numbers exist, or <code>Nothing</code> if either or both do not.</p>

<p>Go ahead and convince yourself that it is painful to express this generically with just <code>fmap</code>.</p>

<h3>Parsers</h3>

<p>There are many ways of looking at what a parser is. Let’s work with one definition: A parser,</p>

<ul>
<li>Takes some input</li>
<li>Converts some or all of it into something else if it can</li>
<li>Returns whatever input was not used in the conversion</li>
</ul>

<p>How do we represent something that converts something to something else? It’s a <em>function</em>, of course. Let’s write that down as a type:</p>

<p></p><pre class="crayon-plain-tag">newtype Parser i o = Parser (i -&gt; (Maybe o, i))</pre><p></p>

<p>This more or less directly maps to what we just said. A <code>Parser</code> is a data type which has two type parameters — an input type and an output type. It contains a function that takes one argument of the input type, and produces a tuple of <code>Maybe</code> the output type (signifying if parsing succeeded) and the rest of the input.</p>

<p>We can name the field <code>runParser</code>, so it becomes easier to get a hold of the function inside our <code>Parser</code> type:</p>

<p></p><pre class="crayon-plain-tag">newtype Parser i o = Parser { runParser :: i -&gt; (Maybe o, i) }</pre><p></p>

<h4>Parser combinators</h4>

<p>The “rest” part is important for the reason that we would like to be able to chain small parsers together to make bigger parsers. We do this using “parser combinators” — functions that take one or more parsers and return a more complex parser formed by combining them in some way. We’ll see some of those ways as we go along.</p>

<h3>Parser instances</h3>

<p>Before we proceed, let’s define <code>Functor</code> and <code>Applicative</code> instances for our <code>Parser</code> type.</p>

<p></p><pre class="crayon-plain-tag">instance Functor (Parser i) where
  fmap f p = Parser $ \input -&gt;
    let (mo, i) = runParser p input
    in (f &lt;$&gt; mo, i)</pre><p></p>

<p>The intuition here is clear — if I have a parser that takes some input and provides some output, <code>fmap</code>ing a function on that parser translates to applying that function on the <em>output</em> of the parser.</p>

<p></p><pre class="crayon-plain-tag">instance Applicative (Parser i) where
  pure x = Parser $ \input -&gt; (Just x, input)

  pf &lt;*&gt; po = Parser $ \input -&gt;
    case runParser pf input of
         (Just f, rest) -&gt; case runParser po rest of
                                (Just o, rest') -&gt; (Just (f o), rest')
                                (Nothing, _)    -&gt; (Nothing, input)
         (Nothing, _)   -&gt; (Nothing, input)</pre><p></p>

<p>The Applicative instance is a bit more involved than Functor. What we’re doing first is “running” the first parser which gives us the function we want to apply (remember that this is a curried function, so rather than parsing out a function, we are most likely parsing out a value and creating a function with that). If we succeed, then we run the second parser to get a value to apply the function to. If this is also successful, we apply the function to the value, and return the result within the parser context (i.e. the result, and the rest of the input).</p>

<h3>Implementing some parsers</h3>

<p>Now let’s take our new data type and instances for a spin. Before we write a real parser, let’s write a helper function. A common theme while parsing a string is to match a single character on a predicate — for example, “is this character an alphabet”, or “is this character a semi-colon”. We write a function to take a predicate and return the corresponding parser:</p>

<p></p><pre class="crayon-plain-tag">satisfy :: (Char -&gt; Bool) -&gt; Parser String Char
satisfy p = Parser $ \input -&gt;
  case input of
       (c:cs) | p c -&gt; (Just c, cs)
       _            -&gt; (Nothing, input)</pre><p></p>

<p>Now let’s try to make a parser that takes a string, and if it finds a ASCII digit character, provides the corresponding integer value. We have a function from the <code>Data.Char</code> module to match ASCII digit characters — <code>isDigit</code>. We also have a function to take a digit character and give us an integer — <code>digitToInt</code>. Putting this together with <code>satisfy</code> above.</p>

<p></p><pre class="crayon-plain-tag">import Data.Char (digitToInt, isDigit)

digit :: Parser String Int
digit = digitToInt &lt;$&gt; satisfy isDigit</pre><p></p>

<p>And that’s it! Note how we used our higher-order <code>satisfy</code> function to match a ASCII digit character and the <code>Functor</code> instance to apply <code>digitToInt</code> to the <em>result</em> of that parser (reminder: <code>$&gt;</code> is just the infix form of writing <code>fmap</code> — this is the same as <code>fmap digitToInt (satisfy digit)</code>.</p>

<p>Another example — a character parser, which succeeds if the next character in the input is a specific character we choose.</p>

<p></p><pre class="crayon-plain-tag">char :: Char -&gt; Parser String Char
char x = satisfy (x ==)</pre><p></p>

<p>Once again, the <code>satisfy</code> function makes this a breeze. I must say  I’m pleased with the conciseness of this.</p>

<p>Finally, let’s combine character parsers to create a word parser — a parser that succeeds if the input is a given word.</p>

<p></p><pre class="crayon-plain-tag">word :: String -&gt; Parser String String
word ""     = Parser $ \input -&gt; (Just "", input)
word (c:cs) = (:) &lt;$&gt; char c &lt;*&gt; word cs</pre><p></p>

<p>A match on an empty word always succeeds. For anything else, we just break down the parser to a character parser of the first character and a recursive call to the word parser for the rest. Again, note the use of the Functor and Applicative instance. Let’s look at the type signature of the <code>(:)</code> (list cons) function, which prepends an element to a list:</p>

<p></p><pre class="crayon-plain-tag">(:) :: a -&gt; [a] -&gt; [a]</pre><p></p>

<p>The function takes two arguments — a single element of type <code>a</code>, and a list of elements of type <code>a</code>. If we expand the types some more, we’ll see that the first argument we give it is a <code>Parser String Char</code> and the second is a <code>Parser String [Char]</code> (String is just an alias for <code>[Char]</code>).</p>

<p>In this way we are able to take the basic list prepend function and use it to construct a list of characters <strong>within the Parser context.</strong> (a-ha!?)</p>

<h3>JSON</h3>

<p><acronym title="JavaScript Object Notation">JSON</acronym> is a relatively simple format to parse, and makes for a good example for building a parser. The <a href="http://json.org/">JSON website</a> has a couple of good depictions of the JSON language grammar front and center.</p>

<p>So that defines our parser problem  then — we want to read a string input, and convert it into some sort of in-memory representation of the JSON value. Let’s see what that would look like in Haskell.</p>

<p></p><pre class="crayon-plain-tag">data JsonValue = JsonString String
               | JsonNumber JsonNum
               | JsonObject [(String, JsonValue)]
               | JsonArray [JsonValue]
               | JsonBool Bool
               | JsonNull

-- We represent a number as an infinite precision
-- floating point number with a base 10 exponent
data JsonNum = JsonNum { negative :: Bool
                       , signif   :: Integer
                       , expo     :: Integer
                       }</pre><p></p>

<p>The JSON specification does not really tell us what type to use for numbers. We could just use a <code>Double</code>, but to make things interesting, we represent it as an arbitrary precision floating point number.</p>

<p>Note that the <code>JsonArray</code> and <code>JsonObject</code> constructors are recursive, as they should be — a JSON array is an array of JSON values, and a JSON object is a mapping from string keys to JSON values.</p>

<h3>Parsing JSON</h3>

<p>We now have the pieces we need to start parsing JSON. Let’s start with the easy bits.</p>

<h4>null</h4>

<p>To parse a <code>null</code> we literally just look for the word “null”.</p>

<p></p><pre class="crayon-plain-tag">jsonNull :: Parser String JsonValue
jsonNull = word "null" $&gt; JsonNull</pre><p></p>

<p>The <code>$&gt;</code> operator is a flipped shortcut for <code>fmap . const</code> — it evaluates the argument on the left, and then <code>fmap</code>s the argument on the right onto it. If the <code>word "null"</code> parser is successful (<code>Just "null"</code>), we’ll <code>fmap</code> the <code>JsonValue</code> representing <code>null</code> to replace the string <code>"null"</code> (i.e. we’ll get a <code>(Just JsonNull, &lt;rest of the input&gt;)</code>).</p>

<h4>true and false</h4>

<p>First a quick detour:</p>

<p></p><pre class="crayon-plain-tag">instance Alternative (Parser i) where
  empty = Parser $ \input -&gt; (Nothing, input)
  p1 &lt;|&gt; p2 = Parser $ \input -&gt;
      case runParser p1 input of
           (Nothing, _) -&gt; case runParser p2 input of
                                (Nothing, _) -&gt; (Nothing, input)
                                justValue    -&gt; justValue
           justValue    -&gt; justValue</pre><p></p>

<p>The Alternative instance is easy to follow once you understand Applicative. We define an empty parser that matches nothing. Then we define the alternative operator (<code>&lt;|&gt;</code>) as we might intuitively imagine.</p>

<p>We run the parser given as the first argument first, if it succeeds we are done. If it fails, we run the second parser on the whole input again, if it succeeds, we return that value. If both fail, we return <code>Nothing</code>.</p>

<p>Parsing <code>true</code> and <code>false</code> with this in our belt looks like:</p>

<p></p><pre class="crayon-plain-tag">jsonBool :: Parser String JsonValue
jsonBool =  (word "true" $&gt; JsonBool True)
        &lt;|&gt; (word "false" $&gt; JsonBool False)</pre><p></p>

<p>We are easily able express the idea of trying to parse for the string “true”, and if that fails, trying again for the string “false”. If either matches, we have a boolean value, if not, <code>Nothing</code>. Again, nice and concise.</p>

<h4>String</h4>

<p>This is only slightly more complex. We need a couple of helper functions first:</p>

<p></p><pre class="crayon-plain-tag">hexDigit :: Parser String Int
hexDigit = digitToInt &lt;$&gt; satisfy isHexDigit

digitsToNumber :: Int -&gt; [Int] -&gt; Integer
digitsToNumber base digits = foldl (\num d -&gt; num * fromIntegral base + fromIntegral d) 0 digits</pre><p></p>

<p><code>hexDigit</code> is easy to follow. It just matches anything from <code>0-9</code> and <code>a-f</code> or <code>A-F</code>.</p>

<p><code>digitsToNumber</code> is a pure function that takes a list of digits, and interprets it as a number in the given base. We do some jumping through hoops with <code>fromIntegral</code> to take <code>Int</code> digits (mapping to a normal word-sized integer) and produce an <code>Integer</code> (arbitrary sized integer).</p>

<p>Now follow along one line at a time:</p>

<p></p><pre class="crayon-plain-tag">jsonString :: Parser String String
jsonString = (char '"' *&gt; many jsonChar &lt;* char '"')
  where
    jsonChar =  satisfy (\c -&gt; not (c == '\"' || c == '\\' || isControl c))
            &lt;|&gt; word "\\\"" $&gt; '"'
            &lt;|&gt; word "\\\\" $&gt; '\\'
            &lt;|&gt; word "\\/"  $&gt; '/'
            &lt;|&gt; word "\\b"  $&gt; '\b'
            &lt;|&gt; word "\\f"  $&gt; '\f'
            &lt;|&gt; word "\\n"  $&gt; '\n'
            &lt;|&gt; word "\\r"  $&gt; '\r'
            &lt;|&gt; word "\\t"  $&gt; '\t'
            &lt;|&gt; chr . fromIntegral . digitsToNumber 16 &lt;$&gt; (word "\\u" *&gt; replicateM 4 hexDigit)</pre><p></p>

<p>A string is a valid JSON character, surrounded by quotes. The <code>*&gt;</code> and <code>&lt;*</code> operators allow us to chain parsers whose output we wish to discard (since the quotes are not part of the actual string itself). The <code>many</code> function comes from the Alternative typeclass. It represents zero or more instances of context. In our case, it tries to match zero or more <code>jsonChar</code> parsers.</p>

<p>So what does <code>jsonChar</code> do? Following the definition of a character in the JSON spec, first we try to match something that is not a quote (<code>"</code>), a backslash (<code>\</code>) or a control character. If that doesn’t match, we try to match the various escape characters that the specification mentions.</p>

<p>Finally, if we get a <code>\u</code> followed by 4 hexadecimal characters, we put them in a list (<code>replicateM 4 hexDigit</code> chains 4 <code>hexDigit</code> parsers and provides the output as a list), convert that list into a base 16 integer (<code>digitsToNumber</code>), and then convert that to a Unicode character (<code>chr</code>).</p>

<p>The order of chaining these parsers does matter for performance. The first parser in our <code>&lt;|&gt;</code> chain is the one that is most likely (most characters are not escaped). This follows from our definition of the Alternative instance. We <em>run</em> the first parser, then the second, and so on. We want this to succeed as early as possible so we don’t run more parsers than necessary.</p>

<h4>Arrays</h4>

<p>Arrays and objects have something in common — they have items which are separated by some value (commas for array values, commas for each key-value pair in an object, and colons separating keys and values). Let’s just factor this commonality out:</p>

<p></p><pre class="crayon-plain-tag">sepBy :: Parser i v -&gt; Parser i s -&gt; Parser i [v]
sepBy v s = (:) &lt;$&gt; v &lt;*&gt; many (s *&gt; v) 
         &lt;|&gt; pure []</pre><p></p>

<p>We take a parser for our values (<code>v</code>), and a parser for our separator (<code>s</code>). We try to parse one or more <code>v</code> separated by <code>s</code>, and or just return an empty list in the parser context if there are none.</p>

<p>Now we write our JSON array parser as:</p>

<p></p><pre class="crayon-plain-tag">jsonArray :: Parser String JsonValue
jsonArray = JsonArray &lt;$&gt; (char '[' *&gt; (json `sepBy` char ',') &lt;* char ']')</pre><p></p>

<p>Nice, that’s really succinct. But wait! What is <code>json</code>?</p>

<h4>Putting it all together</h4>

<p>We know that arrays contain JSON values. And we know how to parse some JSON values. Let’s try to put those together for our recursive definition:</p>

<p></p><pre class="crayon-plain-tag">json :: Parser String JsonValue
json =  jsonNull
    &lt;|&gt; jsonBool
    &lt;|&gt; jsonString
    &lt;|&gt; jsonArray
--  &lt;|&gt; jsonNumber
--  &lt;|&gt; jsonObject</pre><p></p>

<p>And that’s it!</p>

<p>The JSON object and number parsers follow the same pattern. So far we’ve ignored spaces in the input, but those can be consumed and ignored easily enough based on what we’ve learned.</p>

<p>You can find the complete code for this exercise <a href="https://github.com/ford-prefect/haskell-classes/blob/master/year3/json.hs">on Github</a>.</p>

<p>Some examples of what this looks like in the REPL:</p>

<p></p><pre class="crayon-plain-tag">*Json&gt; runParser json "null"
(Just null,"")

*Json&gt; runParser json "true"
(Just true,"")

*Json&gt; runParser json "[null,true,\"hello!\"]"
(Just [null, true, "hello!" ],"")</pre><p></p>

<h3>Concluding thoughts</h3>

<p>If you’ve made it this far, thank you! I realise this is long and somewhat dense, but I am very excited by how elegantly Haskell allows us to express these ideas, using fundamental aspects of its type(class) system.</p>

<p>A nice real world example of how you might use this is the <a href="https://github.com/pcapriotti/optparse-applicative">optparse-applicative</a> package which uses these ideas to greatly simplify the otherwise dreary task of parsing command line arguments.</p>

<p>I hope this post generates at least some of the excitement in you that it has in me. Feel free to leave your comments and thoughts below.</p></div>

<p class="date">
<a href="https://arunraghavan.net/2018/02/applicative-functors-for-fun-and-parsing/">by Arun at February 21, 2018 07:24 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">February 16, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="/news/#2018-02-16T12:00:00Z">
<h3><a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> — <a href="https://gstreamer.freedesktop.org/news/#2018-02-16T12:00:00Z">GStreamer 1.13.1 unstable development release</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="gstgotchi.png" alt="(GStreamer)" width="80" height="80">
<p>
The GStreamer team is pleased to announce the first development release in
the unstable 1.13 release series.
</p><p>
The unstable 1.13 release series adds new features on top of the current
stable 1.12 series and is part of the API and ABI-stable 1.x release series
of the GStreamer multimedia framework.
</p><p>
The unstable 1.13 release series is for testing and development purposes in
the lead-up to the stable 1.14 series which is scheduled for release in a
few weeks time. Any newly-added API can still change until that point, although
it is rare for that to happen.
</p><p>
Full release notes will be provided in the near future, highlighting all the
new features, bugfixes, performance optimizations and other important changes.
</p><p>
Packagers: please note that quite a few plugins and libraries have moved
between modules, so please take extra care and make sure inter-module
version dependencies are such that users can only upgrade all modules in
one go, instead of seeing a mix of 1.13 and 1.12 on their system.
</p><p>
Binaries for Android, iOS, Mac OS X and Windows will be provided shortly.
</p><p>
Release tarballs can be downloaded directly here:
</p><ul>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer/gstreamer-1.13.1.tar.xz">gstreamer-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-base/gst-plugins-base-1.13.1.tar.xz">gst-plugins-base-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-good/gst-plugins-good-1.13.1.tar.xz">gst-plugins-good-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-ugly/gst-plugins-ugly-1.13.1.tar.xz">gst-plugins-ugly-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-plugins-bad/gst-plugins-bad-1.13.1.tar.xz">gst-plugins-bad-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-libav/gst-libav-1.13.1.tar.xz">gst-libav-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-rtsp-server/gst-rtsp-server-1.13.1.tar.xz">gst-rtsp-server-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-python/gst-python-1.13.1.tar.xz">gst-python-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-editing-services/gstreamer-editing-services-1.13.1.tar.xz">gst-editing-services-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-validate/gst-validate-1.13.1.tar.xz">gst-validate-1.13.1.tar.xz</a>,</li>
  <li><a href="https://gstreamer.freedesktop.org/src/gstreamer-vaapi/gstreamer-vaapi-1.13.1.tar.xz">gstreamer-vaapi-1.13.1.tar.xz</a></li>
  <li><a href="https://gstreamer.freedesktop.org/src/gst-omx/gst-omx-1.13.1.tar.xz">gst-omx-1.13.1.tar.xz</a></li>
</ul>
        <p></p></div>

<p class="date">
<a href="https://gstreamer.freedesktop.org/news/#2018-02-16T12:00:00Z">February 16, 2018 12:00 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">February 07, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://wingolog.org/2018/02/07/design-notes-on-inline-caches-in-guile">
<h3><a href="http://wingolog.org/" title="wingolog">Andy Wingo</a> — <a href="http://wingolog.org/archives/2018/02/07/design-notes-on-inline-caches-in-guile">design notes on inline caches in guile</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="awingo.png" alt="(Andy Wingo)" width="74" height="100">
<div><p>Ahoy, programming-language tinkerfolk!  Today's rambling missive chews the gnarly bones of <a href="https://en.wikipedia.org/wiki/Inline_caching">"inline caches"</a>, in general but also with particular respect to the <a href="https://gnu.org/s/guile">Guile</a> implementation of Scheme.  First, a little intro.</p><p><b>inline what?</b></p><p>Inline caches are a language implementation technique used to accelerate polymorphic dispatch.  Let's dive in to that.</p><p>By <i>implementation technique</i>, I mean that the technique applies to the language compiler and runtime, rather than to the semantics of the language itself.  The effects on the language do exist though in an indirect way, in the sense that inline caches can make some operations faster and therefore more common.  Eventually inline caches can affect what users expect out of a language and what kinds of programs they write.</p><p>But I'm getting ahead of myself.  <i>Polymorphic dispatch</i> literally means "choosing based on multiple forms".  Let's say your language has immutable strings -- like Java, Python, or Javascript.  Let's say your language also has operator overloading, and that it uses <tt>+</tt> to concatenate strings.  Well at that point you have a problem -- while you can specify a terse semantics of some core set of operations on strings (win!), you can't choose one representation of strings that will work well for all cases (lose!).  If the user has a workload where they regularly build up strings by concatenating them, you will want to store strings as trees of substrings.  On the other hand if they want to access <s>characters</s>codepoints by index, then you want an array.  But if the codepoints are all below 256, maybe you should represent them as bytes to save space, whereas maybe instead as 4-byte codepoints otherwise?  Or maybe even UTF-8 with a codepoint index side table.</p><p>The right representation (form) of a string depends on the myriad ways that the string might be used.  The <tt>string-append</tt> operation is <i>polymorphic</i>, in the sense that the precise code for the operator depends on the representation of the operands -- despite the fact that the <i>meaning</i> of <tt>string-append</tt> is monomorphic!</p><p>Anyway, that's the problem.  Before inline caches came along, there were two solutions: callouts and open-coding.  Both were bad in similar ways.  A callout is where the compiler generates a call to a generic runtime routine.  The runtime routine will be able to handle all the myriad forms and combination of forms of the operands.  This works fine but can be a bit slow, as all callouts for a given operator (e.g. <tt>string-append</tt>) dispatch to a single routine for the whole program, so they don't get to optimize for any particular call site.</p><p>One tempting thing for compiler writers to do is to effectively inline the <tt>string-append</tt> operation into each of its call sites.  This is "open-coding" (in the terminology of the early Lisp implementations like MACLISP).  The advantage here is that maybe the compiler knows something about one or more of the operands, so it can eliminate some cases, effectively performing some compile-time specialization.  But this is a limited technique; one could argue that the whole point of polymorphism is to allow for generic operations on generic data, so you rarely have compile-time invariants that can allow you to specialize.  Open-coding of polymorphic operations instead leads to code bloat, as the <tt>string-append</tt> operation is just so many copies of the same thing.</p><p>Inline caches emerged to solve this problem.  They trace their lineage back to Smalltalk 80, gained in complexity and power with Self and finally reached mass consciousness through Javascript.  These languages all share the characteristic of being dynamically typed and object-oriented.  When a user evaluates a statement like <tt>x = y.z</tt>, the language implementation needs to figure out where <tt>y.z</tt> is actually located.  This location depends on the representation of <tt>y</tt>, which is rarely known at compile-time.</p><p>However for any given reference <tt>y.z</tt> in the source code, there is a finite set of concrete representations of <tt>y</tt> that will actually flow to that call site at run-time.  Inline caches allow the language implementation to specialize the <tt>y.z</tt> access for its particular call site.  For example, at some point in the evaluation of a program, <tt>y</tt> may be seen to have representation R1 or R2.  For R1, the <tt>z</tt> property may be stored at offset 3 within the object's storage, and for R2 it might be at offset 4.  The inline cache is a bit of specialized code that compares the type of the object being accessed against R1 , in that case returning the value at offset 3, otherwise R2 and offset r4, and otherwise falling back to a generic routine.  If this isn't clear to you, Vyacheslav Egorov write a <a href="http://mrale.ph/blog/2012/06/03/explaining-js-vms-in-js-inline-caches.html">fine article describing and implementing the object representation optimizations enabled by inline caches</a>.</p><p>Inline caches also serve as input data to later stages of an adaptive compiler, allowing the compiler to selectively inline (open-code) only those cases that are appropriate to values actually seen at any given call site.</p><p><b>but how?</b></p><p>The classic formulation of inline caches from Self and early V8 actually patched the code being executed.  An inline cache might be allocated at address <tt>0xcabba9e5</tt> and the code emitted for its call-site would be <tt>jmp 0xcabba9e5</tt>.  If the inline cache ended up bottoming out to the generic routine, a new inline cache would be generated that added an implementation appropriate to the newly seen "form" of the operands and the call-site.  Let's say that new IC (inline cache) would have the address <tt>0x900db334</tt>.  Early versions of V8 would actually patch the machine code at the call-site to be <tt>jmp 0x900db334</tt> instead of <tt>jmp 0xcabba6e5</tt>.</p><p>Patching machine code has a number of disadvantages, though.  It inherently target-specific: you will need different strategies to patch x86-64 and armv7 machine code.  It's also expensive: you have to flush the instruction cache after the patch, which slows you down.  That is, of course, if you are allowed to patch executable code; on many systems that's impossible.  Writable machine code is a potential vulnerability if the system may be vulnerable to remote code execution.</p><p>Perhaps worst of all, though, patching machine code is not thread-safe.  In the case of early Javascript, this perhaps wasn't so important; but as JS implementations gained parallel garbage collectors and JS-level parallelism via "service workers", this becomes less acceptable.</p><p>For all of these reasons, the modern take on inline caches is to implement them as a memory location that can be atomically modified.  The call site is just <tt>jmp *<i>loc</i></tt>, as if it were a virtual method call.  Modern CPUs have "branch target buffers" that predict the target of these indirect branches with very high accuracy so that the indirect jump does not become a pipeline stall.  (What does this mean in the face of the Spectre v2 vulnerabilities?  Sadly, God only knows at this point.  Saddest panda.)</p><p><b>cry, the beloved country</b></p><p>I am interested in ICs in the context of the Guile implementation of Scheme, but first I will make a digression.  Scheme is a very monomorphic language.  Yet, this monomorphism is entirely cultural.  It is in no way essential.  Lack of ICs in implementations has actually fed back and encouraged this monomorphism.</p><p>Let us take as an example the case of property access.  If you have a pair in Scheme and you want its first field, you do <tt>(car x)</tt>.  But if you have a vector, you do <tt>(vector-ref x 0)</tt>.</p><p>What's the reason for this nonuniformity?  You could have a generic <tt>ref</tt> procedure, which when invoked as <tt>(ref x 0)</tt> would return the field in <tt>x</tt> associated with 0.  Or <tt>(ref x 'foo)</tt> to return the <tt>foo</tt> property of <tt>x</tt>.  It would be more orthogonal in some ways, and it's completely valid Scheme.</p><p>We don't write Scheme programs this way, though.  From what I can tell, it's for two reasons: one good, and one bad.</p><p>The good reason is that saying <tt>vector-ref</tt> means more to the reader.  You know more about the complexity of the operation and what side effects it might have.  When you call <tt>ref</tt>, who knows?  Using concrete primitives allows for better program analysis and understanding.</p><p>The bad reason is that Scheme implementations, Guile included, tend to compile <tt>(car x)</tt> to much better code than <tt>(ref x 0)</tt>.  Scheme implementations in practice aren't well-equipped for polymorphic data access.  In fact it is standard Scheme practice to abuse the "macro" facility to manually inline code so that that certain performance-sensitive operations get inlined into a closed graph of monomorphic operators with no callouts.  To the extent that this is true, Scheme programmers, Scheme programs, and the Scheme language as a whole are all victims of their implementations.  JavaScript, for example, does not have this problem -- to a small extent, maybe, yes, performance tweaks and tuning are always a thing but JavaScript implementations' ability to burn away polymorphism and abstraction results in an entirely different character in JS programs versus Scheme programs.</p><p><b>it gets worse</b></p><p>On the most basic level, Scheme is the call-by-value lambda calculus.  It's well-studied, well-understood, and eminently flexible.  However the way that the syntax maps to the semantics hides a constrictive monomorphism: that the "callee" of a call refer to a lambda expression.</p><p>Concretely, in an expression like <tt>(a b)</tt>, in which <tt>a</tt> is not a macro, <tt>a</tt> must evaluate to the result of a <tt>lambda</tt> expression.  Perhaps by reference (e.g. <tt>(define a (lambda (x) x))</tt>), perhaps directly; but a lambda nonetheless.  But what if <tt>a</tt> is actually a vector?  At that point the Scheme language standard would declare that to be an error.</p><p>The semantics of Clojure, though, would allow for <tt>((vector 'a 'b 'c) 1)</tt> to evaluate to <tt>b</tt>.  Why not in Scheme?  There are the same good and bad reasons as with <tt>ref</tt>.  Usually, the concerns of the language implementation dominate, regardless of those of the users who generally want to write terse code.  Of course in some cases the implementation concerns <i>should</i> dominate, but not always.  Here, Scheme could be more flexible if it wanted to.</p><p><b>what have you done for me lately</b></p><p>Although inline caches are not a miracle cure for performance overheads of polymorphic dispatch, they are a tool in the box.  But what, precisely, can they do, both in general and for Scheme?</p><p>To my mind, they have five uses.  If you can think of more, please let me know in the comments.</p><p>Firstly, they have the classic named property access optimizations as in JavaScript.  These apply less to Scheme, as we don't have generic property access.  Perhaps this is a deficiency of Scheme, but it's not exactly low-hanging fruit.  Perhaps this would be more interesting if Guile had more generic protocols such as Racket's iteration.</p><p>Next, there are the arithmetic operators: addition, multiplication, and so on.  Scheme's arithmetic is indeed polymorphic; the addition operator <tt>+</tt> can add any number of complex numbers, with a distinction between exact and inexact values.  On a representation level, Guile has fixnums (small exact integers, no heap allocation), bignums (arbitrary-precision heap-allocated exact integers), fractions (exact ratios between integers), flonums (heap-allocated double-precision floating point numbers), and compnums (inexact complex numbers, internally a pair of doubles).  Also in Guile, arithmetic operators are a "primitive generics", meaning that they can be extended to operate on new types at runtime via GOOPS.</p><p>The usual situation though is that any particular instance of an addition operator only sees fixnums.  In that case, it makes sense to only emit code for fixnums, instead of the product of all possible numeric representations.  This is a clear application where inline caches can be interesting to Guile.</p><p>Third, there is a very specific case related to dynamic linking.  Did you know that most programs compiled for GNU/Linux and related systems have inline caches in them?  It's a bit weird but the <a href="https://www.airs.com/blog/archives/41">"Procedure Linkage Table"</a> (PLT) segment in ELF binaries on Linux systems is set up in a way that when e.g. <tt>libfoo.so</tt> is loaded, the dynamic linker usually doesn't eagerly resolve all of the external routines that <tt>libfoo.so</tt> uses.  The first time that <tt>libfoo.so</tt> calls <tt>frobulate</tt>, it ends up calling a procedure that looks up the location of the <tt>frobulate</tt> procedure, then patches the binary code in the PLT so that the next time <tt>frobulate</tt> is called, it dispatches directly.  To dynamic language people it's the weirdest thing in the world that the C/C++/everything-static universe has at its cold, cold heart a hash table and a dynamic dispatch system that it doesn't expose to any kind of user for instrumenting or introspection -- any user that's not a malware author, of course.</p><p>But I digress!  Guile can use ICs to lazily resolve runtime routines used by compiled Scheme code.  But perhaps this isn't optimal, as the set of primitive runtime calls that Guile will embed in its output is finite, and so resolving these routines eagerly would probably be sufficient.  Guile could use ICs for inter-module references as well, and these should indeed be resolved lazily; but I don't know, perhaps the current strategy of using a call-site cache for inter-module references is sufficient.</p><p>Fourthly (are you counting?), there is a general case of the former:  when you see a call <tt>(a b)</tt> and you don't know what <tt>a</tt> is.  If you put an inline cache in the call, instead of having to emit checks that <tt>a</tt> is a heap object and a procedure and then emit an indirect call to the procedure's code, you might be able to emit simply a check that <tt>a</tt> is the same as <tt>x</tt>, the only callee you ever saw at that site, and in that case you can emit a direct branch to the function's code instead of an indirect branch.</p><p>Here I think the argument is less strong.  Modern CPUs are already very good at indirect jumps and well-predicted branches.  The value of a devirtualization pass in compilers is that it makes the side effects of a virtual method call concrete, allowing for more optimizations; avoiding indirect branches is good but not necessary.  On the other hand, Guile does have polymorphic callees (<a href="https://www.gnu.org/software/guile/docs/docs-2.0/guile-ref/Methods-and-Generic-Functions.html#Methods-and-Generic-Functions">generic functions</a>), and call ICs could help there.  Ideally though we would need to extend the language to allow generic functions to feed back to their inline cache handlers.</p><p>Finally, ICs could allow for cheap tracepoints and breakpoints.  If at every breakable location you included a <tt>jmp *<i>loc</i></tt>, and the initial value of <tt>*<i>loc</i></tt> was the next instruction, then you could patch individual locations with code to run there.  The patched code would be responsible for saving and restoring machine state around the instrumentation.</p><p>Honestly I struggle a lot with the idea of debugging native code.  GDB does the least-overhead, most-generic thing, which is patching code directly; but it runs from a separate process, and in Guile we need in-process portable debugging.  The debugging use case is a clear area where you want adaptive optimization, so that you can emit debugging ceremony from the hottest code, knowing that you can fall back on some earlier tier.  Perhaps Guile should bite the bullet and go this way too.</p><p><b>implementation plan</b></p><p>In Guile, monomorphic as it is in most things, probably only arithmetic is worth the trouble of inline caches, at least in the short term.</p><p>Another question is how much to specialize the inline caches to their call site.  On the extreme side, each call site could have a custom calling convention: if the first operand is in register A and the second is in register B and they are expected to be fixnums, and the result goes in register C, and the continuation is the code at L, well then you generate an inline cache that specializes to all of that.  No need to shuffle operands or results, no need to save the continuation (return location) on the stack.</p><p>The opposite would be to call ICs as if their were normal procedures:  shuffle arguments into fixed operand registers, push a stack frame, and when the IC returns, shuffle the result into place.</p><p>Honestly I am looking mostly to the simple solution.  I am concerned about code and heap bloat if I specify to every last detail of a call site.  Also maximum speed comes with an adaptive optimizer, and in that case simple lower tiers are best.</p><p><b>sanity check</b></p><p>To compare these impressions, I took a look at V8's current source code to see where they use ICs in practice.  When I worked on V8, the compiler was entirely different -- there were two tiers, and both of them generated native code.  Inline caches were everywhere, and they were gnarly; every architecture had its own implementation.  Now in V8 there are two tiers, not the same as the old ones, and the lowest one is a bytecode interpreter.</p><p>As an adaptive optimizer, V8 doesn't need breakpoint ICs.  It can always deoptimize back to the interpreter.  In actual practice, to debug at a source location, V8 will patch the bytecode to insert a "DebugBreak" instruction, which has its own support in the interpreter.  V8 also supports optimized compilation of this operation.  So, no ICs needed here.</p><p>Likewise for generic type feedback, V8 records types as data rather than in the classic formulation of inline caches as in Self.  I think WebKit's JavaScriptCore uses a similar strategy.</p><p>V8 does use inline caches for property access (loads and stores).  Besides that there is an inline cache used in calls which is just used to record callee counts, and not used for direct call optimization.</p><p>Surprisingly, V8 doesn't even seem to use inline caches for arithmetic (any more?).  Fair enough, I guess, given that JavaScript's numbers aren't very polymorphic, and even with a system with fixnums and heap floats like V8, floating-point numbers are rare in cold code.</p><p>The dynamic linking and relocation points don't apply to V8 either, as it doesn't receive binary code from the internet; it always starts from source.</p><p><b>twilight of the inline cache</b></p><p>There was a time when inline caches were recommended to solve all your VM problems, but it would seem now that their heyday is past.</p><p>ICs are still a win if you have named property access on objects whose shape you don't know at compile-time.  But improvements in CPU branch target buffers mean that it's no longer imperative to use ICs to avoid indirect branches (modulo Spectre v2), and creating direct branches via code-patching has gotten more expensive and tricky on today's targets with concurrency and deep cache hierarchies.</p><p>Besides that, the type feedback component of inline caches seems to be taken over by explicit data-driven call-site caches, rather than executable inline caches, and the highest-throughput tiers of an adaptive optimizer burn away inline caches anyway.  The pressure on an inline cache infrastructure now is towards simplicity and ease of type and call-count profiling, leaving the speed component to those higher tiers.</p><p>In Guile the bounded polymorphism on arithmetic combined with the need for ahead-of-time compilation means that ICs are probably a code size and execution time win, but it will take some engineering to prevent the calling convention overhead from dominating cost.</p><p>Time to experiment, then -- I'll let y'all know how it goes.  Thoughts and feedback welcome from the compilerati.  Until then, happy hacking :)</p></div></div>

<p class="date">
<a href="http://wingolog.org/archives/2018/02/07/design-notes-on-inline-caches-in-guile">by Andy Wingo at February 07, 2018 03:14 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">February 05, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://wingolog.org/2018/02/05/notes-from-the-fosdem-2018-networking-devroom">
<h3><a href="http://wingolog.org/" title="wingolog">Andy Wingo</a> — <a href="http://wingolog.org/archives/2018/02/05/notes-from-the-fosdem-2018-networking-devroom">notes from the fosdem 2018 networking devroom</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="awingo.png" alt="(Andy Wingo)" width="74" height="100">
<div><p>Greetings, internet!</p><p>I am on my way back from <a href="https://fosdem.org/">FOSDEM</a> and thought I would share with yall some impressions from talks in the <a href="https://fosdem.org/2018/schedule/track/sdn_and_nfv/">Networking devroom</a>.  I didn't get to go to all that many talks -- FOSDEM's hallway track is the hottest of them all -- but I did hit a select few.  Thanks to Dave Neary at Red Hat for organizing the room.</p><p><b>Ray Kinsella -- Intel -- <a href="https://fosdem.org/2018/schedule/event/dpdk_microservices/">The path to data-plane micro-services</a></b></p><p>The day started with a drum-beating talk that was very light on technical information.</p><p>Essentially Ray was arguing for an evolution of network function virtualization -- that instead of running VNFs on bare metal as was done in the days of yore, that people started to run them in virtual machines, and now they run them in containers -- what's next?  Ray is saying that "cloud-native VNFs" are the next step.</p><p>Cloud-native VNFs to move from "greedy" VNFs that take charge of the cores that are available to them, to some kind of resource sharing.  "Maybe users value flexibility over performance", says Ray.  It's the Care Bears approach to networking: (resource) sharing is caring.</p><p>In practice he proposed two ways that VNFs can map to cores and cards.</p><p>One was in-process sharing, which if I understood him properly was actually as nodes running within a VPP process.  Basically in this case VPP or DPDK is the scheduler and multiplexes two or more network functions in one process.</p><p>The other was letting Linux schedule separate processes.  In networking, we don't usually do it this way: <a href="https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/doc/performance.md">we run network functions on dedicated cores on which nothing else runs</a>.  Ray was suggesting that perhaps network functions could be more like "normal" Linux services.  Ray doesn't know if Linux scheduling will work in practice.  Also it might mean allowing DPDK to work with 4K pages instead of the 2M hugepages it currently requires.  This obviously has the potential for more latency hazards and would need some tighter engineering, and ultimately would have fewer guarantees than the "greedy" approach.</p><p>Interesting side things I noticed:</p><ul>
<li><p>All the diagrams show Kubernetes managing CPU node allocation and interface assignment.  I guess in marketing diagrams, Kubernetes has completely replaced OpenStack.</p></li>
<li><p>One slide showed guest VNFs differentiated between "virtual network functions" and "socket-based applications", the latter ones being the legacy services that use kernel APIs.  It's a useful terminology difference.</p></li>
<li><p>The talk identifies user-space networking with DPDK (only!).</p></li>
</ul><p>Finally, I note that <a href="https://www.wingolog.org/archives/2015/11/09/embracing-conways-law">Conway's law</a> is obviously reflected in the performance overheads: because there are organizational isolations between dev teams, vendors, and users, there are big technical barriers between them too.  The least-overhead forms of resource sharing are also those with the highest technical consistency and integration (nodes in a single VPP instance).</p><p><b>Magnus Karlsson -- Intel -- <a href="https://fosdem.org/2018/schedule/event/af_xdp/">AF_XDP</a></b></p><p>This was a talk about getting good throughput from the NIC to userspace, but by using some kernel facilities.  The idea is to get the kernel to set up the NIC and virtualize the transmit and receive ring buffers, but to let the NIC's DMA'd packets go directly to userspace.</p><p>The performance goal is 40Gbps for thousand-byte packets, or 25 Gbps for traffic with only the smallest packets (64 bytes).  The fast path does "zero copy" on the packets if the hardware has the capability to steer the subset of traffic associated with the AF_XDP socket to that particular process.</p><p>The AF_XDP project builds on <a href="https://www.iovisor.org/technology/xdp">XDP</a>, a newish thing where a little kind of bytecode can run on the kernel or possibly on the NIC.  One of the bytecode commands (REDIRECT) causes packets to be forwarded to user-space instead of handled by the kernel's otherwise heavyweight networking stack.  AF_XDP is the bridge between XDP on the kernel side and an interface to user-space using sockets (as opposed to e.g. AF_INET).  The performance goal was to be within 10% or so of DPDK's raw user-space-only performance.</p><p>The benefits of AF_XDP over the current situation would be that you have just one device driver, in the kernel, rather than having to have one driver in the kernel (which you have to have anyway) and one in user-space (for speed).  Also, with the kernel involved, there is a possibility for better isolation between different processes or containers, when compared with raw PCI access from user-space..</p><p>AF_XDP is what was previously known as AF_PACKET v4, and its numbers are looking somewhat OK.  Though it's not upstream yet, it might be interesting to get a Snabb driver here.</p><p>I would note that kernel-userspace cooperation is a bit of a theme these days.  There are other points of potential cooperation or common domain sharing, storage being an obvious one.  However I heard more than once this weekend the kind of "I don't know, that area of the kernel has a different culture" sort of concern as that highlighted by Daniel Vetter in his <a href="https://lwn.net/Articles/745817/">recent LCA talk</a>.</p><p><b>François-Frédéric Ozog -- Linaro -- <a href="https://fosdem.org/2018/schedule/event/netmdev/">Userland Network I/O</a></b></p><p>This talk is hard to summarize.  Like the previous one, it's again about getting packets to userspace with some support from the kernel, but the speaker went really deep and I'm not quite sure what in the talk is new and what is known.</p><p>François-Frédéric is working on a new set of abstractions for relating the kernel and user-space.  He works on OpenDataPlane (ODP), which is kinda like DPDK in some ways.  ARM seems to be a big target for his work; that x86-64 is also a target goes without saying.</p><p>His problem statement was, how should we enable fast userland network I/O, without duplicating drivers?</p><p>François-Frédéric was a bit negative on AF_XDP because (he says) it is so focused on packets that it neglects other kinds of devices with similar needs, such as crypto accelerators.  Apparently the challenge here is accelerating a single large IPsec tunnel -- because the cryptographic operations are serialized, you need good single-core performance, and making use of hardware accelerators seems necessary right now for even a single 10Gbps stream.  (If you had many tunnels, you could parallelize, but that's not the case here.)</p><p>He was also a bit skeptical about standardizing on the "packet array I/O model" which AF_XDP and most NICS use.  What he means here is that most current NICs move packets to and from main memory with the help of a "descriptor array" ring buffer that holds pointers to packets.  A transmit array stores packets ready to transmit; a receive array stores maximum-sized packet buffers ready to be filled by the NIC.  The packet data itself is somewhere else in memory; the descriptor only points to it.  When a new packet is received, the NIC fills the corresponding packet buffer and then updates the "descriptor array" to point to the newly available packet.  This requires at least two memory writes from the NIC to memory: at least one to write the packet data (one per 64 bytes of packet data), and one to update the DMA descriptor with the packet length and possible other metadata.</p><p>Although these writes go directly to cache, there's a limit to the number of DMA operations that can happen per second, and with 100Gbps cards, we can't afford to make one such transaction per packet.</p><p>François-Frédéric promoted an alternative I/O model for high-throughput use cases: the "tape I/O model", where packets are just written back-to-back in a uniform array of memory.  Every so often a block of memory containing some number of packets is made available to user-space.  This has the advantage of packing in more packets per memory block, as there's no wasted space between packets.  This increases cache density and decreases DMA transaction count for transferring packet data, as we can use each 64-byte DMA write to its fullest.  Additionally there's no side table of descriptors to update, saving a DMA write there.</p><p>Apparently the only cards currently capable of 100 Gbps traffic, the Chelsio and Netcope cards, use the "tape I/O model".</p><p>Incidentally, the DMA transfer limit isn't the only constraint.  Something I hadn't fully appreciated before was memory write bandwidth.  Before, I had thought that because the NIC would transfer in packet data directly to cache, that this wouldn't necessarily cause any write traffic to RAM.  Apparently that's not the case.  Later over drinks (thanks to Red Hat's networking group for organizing), François-Frédéric asserted that the DMA transfers would eventually use up DDR4 bandwidth as well.</p><p>A NIC-to-RAM DMA transaction will write one cache line (usually 64 bytes) to the socket's last-level cache.  This write will evict whatever was there before.  As far as I can tell, there are three cases of interest here.  The best case is where the evicted cache line is from a previous DMA transfer to the same address.  In that case it's modified in the cache and not yet flushed to main memory, and we can just update the cache instead of flushing to RAM.  (Do I misunderstand the way caches work here?  Do let me know.)</p><p>However if the evicted cache line is from some other address, we might have to flush to RAM if the cache line is dirty.  That causes a memory write traffic.  But if the cache line is clean, that means it was probably loaded as part of a memory read operation, and then that means we're evicting part of the network function's working set, which will later cause memory read traffic as the data gets loaded in again, and write traffic to flush out the DMA'd packet data cache line.</p><p>François-Frédéric simplified the whole thing to equate packet bandwidth with memory write bandwidth, that yes, the packet goes directly to cache but it is also written to RAM.  I can't convince myself that that's the case for all packets, but I need to look more into this.</p><p>Of course the cache pressure and the memory traffic is worse if the packet data is less compact in memory; and worse still if there is any need to copy data.  Ultimately, processing small packets at 100Gbps is still a huge challenge for user-space networking, and it's no wonder that there are only a couple devices on the market that can do it reliably, not that I've seen either of them operate first-hand :)</p><p>Talking with Snabb's Luke Gorrie later on, he thought that it could be that we can still stretch the packet array I/O model for a while, given that PCIe gen4 is coming soon, which will increase the DMA transaction rate.  So that's a possibility to keep in mind.</p><p>At the same time, apparently there are some <a href="https://www.ccixconsortium.com/">"coherent interconnects"</a> coming too which will allow the NIC's memory to be mapped into the "normal" address space available to the CPU.  In this model, instead of having the NIC transfer packets to the CPU, the NIC's memory will be directly addressable from the CPU, as if it were part of RAM.  The latency to pull data in from the NIC to cache is expected to be slightly longer than a RAM access; for comparison, RAM access takes about 70 nanoseconds.</p><p>For a user-space networking workload, coherent interconnects don't change much.  You still need to get the packet data into cache.  True, you do avoid the writeback to main memory, as the packet is already in addressable memory before it's in cache.  But, if it's possible to keep the packet on the NIC -- like maybe you are able to add some kind of inline classifier on the NIC that could directly shunt a packet towards an on-board IPSec accelerator -- in that case you could avoid a lot of memory transfer.  That appears to be the driving factor for coherent interconnects.</p><p>At some point in François-Frédéric's talk, my brain just died.  I didn't quite understand all the complexities that he was taking into account.  Later, after he kindly took the time to dispell some more of my ignorance, I understand more of it, though not yet all :)  The concrete "deliverable" of the talk was a model for kernel modules and user-space drivers that uses the paradigms he was promoting.  It's a work in progress from Linaro's networking group, with some support from NIC vendors and CPU manufacturers.</p><p><b>Luke Gorrie and Asumu Takikawa -- SnabbCo and Igalia -- <a href="https://fosdem.org/2018/schedule/event/lua_snabb/">How to write your own NIC driver, and why</a></b></p><p>This talk had the most magnificent beginning: a sort of "repent now ye sinners" sermon from Luke Gorrie, a seasoned veteran of software networking.  Luke started by describing the path of righteousness leading to "driver heaven", a world in which all vendors have publically accessible datasheets which parsimoniously describe what you need to get packets flowing.  In this blessed land it's easy to write drivers, and for that reason there are many of them.  Developers choose a driver based on their needs, or they write one themselves if their needs are quite specific.</p><p>But there is another path, says Luke, that of "driver hell": a world of wickedness and proprietary datasheets, where even when you buy the hardware, you can't program it unless you're buying a hundred thousand units, and even then you are smitten with the cursed non-disclosure agreements.  In this inferno, only a vendor is practically empowered to write drivers, but their poor driver developers are only incentivized to get the driver out the door deployed on all nine architectural circles of driver hell.  So they include some kind of circle-of-hell abstraction layer, resulting in a hundred thousand lines of code like a tangled frozen beard.  We all saw the abyss and repented.</p><p>Luke described the process that led to Mellanox releasing the specification for its ConnectX line of cards, something that was warmly appreciated by the entire audience, users and driver developers included.  Wonderful stuff.</p><p>My Igalia colleague Asumu Takikawa took the last half of the presentation, showing some code for the driver for the Intel i210, i350, and 82599 cards.  For more on that, I recommend his recent <a href="https://www.asumu.xyz/blog/2018/01/15/supporting-both-vmdq-and-rss-in-snabb">blog post on user-space driver development</a>.  It was truly a ray of sunshine in dark, dark Brussels.</p><p><b>Ole Trøan -- Cisco -- <a href="https://fosdem.org/2018/schedule/event/vnf_vpp/">Fast dataplanes with VPP</a></b></p><p>This talk was a delightful introduction to <a href="https://wiki.fd.io/view/VPP">VPP</a>, but without all of the marketing; the sort of talk that makes FOSDEM worthwhile.  Usually at more commercial, vendory events, you can't really get close to the technical people unless you have a vendor relationship: they are surrounded by a phalanx of salesfolk.  But in FOSDEM it is clear that we are all comrades out on the open source networking front.</p><p>The speaker expressed great personal pleasure on having being able to work on open source software; his relief was palpable.  A nice moment.</p><p>He also had some kind words about Snabb, too, saying at one point that "of course you can do it on snabb as well -- Snabb and VPP are quite similar in their approach to life".  He trolled the horrible complexity diagrams of many "NFV" stacks whose components reflect the org charts that produce them more than the needs of the network functions in question (service chaining anyone?).</p><p>He did get to drop some numbers as well, which I found interesting.  One is that recently they have been working on carrier-grade NAT, aiming for 6 terabits per second.  Those are pretty big boxes and I hope they are getting paid appropriately for that :)  For context he said that for a 4-unit server, these days you can build one that does a little less than a terabit per second.  I assume that's with ten dual-port 40Gbps cards, and I would guess to power that you'd need around 40 cores or so, split between two sockets.</p><p>Finally, he finished with a long example on lightweight 4-over-6.  Incidentally this is the same network function my group at Igalia has been building in Snabb over the last couple years, so it was interesting to see the comparison.  I enjoyed his commentary that although all of these technologies (carrier-grade NAT, MAP, lightweight 4-over-6) have the ostensible goal of keeping IPv4 running, in reality "we're day by day making IPv4 work worse", mainly by breaking the assumption that just because you get traffic from port P on IP M, doesn't mean you can send traffic to M from another port or another protocol and have it reach the target.</p><p>All of these technologies also have problems with IPv4 fragmentation.  Getting it right is possible but expensive.  Instead, Ole mentions that he and a cross-vendor cabal of dataplane people have a "dark RFC" in the works to deprecate IPv4 fragmentation entirely :)</p><p>OK that's it.  If I get around to writing up the couple of interesting Java talks I went to (I know right?) I'll let yall know.  Happy hacking!</p></div></div>

<p class="date">
<a href="http://wingolog.org/archives/2018/02/05/notes-from-the-fosdem-2018-networking-devroom">by Andy Wingo at February 05, 2018 05:22 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="tag:blogger.com,1999:blog-701969077517001201.post-5460568175258307802">
<h3><a href="http://blog.nirbheek.in/" title="Nirbheek’s Rantings">Nirbheek Chauhan</a> — <a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html">GStreamer has grown a WebRTC implementation</a></h3>
<div class="entry">
<div class="content">
<div dir="ltr"><span><i>In other news, GStreamer is now almost buzzword-compliant! The next blog post on our list: blockchains and smart contracts in GStreamer.</i></span><br><br>Late last year, <a href="https://twitter.com/centricular/status/921727092810592256" target="_top">we at Centricular announced</a> a new <a href="http://webrtcbydralex.com/index.php/2017/10/21/a-new-webrtc-implementation-is-out/" target="_top">implementation of WebRTC</a> in GStreamer.&nbsp; Today we're happy to announce that after  community review, that work has been <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/commit/?id=1894293d6378c69548d974d2965e9decc1527654" target="_top">merged into GStreamer</a> itself! The plugin is called <tt>webrtcbin</tt>, and the library is, naturally, called <tt>gstwebrtc</tt>.<br><br>The implementation has all the basic features, is transparently compatible with other WebRTC stacks (particularly in browsers), and has been  well-tested with both Firefox and Chrome.<br><br>Some of the more advanced features such as FEC  are already a <a href="https://bugzilla.gnome.org/show_bug.cgi?id=792696" target="_top">work in progress</a>, and others will be too—if you want them to be! Hop onto IRC on #gstreamer @ Freenode.net or join <a href="https://lists.freedesktop.org/mailman/listinfo/gstreamer-devel" target="_top">the mailing list</a>.<br><br><h3>How do I use it?</h3><br>Currently, the easiest way to use <tt>webrtcbin</tt> is to build GStreamer using either <a href="https://arunraghavan.net/2014/07/quick-start-guide-to-gst-uninstalled-1-x/">gst-uninstalled</a> (Linux and macOS) or <a href="https://gstreamer.freedesktop.org/documentation/installing/building-from-source-using-cerbero.html">Cerbero</a> (Windows, iOS, Android). If you're a patient person, you can follow <a href="https://twitter.com/gstreamer">@gstreamer</a> and wait for GStreamer 1.14 to be released which will include <a href="https://gstreamer.freedesktop.org/download/">Windows, macOS, iOS, and Android binaries</a>.<br><br>The API currently lacks documentation, so the best way to learn it is to dive into the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-bad/tree/tests/examples/webrtc">source-tree examples</a>. Help on this will be most appreciated! To see how to use GStreamer to do WebRTC with a browser, checkout the <a href="https://github.com/centricular/gstwebrtc-demos/">bidirectional audio-video demos</a> that I wrote.<br><br><a name="show-code"></a><h3>Show me the code! <span><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#no-code-pls">[skip]</a></span></h3><br>Here's a quick highlight of the important bits that should get you started if you already know how GStreamer works. This example is in C, but GStreamer also has bindings for <a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/">Rust</a>, <a href="https://cgit.freedesktop.org/gstreamer/gst-python/">Python</a>, <a href="https://github.com/gstreamer-java/gst1-java-core">Java</a>, <a href="https://cgit.freedesktop.org/gstreamer/gstreamer-sharp/">C#</a>, Vala, and so on.<br><br>Let's say you want to capture video from <a href="https://en.wikipedia.org/wiki/Video4Linux">V4L2</a>, stream it to a webrtc peer, and receive video back from it. The first step is the streaming pipeline, which will look something like this:<br><br><!-- HTML generated using hilite.me --><div><pre><span>v4l2src</span> <span>!</span> <span>queue</span> <span>!</span> <span>vp8enc</span> <span>!</span> <span>rtpvp8pay</span> <span>!</span><br>    <span>application</span><span>/</span><span>x</span><span>-</span><span>rtp,media</span><span>=</span><span>video,encoding</span><span>-</span><span>name</span><span>=</span><span>VP8</span><span>,payload</span><span>=</span><span>96</span> <span>!</span> <br>    <span>webrtcbin</span> <span>name</span><span>=</span><span>sendrecv</span><br></pre><table><tbody><tr></tr></tbody></table></div></div><br>As a short-cut, let's parse the string description to create the pipeline.<br><br><!-- HTML generated using hilite.me --><div><table><tbody><tr><td><pre>1<br>2<br>3<br>4<br>5</pre></td><td><pre><span>GstElement</span> <span>*</span><span>pipe;</span><br><br><span>pipe</span> <span>=</span> <span>gst_parse_launch</span> <span>(</span><span>"v4l2src ! queue ! vp8enc ! rtpvp8pay ! "</span><br>    <span>"application/x-rtp,media=video,encoding-name=VP8,payload=96 !"</span><br>    <span>" webrtcbin name=sendrecv"</span><span>,</span> <span>NULL</span><span>);</span><br></pre></td></tr></tbody></table></div><br>Next, we get a reference to the <tt>webrtcbin</tt> element and attach some callbacks to it.<br><br><!-- HTML generated using hilite.me --><div><table><tbody><tr><td><pre>&nbsp;1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19</pre></td><td><pre><span>GstElement</span> <span>*</span><span>webrtc;</span><br><br><span>webrtc</span> <span>=</span> <span>gst_bin_get_by_name</span> <span>(</span><span>GST_BIN</span> <span>(pipe),</span> <span>"sendrecv"</span><span>);</span><br><span>g_assert</span> <span>(webrtc</span> <span>!=</span> <span>NULL</span><span>);</span><br><br><span>/* This is the gstwebrtc entry point where we create the offer.</span><br><span> * It will be called when the pipeline goes to PLAYING. */</span><br><span>g_signal_connect</span> <span>(webrtc,</span> <span>"on-negotiation-needed"</span><span>,</span><br>    <span>G_CALLBACK</span> <span>(on_negotiation_needed),</span> <span>NULL</span><span>);</span><br><span>/* We will transmit this ICE candidate to the remote using some</span><br><span> * signalling. Incoming ICE candidates from the remote need to be</span><br><span> * added by us too. */</span><br><span>g_signal_connect</span> <span>(webrtc,</span> <span>"on-ice-candidate"</span><span>,</span><br>    <span>G_CALLBACK</span> <span>(send_ice_candidate_message),</span> <span>NULL</span><span>);</span><br><span>/* Incoming streams will be exposed via this signal */</span><br><span>g_signal_connect</span> <span>(webrtc,</span> <span>"pad-added"</span><span>,</span><br>    <span>G_CALLBACK</span> <span>(on_incoming_stream),</span> <span>pipe);</span><br><span>/* Lifetime is the same as the pipeline itself */</span><br><span>gst_object_unref</span> <span>(webrtc);</span><br></pre></td></tr></tbody></table></div><br>When the pipeline goes to PLAYING, the <tt>on_negotiation_needed()</tt> callback will be called, and we will ask <tt>webrtcbin</tt> to create an offer which will match the pipeline above.<br><br><!-- HTML generated using hilite.me --><div><table><tbody><tr><td><pre>&nbsp;1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10</pre></td><td><pre><span>static</span> <span>void</span><br><span>on_negotiation_needed</span> <span>(GstElement</span> <span>*</span> <span>webrtc,</span> <span>gpointer</span> <span>user_data)</span><br><span>{</span><br>  <span>GstPromise</span> <span>*</span><span>promise;</span><br><br>  <span>promise</span> <span>=</span> <span>gst_promise_new_with_change_func</span> <span>(on_offer_created,</span><br>      <span>user_data,</span> <span>NULL</span><span>);</span><br>  <span>g_signal_emit_by_name</span> <span>(webrtc,</span> <span>"create-offer"</span><span>,</span> <span>NULL</span><span>,</span><br>      <span>promise);</span><br><span>}</span><br></pre></td></tr></tbody></table></div><br>When webrtcbin has created the offer, it will call <tt>on_offer_created()</tt><br><br><!-- HTML generated using hilite.me --><div><table><tbody><tr><td><pre>&nbsp;1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21</pre></td><td><pre><span>static</span> <span>void</span><br><span>on_offer_created</span> <span>(</span><span>GstPromise</span> <span>*</span> <span>promise,</span> <span>GstElement</span> <span>*</span> <span>webrtc)</span><br><span>{</span><br>  <span>GstWebRTCSessionDescription</span> <span>*</span><span>offer</span> <span>=</span> <span>NULL</span><span>;</span><br>  <span>const</span> <span>GstStructure</span> <span>*</span><span>reply;</span><br>  <span>gchar</span> <span>*</span><span>desc;</span><br><br>  <span>reply</span> <span>=</span> <span>gst_promise_get_reply</span> <span>(promise);</span><br>  <span>gst_structure_get</span> <span>(reply,</span> <span>"offer"</span><span>,</span><br>      <span>GST_TYPE_WEBRTC_SESSION_DESCRIPTION,</span> <br>      <span>&amp;</span><span>offer,</span> <span>NULL</span><span>);</span><br>  <span>gst_promise_unref</span> <span>(promise);</span><br><br>  <span>/* We can edit this offer before setting and sending */</span><br>  <span>g_signal_emit_by_name</span> <span>(webrtc,</span><br>      <span>"set-local-description"</span><span>,</span> <span>offer,</span> <span>NULL</span><span>);</span><br><br>  <span>/* Implement this and send offer to peer using signalling */</span><br>  <span>send_sdp_offer</span> <span>(offer);</span><br>  <span>gst_webrtc_session_description_free</span> <span>(offer);</span><br><span>}</span><br></pre></td></tr></tbody></table></div><br>Similarly, when we have the SDP <tt>answer</tt> from the remote, we must call <tt>"set-remote-description"</tt> on <tt>webrtcbin</tt>. <br><br><!-- HTML generated using hilite.me --><div><table><tbody><tr><td><pre>1<br>2<br>3<br>4<br>5<br>6<br>7</pre></td><td><pre><span>answer</span> <span>=</span> <span>gst_webrtc_session_description_new</span> <span>(</span><br>    <span>GST_WEBRTC_SDP_TYPE_ANSWER,</span> <span>sdp);</span><br><span>g_assert</span> <span>(answer);</span><br><br><span>/* Set remote description on our pipeline */</span><br><span>g_signal_emit_by_name</span> <span>(webrtc,</span> <span>"set-remote-description"</span><span>,</span><br>    <span>answer,</span> <span>NULL</span><span>);</span><br></pre></td></tr></tbody></table></div><br>ICE handling is very similar; when the <tt>"on-ice-candidate"</tt> signal is emitted, we get a local ICE candidate which we must <a href="https://github.com/centricular/gstwebrtc-demos/blob/master/sendrecv/gst/webrtc-sendrecv.c#L167">send to the remote</a>. When we have an ICE candidate from the remote, we must <a href="https://github.com/centricular/gstwebrtc-demos/blob/master/sendrecv/gst/webrtc-sendrecv.c#L500">call</a> <tt>"add-ice-candidate"</tt> on <tt>webrtcbin</tt>.<br><br>There's just one piece left now; handling incoming streams that are sent by the remote. For that, we have <tt>on_incoming_stream()</tt> attached to the <tt>"pad-added"</tt> signal on <tt>webrtcbin</tt>.<br><br><!-- HTML generated using hilite.me --><div><table><tbody><tr><td><pre>&nbsp;1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15</pre></td><td><pre><span>static</span> <span>void</span><br><span>on_incoming_stream</span> <span>(</span><span>GstElement</span> <span>*</span> <span>webrtc,</span> <span>GstPad</span> <span>*</span> <span>pad,</span><br>    <span>GstElement</span> <span>*</span> <span>pipe)</span><br><span>{</span><br>  <span>GstElement</span> <span>*</span><span>play;</span><br><br>  <span>play</span> <span>=</span> <span>gst_parse_bin_from_description</span> <span>(</span><br>      <span>"queue ! vp8dec ! videoconvert ! autovideosink"</span><span>,</span><br>      <span>TRUE</span><span>,</span> <span>NULL</span><span>);</span><br>  <span>gst_bin_add</span> <span>(</span><span>GST_BIN</span> <span>(pipe),</span> <span>play);</span><br><br>  <span>/* Start displaying video */</span><br>  <span>gst_element_sync_state_with_parent</span> <span>(play);</span><br>  <span>gst_element_link</span> <span>(webrtc,</span> <span>play);</span><br><span>}</span><br></pre></td></tr></tbody></table></div><br>That's it! This is what a basic webrtc workflow looks like. Those of you that have used the <tt>PeerConnection</tt> API before will be happy to see that this maps to that quite closely.<br><br><a name="no-code-pls"></a>The <a href="https://github.com/centricular/gstwebrtc-demos/">aforementioned demos</a> also include a Websocket signalling server and JS browser components, and I will be doing an in-depth application newbie developer's guide at a later time, so you can <a href="https://twitter.com/nirbheek" target="_top">follow me @nirbheek</a> to hear when it comes out!<br><br><h3>Tell me more!</h3><br>The code is already being used in production in a number of places, such as <a href="http://www.easymile.com/" target="_top">EasyMile</a>'s autonomous vehicles, and we're excited to see where else the community can take it.<br><div name="why-gst"><br></div>If you're wondering why we decided a new implementation was needed, read on! For a more detailed discussion into that, you should watch <a href="https://gstconf.ubicast.tv/videos/gstreamer-webrtc/">Matthew Waters' talk</a> from the <a href="https://gstreamer.freedesktop.org/conference/2017/">GStreamer conference last year</a>. It's a great companion for this article!<br><br>But before we can dig into  details, we need to lay some foundations first.  <br><br><h3 name="what-is">What is GStreamer, and what is WebRTC? <span><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#why-build">[skip]</a></span></h3><div><br><b>GStreamer</b> is a cross-platform <a href="https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html">open-source multimedia framework</a> that is, in my opinion, the easiest and most flexible way to implement any application that needs to play, record, or transform media-like data across an extremely versatile scale of devices and products. Embedded (IoT, IVI, phones, TVs, …), desktop (video/music players, video recording, non-linear editing, videoconferencing and <a href="https://en.wikipedia.org/wiki/Voice_over_IP">VoIP</a> clients, browsers …), to servers (encode/transcode farms, video/voice conferencing servers, …) and <a href="https://wiki.ligo.org/DASWG/GstLAL">more</a>.</div><div><br></div><div>But what I like the most about GStreamer is the pipeline-based model which solves one of the hardest problems in API design: catering to applications of varying complexity; from the simplest one-liners and quick solutions to those that need several hundreds of thousands of lines of code to implement their full featureset.&nbsp;</div><div><br></div><div>If you want to learn more about GStreamer, <a href="https://www.youtube.com/watch?v=ZphadMGufY8">Jan Schmidt's tutorial</a> from <a href="http://lca2018.linux.org.au/">Linux.conf.au</a> is a good start.</div><div><br></div><div><b>WebRTC</b> is a set of draft specifications that build upon existing <a href="https://en.wikipedia.org/wiki/Real-time_Transport_Protocol">RTP</a>, <a href="https://en.wikipedia.org/wiki/RTP_Control_Protocol">RTCP</a>, <a href="https://en.wikipedia.org/wiki/Session_Description_Protocol">SDP</a>, <a href="https://en.wikipedia.org/wiki/Datagram_Transport_Layer_Security">DTLS</a>, <a href="https://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment">ICE</a> (and many other) real-time communication specifications and defines an API for making <abbr title="Real-time Communication">RTC</abbr> accessible using browser JS APIs.</div><div><br></div><div>People have been doing real-time communication over <a href="https://en.wikipedia.org/wiki/Internet_Protocol">IP</a> for <a href="https://en.wikipedia.org/wiki/Session_Initiation_Protocol">decades</a> with the previously-listed protocols that WebRTC builds upon. The real innovation of WebRTC was creating a bridge between native  applications and webapps by defining a standard, yet flexible, API that browsers can expose to untrusted JavaScript code.</div><div><br></div><div>These specifications are <a href="https://datatracker.ietf.org/wg/rtcweb/documents/">constantly</a> being <a href="https://datatracker.ietf.org/wg/rmcat/documents/">improved upon</a>, which combined with the ubiquitous nature of browsers means WebRTC is fast becoming the standard choice for  videoconferencing on all platforms and for most applications.</div><br><a name="why-build"></a><h3>Everything is great, let's build amazing apps! <span><a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#why-gst">[skip]</a></span></h3><div><br>Not so fast, there's more to the story! For WebApps, the <a href="https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection">PeerConnection API</a> is <a href="https://caniuse.com/#feat=rtcpeerconnection">everywhere</a>. There are some browser-specific quirks as usual, and the API itself keeps changing, but  the <a href="https://github.com/webrtc/adapter">WebRTC JS adapter</a> handles most of that. Overall the WebApp experience is mostly 👍.</div><div><br></div><div>Sadly, for native code or applications that need more flexibility than a sandboxed JS app can achieve, there <i>haven't</i> been a lot of great options.</div><div><br></div><div><a href="http://webrtc.org/" target="_top">libwebrtc</a> (Chrome's implementation), <a href="https://janus.conf.meetecho.com/" target="_top">Janus</a>, <a href="https://www.kurento.org/kurento-architecture" target="_top">Kurento</a>, and <a href="https://en.wikipedia.org/wiki/OpenWebRTC" target="_top">OpenWebRTC</a> have traditionally been the main contenders, but after having worked with all of these, we found that each implementation has its own inflexibilities, shortcomings, and constraints.</div><div><br></div><div><b>libwebrtc</b> is still the most mature implementation, but it is also the most difficult to work with. Since it's embedded inside Chrome, it's a moving target, the API can be hard to work with, and the project <a href="https://webrtchacks.com/building-webrtc-from-source/" target="_top">is quite difficult to build and integrate</a>, all of which are obstacles in the way of native or server app developers trying to quickly prototype and try out things.</div><div><br></div><div>It was also not built for multimedia use-cases, so while the webrtc bits are great, the lower layers get in the way of non-browser use-cases and applications. It is quite painful to do anything other than the default "set raw media, transmit" and "receive from remote, get raw media". This means that if you want to use your own filters, or hardware-specific codecs or sinks/sources, you end up having to fork libwebrtc.</div><br><div>In contrast, <a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer#show-code">as shown above</a>, our implementation gives you full control over this as with any other <a href="https://gstreamer.freedesktop.org/documentation/application-development/introduction/basics.html" target="_top">GStreamer pipeline</a>.</div><div><br></div><div><b>OpenWebRTC</b> by Ericsson was the first attempt to rectify this situation, and it was built on top of GStreamer. The target audience was app developers, and it fit the bill quite well as a proof-of-concept—even though it used a custom API and some of the architectural decisions made it quite inflexible for most other use-cases.</div><div><br></div><div>However, after an initial flurry of activity around the project, momentum petered out, the project failed to gather a community around itself, and is now <a href="https://www.youtube.com/watch?v=npjOSLCR2hE" target="_top">effectively dead</a>.</div><div><br></div><div><i>Full disclosure: <a href="https://centricular.com/">we</a> worked with Ericsson to polish some of the rough edges around the project immediately prior to its public release.</i><br></div><br><a name="why-gst"></a><h3>WebRTC in GStreamer — webrtcbin and gstwebrtc</h3><div><br>Remember how I said the WebRTC standards build upon existing standards and protocols? As it so happens, GStreamer has supported almost all of them for a while now because they were being used for real-time communication, live streaming, and in many other <a href="https://en.wikipedia.org/wiki/Internet_Protocol">IP-based</a> applications. Indeed, that's partly why Ericsson chose it as the base for <abbr title="OpenWebRTC">OWRTC</abbr>.</div><div name="why-gst"><br></div><div>This combined with the SRTP and DTLS plugins that were written during OWRTC's development meant that<i> </i>our implementation is built upon a solid and well-tested base, and that implementing WebRTC features is not as difficult as one might presume. However, WebRTC is a large collection of standards, and reaching feature-parity with libwebrtc is an ongoing task.<br><br>Lucky for us, <a href="https://github.com/ystreet/">Matthew</a> made some excellent decisions while architecting the internals of webrtcbin, and we follow the PeerConnection specification quite closely, so almost all the missing features involve writing code that would plug into clearly-defined sockets.</div><div name="why-gst"><br></div><div>We believe what we've been building here is the most flexible, versatile, and easy to use WebRTC implementation out there, and it can only get better as time goes by. Bringing the power of pipeline-based multimedia manipulation to WebRTC opens new doors for interesting, unique, and highly efficient applications.<br><br>To demonstrate this, in the near future we will be publishing articles that dive into how to use the PeerConnection-inspired API exposed by webrtcbin to build various kinds of applications—starting with a CPU-efficient multi-party bidirectional conferencing solution with a mesh topology that can work with any webrtc stack.<br><br>Until next time!</div></div>

<p class="date">
<a href="http://blog.nirbheek.in/2018/02/gstreamer-webrtc.html">by Nirbheek (noreply@blogger.com) at February 05, 2018 02:28 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">January 26, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2793" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> — <a href="https://blogs.gnome.org/uraeus/2018/01/26/an-update-on-pipewire-the-multimedia-revolution-an-update/">An update on Pipewire – the multimedia revolution</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>We launched <a href="https://pipewire.org/">PipeWire</a> last September with <a href="https://blogs.gnome.org/uraeus/2017/09/19/launching-pipewire/">this blog entry</a>. I thought it would be interesting for people to hear about the latest progress on what I believe is going to be a gigantic step forward for the Linux desktop. So I caught up with Pipewire creator <a href="https://twitter.com/wtaymans">Wim Taymans</a> during <a href="https://devconf.cz/cz/2018">DevConf 2018 in Brno</a> where Wim is doing a talk about Pipewire and we discussed the current state of the code and Wim demonstrated a few of the things that PipeWire now can do.<br>
</p><div id="attachment_2796" class="wp-caption aligncenter"><a href="https://blogs.gnome.org/uraeus/files/2018/01/wimchristianpipewire.jpg"><img src="wimchristianpipewire-300x170.jpg" alt="Christian Schaller and Wim Taymans testing PipeWire with Cheese" class="size-medium wp-image-2796" width="300" height="170"></a><p class="wp-caption-text">Christian Schaller and Wim Taymans testing PipeWire with Cheese</p></div><p></p>
<h2>Priority number 1: video handling</h2>
<p>So as we said when we launched the top priority for PipeWire is to address our needs on the video side of multimedia. This is critical due to the more secure nature of Wayland, which makes the old methods for screen sharing not work anymore and the emergence of desktop containers in the form of <a href="http://www.flatpak.org/">Flatpak</a>. Thus we need PipeWire to help us provide appliation and desktop developers with a new method for doing screen sharing and also to provide a secure way for applications inside a container to access audio and video devices on the system.</p>
<p>There are 3 major challenges PipeWire wants to solve for video. One is device sharing, meaning that multiple applications can share the same video hardware device, second it wants to be able to do so in a secure manner, ensuring your video streams are not highjacked by a rogue process and finally it wants to provide an efficient method for sharing of multimedia between applications, like for instance fullscreen capture from your compositor (like GNOME Shell) to your video conferencing application running in your browser like Google Hangouts, Blue Jeans or Pexip.</p>
<p>So the first thing Wim showed me in action was the device sharing. We launched the GNOME photoboot application <a href="https://wiki.gnome.org/action/show/Cheese?action=show&amp;redirect=Apps%2FCheese">Cheese</a> which gets PipeWire support for free thanks to the PipeWire GStreamer plugin. And this is an important thing to remember, thanks to so many Linux applications using GStreamer these days we don’t need to port each one of them to PipeWire, instead the PipeWire GStreamer plugin does the ‘porting’ for us. We then launched a gst-launch command line pipeline in a terminal. The result is two applications sharing the same webcam input without one of them blocking access for the other.</p>
<p><a href="https://blogs.gnome.org/uraeus/files/2018/01/pipewire-cheese.png"><img src="pipewire-cheese-300x169.png" alt="Cheese and GStreamer pipeline running on Pipewiere" class="aligncenter size-medium wp-image-2795" width="300" height="169"></a></p>
<p>As you can see from the screenshot above it worked fine, and this was actually done on my Fedora Workstation 27 system and the only thing we had to do was to start the ‘pipewire’ process in a termal before starting Cheese and the gst-launch pipeline. GStreamer autoplugging took care of the rest. So feel free to try this out yourself if you are interested, but be aware that you will find bugs quickly if you try things like on the fly resolution changes or switching video devices. This is still tech preview level software in Fedora 27.</p>
<p>The plan is for Wim Taymans to sit down with the web browser maintainers at Red Hat early next week and see if we can make progress on supporting PipeWire in Firefox and Chrome, so that conferencing software like the ones mentioned above can start working fully under Wayland.</p>
<p>Since security was one of the drivers for the move to Wayland from X Windows we of course also put a lot of emphasis of not recreating the security holes of X in the compositor. So the way PipeWire now works is that if an application wants to do full screen capture it will check with the compositor through a dbus-api, or a portal in Flatpak and Wayland terminology, and only allows the permited application to do the screen capture, so the stream can’t be highjacked by a random rougue application or process on your computer. This also works from within a sandboxed setting like Flatpaks.</p>
<h2>Jack Support</h2>
<p>Another important goal of PipeWire was to bring all Linux audio and video together, which means PipeWire needed to be as good or better replacement for <a href="http://www.jackaudio.org/">Jack</a> for the Pro-Audio usecase. This is a tough usecase to satisfy so while getting the video part has been the top development priority Wim has also worked on verifying that the design allows for the low latency and control needed for Pro-Audio. To do this Wim has implemented the Jack protocol on top of PipeWire.<br>
</p><div id="attachment_2800" class="wp-caption aligncenter"><a href="https://blogs.gnome.org/uraeus/files/2018/01/carla-pipewire.png"><img src="carla-pipewire-300x210.png" alt="" class="size-medium wp-image-2800" width="300" height="210"></a><p class="wp-caption-text">Carla, a Jack application running on top of PipeWire.</p></div><br>
Through that work he has now verified that he is able to achieve the low latency needed for pro-audio with PipeWire and that he will be able to run Jack applications without changes on top of PipeWire. So above you see a screenshot of Carla, a Jack-based application running on top of PipeWire with no Jack server running on the system.<p></p>
<h2>ALSA/Legacy applications</h2>
<p>Another item Wim has written the first code for and verfied will work well is the Alsa emulation. The goal of this piece of code is to allow applications using the ALSA userspace API to output to Pipewire without needing special porting or application developer effort. At Red Hat we have many customers with older bespoke applications using this API so it has been of special interest for us to ensure this works just as well as the native ALSA output. It is also worth nothing that Pipewire also does mixing so that sound being routed through ALSA will get seamlessly mixed with audio coming through the Jack layer.</p>
<h2>Bluetooth support</h2>
<p>The last item Wim has spent some time on since last September is working on making sure Bluetooth output works and he demonstrated this to me while we where talking together during DevConf. The Pipewire bluetooth module plugs directly into the Bluez Bluetooth framework, meaning that things like the GNOME Bluetooth control panel just works with it without any porting work needed. And while the code is still quite young, Wim demonstrated pairing and playing music over bluetooth using it to me.</p>
<h2>What about PulseAudio?</h2>
<p>So as you probably noticed one thing we didn’t mention above is how to deal with PulseAudio applications. Handling this usecase is still on the todo list and the plan is to at least initially just keep PulseAudio running on the system outputing its sound through PipeWire. That said we are a bit unsure how many appliations would actually be using this path because as mentioned above all GStreamer applications for instance would be PipeWire native automatically through the PipeWire GStreamer plugins. And for legacy applications the PipeWire ALSA layer would replace the current PulseAudio ALSA layer as the default ALSA output, meaning that the only applications left are those outputing to PulseAudio directly themselves. The plan would also be to keep the PulseAudio ALSA device around so if people want to use things like the PulseAudio networked audio functionality they can choose the PA ALSA device manually to be able to keep doing so.<br>
Over time the goal would of course be to not have to keep the PulseAudio daemon around, but dropping it completely is likely to be a multiyear process with current plans, so it is kinda like XWayland on top of Wayland.</p>
<h2>Summary</h2>
<p>So you might read this and think, hey if all this work we are almost done right? Well unfortunately no, the components mentioned here are good enough for us to verify the design and features, but they still need a lot of maturing and testing before they will be in a state where we can consider switching Fedora Workstation over to using them by default. So there are many warts that needs to be cleaned up still, but a lot of things have become a lot more tangible now than when we last spoke about PipeWire in September. The video handling we hope to enable in Fedora Workstation 28 as mentioned, while the other pieces we will work towards enabling in later releases as the components mature.<br>
Of course the more people interesting in joining the PipeWire community to help us out, the quicker we can mature these different pieces. So if you are interested please join us in #pipewire on irc.freenode.net or just clone the code of github and start hacking. You find the details <a href="https://pipewire.org/#get-involved">for irc and git here</a>.</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2018/01/26/an-update-on-pipewire-the-multimedia-revolution-an-update/">by uraeus at January 26, 2018 02:22 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">January 21, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=525" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net – slomo's blog">Sebastian Dröge</a> — <a href="https://coaxion.net/blog/2018/01/speeding-up-rgb-to-grayscale-conversion-in-rust-by-a-factor-of-2-2-and-various-other-multimedia-related-processing-loops/">Speeding up RGB to grayscale conversion in Rust by a factor of 2.2 – and various other multimedia related processing loops</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dröge)" width="80" height="80">
<p>In the <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/" rel="noopener" target="_top">previous blog post</a> I wrote about how to write a RGB to grayscale conversion filter for <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> in <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a>. In this blog post I’m going to write about how to optimize the processing loop of that filter, without resorting to <em>unsafe</em> code or <a href="https://en.wikipedia.org/wiki/SIMD" rel="noopener" target="_top">SIMD</a> instructions by staying with plain, safe Rust code.</p>
<p>I also tried to implement the processing loop with <a href="https://github.com/AdamNiederer/faster" rel="noopener" target="_top">faster</a>, a Rust crate for writing safe SIMD code. It looks very promising, but unless I missed something in the documentation it currently is missing some features to be able to express this specific algorithm in a meaningful way. Once it works on stable Rust (waiting for SIMD to be stabilized) and includes runtime CPU feature detection, this could very well be a good replacement for the <a href="https://cgit.freedesktop.org/gstreamer/orc" rel="noopener" target="_top">ORC</a> library used for the same purpose in GStreamer in various places. ORC works by JIT-compiling a minimal “array operation language” to SIMD assembly for your specific CPU (and has support for x86 MMX/SSE, PPC Altivec, ARM NEON, etc.).</p>
<p>If someone wants to prove me wrong and implement this with faster, feel free to do so and I’ll link to your solution and include it in the benchmark results below.</p>
<p>All code below can be found in this <a href="https://github.com/sdroege/bgrx-to-grayscale-benchmark.rs" rel="noopener" target="_top">GIT repository</a>.</p>
<h3 id="toc">Table of Contents</h3>
<ol>
<li><a href="https://coaxion.net/blog/feed/#baseline">Baseline Implementation</a></li>
<li><a href="https://coaxion.net/blog/feed/#assertions">First Optimization – Assertions</a></li>
<li><a href="https://coaxion.net/blog/feed/#assertions-2">First Optimization – Assertions Try 2</a>
</li><li><a href="https://coaxion.net/blog/feed/#iterator">Second Optimization – Iterate a bit more</a></li>
<li><a href="https://coaxion.net/blog/feed/#no-more-bounds">Third Optimization – Getting rid of the bounds check finally</a></li>
<li><a href="https://coaxion.net/blog/feed/#summary">Summary</a></li>
<li><a href="https://coaxion.net/blog/feed/#split-at">Addendum: <em>slice::split_at</em></a><a></a></li>
<li><a href="https://coaxion.net/blog/feed/#faster">Addendum 2: SIMD with faster</a><a></a></li>
</ol>
<h3 id="baseline">Baseline Implementation</h3>
<p>This is how the baseline implementation looks like.</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_chunks_no_asserts(
    in_data: &amp;[u8],
    out_data: &amp;mut [u8],
    in_stride: usize,
    out_stride: usize,
    width: usize,
) {
    let in_line_bytes = width * 4;
    let out_line_bytes = width * 4;

    for (in_line, out_line) in in_data
        .chunks(in_stride)
        .zip(out_data.chunks_mut(out_stride))
    {
        for (in_p, out_p) in in_line[..in_line_bytes]
            .chunks(4)
            .zip(out_line[..out_line_bytes].chunks_mut(4))
        {
            let b = u32::from(in_p[0]);
            let g = u32::from(in_p[1]);
            let r = u32::from(in_p[2]);
            let x = u32::from(in_p[3]);

            let grey = ((r * RGB_Y[0]) + (g * RGB_Y[1]) + (b * RGB_Y[2]) + (x * RGB_Y[3])) / 65536;
            let grey = grey as u8;
            out_p[0] = grey;
            out_p[1] = grey;
            out_p[2] = grey;
            out_p[3] = grey;
        }
    }
}</pre><p> </p>
<p>This basically iterates over each line of the input and output frame (outer loop), and then for each BGRx chunk of 4 bytes in each line it converts the values to <em>u32</em>, multiplies with a constant array, converts back to <em>u8</em> and stores the same value in the whole output BGRx chunk.</p>
<p><strong>Note:</strong> This is only doing the actual conversion from linear RGB to grayscale (and in <a href="https://en.wikipedia.org/wiki/YUV#SDTV_with_BT.601" rel="noopener" target="_top">BT.601</a> colorspace). To do this conversion correctly you need to know your colorspaces and use the correct coefficients for conversion, and also do <a href="https://en.wikipedia.org/wiki/Gamma_correction" rel="noopener" target="_top">gamma correction</a>. See <a href="https://web.archive.org/web/20161024090830/http://www.4p8.com/eric.brasseur/gamma.html" rel="noopener" target="_top">this</a> about why it is important.</p>
<p>So what can be improved on this? For starters, let’s write a small benchmark for this so that we know whether any of our changes actually improve something. This is using the (unfortunately <a href="https://github.com/rust-lang/rfcs/pull/2287" rel="noopener" target="_top">still</a>) unstable benchmark feature of Cargo.</p>
<p></p><pre class="crayon-plain-tag">#![feature(test)]
#![feature(exact_chunks)]

extern crate test;

pub fn bgrx_to_gray_chunks_no_asserts(...)
    [...]
}

#[cfg(test)]
mod tests {
    use super::*;
    use test::Bencher;
    use std::iter;

    fn create_vec(w: usize, h: usize) -&gt; Vec&lt;u8&gt; {
        iter::repeat(0).take(w * h * 4).collect::&lt;_&gt;()
    }

    #[bench]
    fn bench_chunks_1920x1080_no_asserts(b: &amp;mut Bencher) {
        let i = test::black_box(create_vec(1920, 1080));
        let mut o = test::black_box(create_vec(1920, 1080));

        b.iter(|| bgrx_to_gray_chunks_no_asserts(&amp;i, &amp;mut o, 1920 * 4, 1920 * 4, 1920));
    }
}</pre><p> </p>
<p>This can be run with <em>cargo bench</em> and then prints the amount of nanoseconds each iterator of the closure was taking. To only really measure the processing itself, allocations and initializations of the input/output frame are happening outside of the closure. We’re not interested in times for that.</p>
<h3 id="assertions">First Optimization – Assertions</h3>
<p>To actually start optimizing this function, let’s take a look at the assembly that the compiler is outputting. The easiest way of doing that is via the <a href="https://godbolt.org/" rel="noopener" target="_top">Godbolt Compiler Explorer</a> website. Select “rustc nightly” and use <em>“-C opt-level=3”</em> for the compiler flags, and then copy &amp; paste your code in there. Once it compiles, to find the assembly that corresponds to a line, simply right-click on the line and “Scroll to assembly”.</p>
<p>Alternatively you can use <em>cargo rustc –release — -C opt-level=3 –emit asm</em> and check the assembly file that is output in the <em>target/release/deps</em> directory.</p>
<p>What we see then for our inner loop is something like the following</p>
<p></p><pre class="crayon-plain-tag">.LBB4_19:
  cmp r15, r11
  mov r13, r11
  cmova r13, r15
  mov rdx, r8
  sub rdx, r13
  je .LBB4_34
  cmp rdx, 3
  jb .LBB4_35
  inc r9
  movzx edx, byte ptr [rbx - 1]
  movzx ecx, byte ptr [rbx - 2]
  movzx esi, byte ptr [rbx]
  imul esi, esi, 19595
  imul edx, edx, 38470
  imul ecx, ecx, 7471
  add ecx, edx
  add ecx, esi
  shr ecx, 16
  mov byte ptr [r10 - 3], cl
  mov byte ptr [r10 - 2], cl
  mov byte ptr [r10 - 1], cl
  mov byte ptr [r10], cl
  add r10, 4
  add r8, -4
  add r15, -4
  add rbx, 4
  cmp r9, r14
  jb .LBB4_19</pre><p> </p>
<p>This is already quite optimized. For each loop iteration the first few instructions are doing some bounds checking and if they fail jump to the <em>.LBB4_34</em> or <em>.LBB4_35</em> labels. How to understand that this is bounds checking? Scroll down in the assembly to where these labels are defined and you’ll see something like the following</p>
<p></p><pre class="crayon-plain-tag">.LBB4_34:
  lea rdi, [rip + .Lpanic_bounds_check_loc.D]
  xor esi, esi
  xor edx, edx
  call core::panicking::panic_bounds_check@PLT
  ud2
.LBB4_35:
  cmp r15, r11
  cmova r11, r15
  sub r8, r11
  lea rdi, [rip + .Lpanic_bounds_check_loc.F]
  mov esi, 2
  mov rdx, r8
  call core::panicking::panic_bounds_check@PLT
  ud2</pre><p> </p>
<p>Also if you check (with the colors, or the “scroll to source” feature) which Rust code these correspond to, you’ll see that it’s the first and third access to the 4-byte slice that contains our BGRx values.</p>
<p>Afterwards in the assembly, the following steps are happening: 0) incrementing of the “loop counter” representing the number of iterations we’re going to do (<em>r9</em>), 1) actual reading of the B, G and R value and conversion to <em>u32</em> (the 3 <em>movzx</em>, note that the reading of the <em>x</em> value is optimized away as the compiler sees that it is always multiplied by 0 later), 2) the multiplications with the array elements (the 3 <em>imul</em>), 3) combining of the results and division (i.e. shift) (the 2 <em>add</em> and the <em>shr</em>), 4) storing of the result in the output (the 4 <em>mov</em>). Afterwards the slice pointers are increased by 4 (<em>rbx</em> and <em>r10</em>) and the lengths (used for bounds checking) are decreased by 4 (<em>r8</em> and <em>r15</em>). Finally there’s a check (<em>cmp</em>) to see if <em>r9</em> (our loop) counter is at the end of the slice, and if not we jump back to the beginning and operate on the next BGRx chunk.</p>
<p>Generally what we want to do for optimizations is to get rid of unnecessary checks (bounds checking), memory accesses, conditions (<em>cmp</em>, <em>cmov</em>) and jumps (the instructions starting with <em>j</em>). These are all things that are slowing down our code.</p>
<p>So the first thing that seems useful to optimize here is the bounds checking at the beginning. It definitely seems not useful to do two checks instead of one for the two slices (the checks are for the both slices at once but Godbolt does not detect that and believes it’s only the input slice). And ideally we could teach the compiler that no bounds checking is needed at all.</p>
<p>As I wrote in the previous blog post, often this knowledge can be given to the compiler by inserting assertions.</p>
<p>To prevent two checks and just have a single check, you can insert a <em>assert_eq!(in_p.len(), 4)</em> at the beginning of the inner loop and the same for the output slice. Now we only have a single bounds check left per iteration.</p>
<p>As a next step we might want to try to move this knowledge outside the inner loop so that there is no bounds checking at all in there anymore. We might want to add assertions like the following outside the outer loop then to give all knowledge we have to the compiler</p>
<p></p><pre class="crayon-plain-tag">assert_eq!(in_data.len() % 4, 0);
assert_eq!(out_data.len() % 4, 0);
assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

assert!(in_line_bytes &lt;= in_stride);
assert!(out_line_bytes &lt;= out_stride);</pre><p></p>
<p>Unfortunately adding those has no effect at all on the inner loop, but having them outside the outer loop for good measure is not the worst idea so let’s just keep them. At least it can be used as some kind of documentation of the invariants of this code for future readers.</p>
<p>So let’s benchmark these two implementations now. The results on my machine are the following</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_no_asserts ... bench:   4,420,145 ns/iter (+/- 139,051)
test tests::bench_chunks_1920x1080_asserts    ... bench:   4,897,046 ns/iter (+/- 166,555)</pre><p></p>
<p>This is surprising, our version without the assertions is actually faster by a factor of ~1.1 although it had fewer conditions. So let’s take a closer look at the assembly at the top of the loop again, where the bounds checking happens, in the version with assertions</p>
<p></p><pre class="crayon-plain-tag">.LBB4_19:
  cmp rbx, r11
  mov r9, r11
  cmova r9, rbx
  mov r14, r12
  sub r14, r9
  lea rax, [r14 - 1]
  mov qword ptr [rbp - 120], rax
  mov qword ptr [rbp - 128], r13
  mov qword ptr [rbp - 136], r10
  cmp r14, 5
  jne .LBB4_33
  inc rcx
  [...]</pre><p> </p>
<p>While this indeed has only one jump as expected for the bounds checking, the number of comparisons is the same and even worse: 3 memory writes to the stack are happening right before the jump. If we follow to the <em>.LBB4_33</em> label we will see that the <em>assert_eq!</em> macro is going to do something with <em>core::fmt::Debug</em>. This is setting up the information needed for printing the assertion failure, the “expected X equals to Y” output. This is certainly not good and the reason why everything is slower now.</p>
<h3 id="assertions-2">First Optimization – Assertions Try 2</h3>
<p>All the additional instructions and memory writes were happening because the <em>assert_eq!</em> macro is outputting something user friendly that actually contains the values of both sides. Let’s try again with the <em>assert!</em> macro instead</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_no_asserts ... bench:   4,420,145 ns/iter (+/- 139,051)
test tests::bench_chunks_1920x1080_asserts    ... bench:   4,897,046 ns/iter (+/- 166,555)
test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)</pre><p></p>
<p>This already looks more promising. Compared to our baseline version this gives us a speedup of a factor of 1.12, and compared to the version with <em>assert_eq!</em> 1.23. If we look at the assembly for the bounds checks (everything else stays the same), it also looks more like what we would’ve expected</p>
<p></p><pre class="crayon-plain-tag">.LBB4_19:
  cmp rbx, r12
  mov r13, r12
  cmova r13, rbx
  add r13, r14
  jne .LBB4_33
  inc r9
  [...]</pre><p> </p>
<p>One <em>cmp</em> less, only one jump left. And no memory writes anymore!</p>
<p>So keep in mind that <em>assert_eq!</em> is more user-friendly but quite a bit more expensive even in the “good case” compared to <em>assert!</em>.</p>
<h3 id="iterator">Second Optimization – Iterate a bit more</h3>
<p>This is still not very satisfying though. No bounds checking should be needed at all as each chunk is going to be exactly 4 bytes. We’re just not able to convince the compiler that this is the case. While it may be possible (let me know if you find a way!), let’s try something different. The <em>zip</em> iterator is done when the shortest iterator of both is done, and there are optimizations specifically for zipped slice iterators implemented. Let’s try that and replace the grayscale value calculation with</p>
<p></p><pre class="crayon-plain-tag">let grey = in_p.iter()
    .zip(RGB_Y.iter())
    .map(|(i, c)| u32::from(*i) * c)
    .sum::&lt;u32&gt;() / 65536;</pre><p> </p>
<p>If we run that through our benchmark after removing the <em>assert!(in_p.len() == 4)</em> (and the same for the output slice), these are the results</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)
test tests::bench_chunks_1920x1080_iter_sum   ... bench:  11,393,600 ns/iter (+/- 347,958)</pre><p></p>
<p>We’re actually 2.9 times slower! Even when adding back the <em>assert!(in_p.len() == 4)</em> assertion (and the same for the output slice) we’re still slower</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)
test tests::bench_chunks_1920x1080_iter_sum   ... bench:  11,393,600 ns/iter (+/- 347,958)
test tests::bench_chunks_1920x1080_iter_sum_2 ... bench:  10,420,442 ns/iter (+/- 242,379)</pre><p></p>
<p>If we look at the assembly of the assertion-less variant, it’s a complete mess now</p>
<p></p><pre class="crayon-plain-tag">.LBB0_19:
  cmp rbx, r13
  mov rcx, r13
  cmova rcx, rbx
  mov rdx, r8
  sub rdx, rcx
  cmp rdx, 4
  mov r11d, 4
  cmovb r11, rdx
  test r11, r11
  je .LBB0_20
  movzx ecx, byte ptr [r15 - 2]
  imul ecx, ecx, 19595
  cmp r11, 1
  jbe .LBB0_22
  movzx esi, byte ptr [r15 - 1]
  imul esi, esi, 38470
  add esi, ecx
  movzx ecx, byte ptr [r15]
  imul ecx, ecx, 7471
  add ecx, esi
  test rdx, rdx
  jne .LBB0_23
  jmp .LBB0_35
.LBB0_20:
  xor ecx, ecx
.LBB0_22:
  test rdx, rdx
  je .LBB0_35
.LBB0_23:
  shr ecx, 16
  mov byte ptr [r10 - 3], cl
  mov byte ptr [r10 - 2], cl
  cmp rdx, 3
  jb .LBB0_36
  inc r9
  mov byte ptr [r10 - 1], cl
  mov byte ptr [r10], cl
  add r10, 4
  add r8, -4
  add rbx, -4
  add r15, 4
  cmp r9, r14
  jb .LBB0_19</pre><p> </p>
<p>In short, there are now various new conditions and jumps for short-circuiting the zip iterator in the various cases. And because of all the noise added, the compiler was not even able to optimize the bounds check for the output slice away anymore (<em>.LBB0_35</em> cases). While it was able to unroll the iterator (note that the 3 <em>imul</em> multiplications are not interleaved with jumps and are actually 3 multiplications instead of yet another loop), which is quite impressive, it couldn’t do anything meaningful with that information it somehow got (it must’ve understood that each chunk has 4 bytes!). This looks like something going wrong somewhere in the optimizer to me.</p>
<p>If we take a look at the variant with the assertions, things look much better</p>
<p></p><pre class="crayon-plain-tag">.LBB3_19:
  cmp r11, r12
  mov r13, r12
  cmova r13, r11
  add r13, r14
  jne .LBB3_33
  inc r9
  movzx ecx, byte ptr [rdx - 2]
  imul r13d, ecx, 19595
  movzx ecx, byte ptr [rdx - 1]
  imul ecx, ecx, 38470
  add ecx, r13d
  movzx ebx, byte ptr [rdx]
  imul ebx, ebx, 7471
  add ebx, ecx
  shr ebx, 16
  mov byte ptr [r10 - 3], bl
  mov byte ptr [r10 - 2], bl
  mov byte ptr [r10 - 1], bl
  mov byte ptr [r10], bl
  add r10, 4
  add r11, -4
  add r14, 4
  add rdx, 4
  cmp r9, r15
  jb .LBB3_19</pre><p> </p>
<p>This is literally the same as the assertion version we had before, except that the reading of the input slice, the multiplications and the additions are happening in iterator order instead of being batched all together. It’s quite impressive that the compiler was able to completely optimize away the zip iterator here, but unfortunately it’s still many times slower than the original version. The reason must be the instruction-reordering. The previous version had all memory reads batched and then the operations batched, which is apparently much better for the internal pipelining of the CPU (it is going to perform the next instructions without dependencies on the previous ones already while waiting for the pending instructions to finish).</p>
<p>It’s also not clear to me why the LLVM optimizer is not able to schedule the instructions the same way here. It apparently has all information it needs for that if no iterator is involved, and both versions are leading to exactly the same assembly except for the order of instructions. This also seems like something fishy.</p>
<p>Nonetheless, we still have our manual bounds check (the assertion) left here and we should really try to get rid of that. No progress so far.</p>
<h3 id="no-more-bounds">Third Optimization – Getting rid of the bounds check finally</h3>
<p>Let’s tackle this from a different angle now. Our problem is apparently that the compiler is not able to understand that each chunk is exactly 4 bytes.</p>
<p>So why don’t we write a new chunks iterator that has always exactly the requested amount of items, instead of potentially less for the very last iteration. And instead of panicking if there are leftover elements, it seems useful to just ignore them. That way we have API that is functionally different from the existing chunks iterator and provides behaviour that is useful in various cases. It’s basically the slice equivalent of the <a href="https://docs.rs/ndarray/0.11.0/ndarray/struct.ArrayBase.html#method.exact_chunks" rel="noopener" target="_top">exact_chunks</a> iterator of the <em>ndarray</em> crate.</p>
<p>By having it functionally different from the existing one, and not just an optimization, I also <a href="https://github.com/rust-lang/rust/pull/47126" rel="noopener" target="_top">submitted</a> it for inclusion in Rust’s standard library and it’s nowadays available as an unstable feature in nightly. Like all newly added API. Nonetheless, the same can also be implemented inside your code with basically the same effect, there are no dependencies on standard library internals.</p>
<p>So, let’s use our new <a href="https://doc.rust-lang.org/nightly/std/primitive.slice.html#method.exact_chunks" rel="noopener" target="_top"><em>exact_chunks</em></a> iterator that is guaranteed (by API) to always give us exactly 4 bytes. In our case this is exactly equivalent to the normal chunks as by construction our slices always have a length that is a multiple of 4, but the compiler can’t infer that information. The resulting code looks as follows</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_exact_chunks(
    in_data: &amp;[u8],
    out_data: &amp;mut [u8],
    in_stride: usize,
    out_stride: usize,
    width: usize,
) {
    assert_eq!(in_data.len() % 4, 0);
    assert_eq!(out_data.len() % 4, 0);
    assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

    let in_line_bytes = width * 4;
    let out_line_bytes = width * 4;

    assert!(in_line_bytes &lt;= in_stride);
    assert!(out_line_bytes &lt;= out_stride);

    for (in_line, out_line) in in_data
        .exact_chunks(in_stride)
        .zip(out_data.exact_chunks_mut(out_stride))
    {
        for (in_p, out_p) in in_line[..in_line_bytes]
            .exact_chunks(4)
            .zip(out_line[..out_line_bytes].exact_chunks_mut(4))
        {
            assert!(in_p.len() == 4);
            assert!(out_p.len() == 4);

            let b = u32::from(in_p[0]);
            let g = u32::from(in_p[1]);
            let r = u32::from(in_p[2]);
            let x = u32::from(in_p[3]);

            let grey = ((r * RGB_Y[0]) + (g * RGB_Y[1]) + (b * RGB_Y[2]) + (x * RGB_Y[3])) / 65536;
            let grey = grey as u8;
            out_p[0] = grey;
            out_p[1] = grey;
            out_p[2] = grey;
            out_p[3] = grey;
        }
    }
}</pre><p> </p>
<p>It’s exactly the same as the previous version with assertions, except for using <em>exact_chunks</em> instead of <em>chunks</em> and the same for the mutable iterator. The resulting benchmark of all our variants now looks as follow</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_no_asserts ... bench:   4,420,145 ns/iter (+/- 139,051)
test tests::bench_chunks_1920x1080_asserts    ... bench:   4,897,046 ns/iter (+/- 166,555)
test tests::bench_chunks_1920x1080_asserts_2  ... bench:   3,968,976 ns/iter (+/- 97,084)
test tests::bench_chunks_1920x1080_iter_sum   ... bench:  11,393,600 ns/iter (+/- 347,958)
test tests::bench_chunks_1920x1080_iter_sum_2 ... bench:  10,420,442 ns/iter (+/- 242,379)
test tests::bench_exact_chunks_1920x1080      ... bench:   2,007,459 ns/iter (+/- 112,287)</pre><p></p>
<p>Compared to our initial version this is a speedup of a factor of 2.2, compared to our version with assertions a factor of 1.98. This seems like a worthwhile improvement, and if we look at the resulting assembly there are no bounds checks at all anymore</p>
<p></p><pre class="crayon-plain-tag">.LBB0_10:
  movzx edx, byte ptr [rsi - 2]
  movzx r15d, byte ptr [rsi - 1]
  movzx r12d, byte ptr [rsi]
  imul r13d, edx, 19595
  imul edx, r15d, 38470
  add edx, r13d
  imul ebx, r12d, 7471
  add ebx, edx
  shr ebx, 16
  mov byte ptr [rcx - 3], bl
  mov byte ptr [rcx - 2], bl
  mov byte ptr [rcx - 1], bl
  mov byte ptr [rcx], bl
  add rcx, 4
  add rsi, 4
  dec r10
  jne .LBB0_10</pre><p> </p>
<p>Also due to this the compiler is able to apply some more optimizations and we only have one loop counter for the number of iterations <em>r10</em> and the two pointers <em>rcx</em> and <em>rsi</em> that are increased/decreased in each iteration. There is no tracking of the remaining slice lengths anymore, as in the assembly of the original version (and the versions with assertions).</p>
<h3 id="summary">Summary</h3>
<p>So overall we got a speedup of a factor of 2.2 while still writing very high-level Rust code with iterators and not falling back to unsafe code or using SIMD. The optimizations the Rust compiler is applying are quite impressive and the Rust marketing line of <em>zero-cost abstractions</em> is really visible in reality here.</p>
<p>The same approach should also work for many similar algorithms, and thus many similar multimedia related algorithms where you iterate over slices and operate on fixed-size chunks.</p>
<p>Also the above shows that as a first step it’s better to write clean and understandable high-level Rust code without worrying too much about performance (assume the compiler can optimize well), and only afterwards take a look at the generated assembly and check which instructions should really go away (like bounds checking). In many cases this can be achieved by adding assertions in strategic places, or like in this case by switching to a slightly different abstraction that is closer to the actual requirements (however I believe the compiler should be able to produce the same code with the help of assertions with the normal chunks iterator, but making that possible requires improvements to the LLVM optimizer probably).</p>
<p>And if all does not help, there’s still the escape hatch of <em>unsafe</em> (for using functions like <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.get_unchecked" rel="noopener" target="_top"><em>slice::get_unchecked()</em></a> or going down to raw pointers) and the possibility of using SIMD instructions (by using <a href="https://github.com/AdamNiederer/faster" rel="noopener" target="_top"><em>faster</em></a> or <a href="https://crates.io/crates/stdsimd" rel="noopener" target="_top"><em>stdsimd</em></a> directly). But in the end this should be a last resort for those little parts of your code where optimizations are needed and the compiler can’t be easily convinced to do it for you.</p>
<h3 id="split-at">Addendum: <em>slice::split_at</em></h3>
<p>User <em>newpavlov</em> <a href="https://www.reddit.com/r/rust/comments/7rxrka/speeding_up_rgb_to_grayscale_conversion_in_rust/dt0ejao/" rel="noopener" target="_top">suggested on Reddit</a> to use repeated <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.split_at" rel="noopener" target="_top"><em>slice::split_at</em></a> in a while loop for similar performance.</p>
<p>This would for example like</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_split_at(
    in_data: &amp;[u8],
    out_data: &amp;mut [u8],
    in_stride: usize,
    out_stride: usize,
    width: usize,
) {
    assert_eq!(in_data.len() % 4, 0);
    assert_eq!(out_data.len() % 4, 0);
    assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

    let in_line_bytes = width * 4;
    let out_line_bytes = width * 4;

    assert!(in_line_bytes &lt;= in_stride);
    assert!(out_line_bytes &lt;= out_stride);

    for (in_line, out_line) in in_data
        .exact_chunks(in_stride)
        .zip(out_data.exact_chunks_mut(out_stride))
    {
        let mut in_pp: &amp;[u8] = in_line[..in_line_bytes].as_ref();
        let mut out_pp: &amp;mut [u8] = out_line[..out_line_bytes].as_mut();
        assert!(in_pp.len() == out_pp.len());

        while in_pp.len() &gt;= 4 {
            let (in_p, in_tmp) = in_pp.split_at(4);
            let (out_p, out_tmp) = { out_pp }.split_at_mut(4);
            in_pp = in_tmp;
            out_pp = out_tmp;

            let b = u32::from(in_p[0]);
            let g = u32::from(in_p[1]);
            let r = u32::from(in_p[2]);
            let x = u32::from(in_p[3]);

            let grey = ((r * RGB_Y[0]) + (g * RGB_Y[1]) + (b * RGB_Y[2]) + (x * RGB_Y[3])) / 65536;
            let grey = grey as u8;
            out_p[0] = grey;
            out_p[1] = grey;
            out_p[2] = grey;
            out_p[3] = grey;
        }
    }
}</pre><p> </p>
<p>Performance-wise this brings us very close to the <em>exact_chunks</em> version</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_exact_chunks_1920x1080      ... bench: 1,965,631 ns/iter (+/- 58,832)
test tests::bench_split_at_1920x1080          ... bench: 2,046,834 ns/iter (+/- 35,990)</pre><p></p>
<p>and the assembly is also very similar</p>
<p></p><pre class="crayon-plain-tag">.LBB0_10:
  add rbx, -4
  movzx r15d, byte ptr [rsi]
  movzx r12d, byte ptr [rsi + 1]
  movzx edx, byte ptr [rsi + 2]
  imul r13d, edx, 19595
  imul r12d, r12d, 38470
  imul edx, r15d, 7471
  add edx, r12d
  add edx, r13d
  shr edx, 16
  movzx edx, dl
  imul edx, edx, 16843009
  mov dword ptr [rcx], edx
  lea rcx, [rcx + 4]
  add rsi, 4
  cmp rbx, 3
  ja .LBB0_10</pre><p> </p>
<p>Here the compiler even optimizes the storing of the value into a single write operation of 4 bytes, at the cost of an additional multiplication and zero-extend register move.</p>
<p>Overall this code performs very well too, but in my opinion it looks rather ugly compared to the versions using the different chunks iterators. Also this is basically what the <em>exact_chunks</em> iterator does internally: repeatedly calling <em>slice::split_at</em>. In theory both versions could lead to the very same assembly, but the LLVM optimizer is currently handling both slightly different.</p>
<h3 id="faster">Addendum 2: SIMD with faster</h3>
<p><a href="https://github.com/AdamNiederer" rel="noopener" target="_top">Adam Niederer</a>, author of <a href="https://github.com/AdamNiederer/faster" rel="noopener" target="_top">faster</a>, <a href="https://github.com/sdroege/bgrx-to-grayscale-benchmark.rs/pull/1" rel="noopener" target="_top">provided a PR</a> that implements the same algorithm with faster to make explicit use of SIMD instructions if available.</p>
<p>Due to some codegen issues, this currently has to be compiled with the CPU being selected as <em>nehalem</em>, i.e. by running <em>RUSTFLAGS=”-C target-cpu=nehalem” cargo +nightly bench</em>, but it provides yet another speedup by a factor of up to 1.27x compared to the fastest previous solution and 2.7x compared to the initial solution:</p>
<p></p><pre class="crayon-plain-tag">test tests::bench_chunks_1920x1080_asserts                     ... bench:   4,539,286 ns/iter (+/- 106,265)
test tests::bench_chunks_1920x1080_asserts_2                   ... bench:   3,550,683 ns/iter (+/- 96,917)
test tests::bench_chunks_1920x1080_iter_sum                    ... bench:   5,233,238 ns/iter (+/- 114,671)
test tests::bench_chunks_1920x1080_iter_sum_2                  ... bench:   3,532,059 ns/iter (+/- 94,964)
test tests::bench_chunks_1920x1080_no_asserts                  ... bench:   4,468,269 ns/iter (+/- 89,329)
test tests::bench_chunks_1920x1080_no_asserts_faster           ... bench:   2,476,077 ns/iter (+/- 54,877)
test tests::bench_chunks_1920x1080_no_asserts_faster_unstrided ... bench:   1,642,980 ns/iter (+/- 108,034)
test tests::bench_exact_chunks_1920x1080                       ... bench:   2,078,950 ns/iter (+/- 64,536)
test tests::bench_split_at_1920x1080                           ... bench:   2,096,603 ns/iter (+/- 107,420)</pre><p></p>
<p>The code in question is very similar to what you would’ve written with <a href="https://cgit.freedesktop.org/gstreamer/orc" rel="noopener" target="_top">ORC</a>, especially the unstrided version. You basically operate on multiple elements at once, doing the same operation on each, but both versions do this in a slightly different way.</p>
<p></p><pre class="crayon-plain-tag">pub fn bgrx_to_gray_chunks_no_asserts_faster_unstrided(in_data: &amp;[u8], out_data: &amp;mut [u8]) {
    // Relies on vector width which is a multiple of 4
    assert!(u8s::WIDTH % 4 == 0 &amp;&amp; u32s::WIDTH % 4 == 0);

    const RGB_Y: [u32; 16] = [19595, 38470, 7471, 0, 19595, 38470, 7471, 0, 19595, 38470, 7471, 0, 19595, 38470, 7471, 0];
    let rgbvec = u32s::load(&amp;RGB_Y, 0);
    in_data.simd_iter(u8s(0)).simd_map(|v| {
        let (a, b) = v.upcast();
        let (a32, b32) = a.upcast();
        let (c32, d32) = b.upcast();

        let grey32a = a32 * rgbvec / u32s(65536);
        let grey32b = b32 * rgbvec / u32s(65536);
        let grey32c = c32 * rgbvec / u32s(65536);
        let grey32d = d32 * rgbvec / u32s(65536);

        let grey16a = grey32a.saturating_downcast(grey32b);
        let grey16b = grey32c.saturating_downcast(grey32d);

        let grey = grey16a.saturating_downcast(grey16b);
        grey
    }).scalar_fill(out_data);
}

pub fn bgrx_to_gray_chunks_no_asserts_faster(in_data: &amp;[u8], out_data: &amp;mut [u8]) {
    // Sane, but slowed down by faster's current striding implementation.
    in_data.stride_four(tuplify!(4, u8s(0))).zip().simd_map(|(r, g, b, _)| {
        let (r16a, r16b) = r.upcast();
        let (r32a, r32b) = r16a.upcast();
        let (r32c, r32d) = r16b.upcast();

        let (g16a, g16b) = g.upcast();
        let (g32a, g32b) = g16a.upcast();
        let (g32c, g32d) = g16b.upcast();

        let (b16a, b16b) = b.upcast();
        let (b32a, b32b) = b16a.upcast();
        let (b32c, b32d) = b16b.upcast();

        let grey32a = (r32a * u32s(19595) + g32a * u32s(38470) + b32a * u32s(7471)) / u32s(65536);
        let grey32b = (r32b * u32s(19595) + g32b * u32s(38470) + b32b * u32s(7471)) / u32s(65536);
        let grey32c = (r32c * u32s(19595) + g32c * u32s(38470) + b32c * u32s(7471)) / u32s(65536);
        let grey32d = (r32d * u32s(19595) + g32d * u32s(38470) + b32d * u32s(7471)) / u32s(65536);

        let grey16a = grey32a.saturating_downcast(grey32b);
        let grey16b = grey32c.saturating_downcast(grey32d);

        let grey = grey16a.saturating_downcast(grey16b);
        grey
    }).scalar_fill(out_data);
}</pre><p> </p></div>

<p class="date">
<a href="https://coaxion.net/blog/2018/01/speeding-up-rgb-to-grayscale-conversion-in-rust-by-a-factor-of-2-2-and-various-other-multimedia-related-processing-loops/">by slomo at January 21, 2018 01:48 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">January 17, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://wingolog.org/2018/01/17/instruction-explosion-in-guile">
<h3><a href="http://wingolog.org/" title="wingolog">Andy Wingo</a> — <a href="http://wingolog.org/archives/2018/01/17/instruction-explosion-in-guile">instruction explosion in guile</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="awingo.png" alt="(Andy Wingo)" width="74" height="100">
<div><p>Greetings, fellow Schemers and compiler nerds: I bring fresh nargery!</p><p><b>instruction explosion</b></p><p>A couple years ago I made a list of <a href="https://wingolog.org/archives/2016/02/04/guile-compiler-tasks">compiler tasks for Guile</a>.  Most of these are still open, but I've been chipping away at the one labeled "instruction explosion":</p><blockquote><p> Now we get more to the compiler side of things. Currently in Guile's VM there are instructions like <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Inlined-Scheme-Instructions.html#Inlined-Scheme-Instructions">vector-ref</a>.  This is a little silly: there are also instructions to branch on the type of an object (<a href="https://www.gnu.org/software/guile/docs/master/guile.html/Branch-Instructions.html#Branch-Instructions">br-if-tc7</a> in this case), to get the vector's length, and to do a branching integer comparison. Really we should replace vector-ref with a combination of these test-and-branches, with real control flow in the function, and then the actual ref should use some more primitive unchecked memory reference instruction. Optimization could end up hoisting everything but the primitive unchecked memory reference, while preserving safety, which would be a win. But probably in most cases optimization wouldn't manage to do this, which would be a lose overall because you have more instruction dispatch.</p><p>Well, this transformation is something we need for native compilation anyway. I would accept a patch to do this kind of transformation on the master branch, after version 2.2.0 has forked. In theory this would remove most all high level instructions from the VM, making the bytecode closer to a virtual CPU, and likewise making it easier for the compiler to emit native code as it's working at a lower level.  </p></blockquote><p>Now that I'm getting close to finished I wanted to share some thoughts.  <a href="https://lists.gnu.org/archive/html/guile-devel/2018-01/msg00003.html">Previous progress reports on the mailing list</a>.</p><p><b>a simple loop</b></p><p>As an example, consider this loop that sums the 32-bit floats in a bytevector.  I've annotated the code with lines and columns so that you can correspond different pieces to the assembly.</p><pre>   0       8   12     19
 +-v-------v---v------v-
 |
1| (use-modules (rnrs bytevectors))
2| (define (f32v-sum bv)
3|   (let lp ((n 0) (sum 0.0))
4|     (if (&lt; n (bytevector-length bv))
5|         (lp (+ n 4)
6|             (+ sum (bytevector-ieee-single-native-ref bv n)))
7|          sum)))
</pre><p>The assembly for the loop before instruction explosion went like this:</p><pre>L1:
  17    (handle-interrupts)     at (unknown file):5:12
  18    (uadd/immediate 0 1 4)
  19    (bv-f32-ref 1 3 1)      at (unknown file):6:19
  20    (fadd 2 2 1)            at (unknown file):6:12
  21    (s64&lt;? 0 4)             at (unknown file):4:8
  22    (jnl 8)                ;; -&gt; L4
  23    (mov 1 0)               at (unknown file):5:8
  24    (j -7)                 ;; -&gt; L1
</pre><p>So, already Guile's compiler has hoisted the <tt>(bytevector-length bv)</tt> and unboxed the loop index <i>n</i> and accumulator <i>sum</i>.  This work aims to simplify further by exploding <tt>bv-f32-ref</tt>.</p><p><b>exploding the loop</b></p><p>In practice, instruction explosion happens in CPS conversion, as we are converting the Scheme-like <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Tree_002dIL.html#Tree_002dIL">Tree-IL</a> language down to the <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Continuation_002dPassing-Style.html#Continuation_002dPassing-Style">CPS soup</a> language.  When we see a Tree-Il primcall (a call to a known primitive), instead of lowering it to a corresponding CPS primcall, we inline a whole blob of code.</p><p>In the concrete case of <tt>bv-f32-ref</tt>, we'd inline it with something like the following:</p><pre>(unless (and (heap-object? bv)
             (eq? (heap-type-tag bv) %bytevector-tag))
  (error "not a bytevector" bv))
(define len (word-ref bv 1))
(define ptr (word-ref bv 2))
(unless (and (&lt;= 4 len)
             (&lt;= idx (- len 4)))
  (error "out of range" idx))
(f32-ref ptr len)
</pre><p>As you can see, there are four branches hidden in the <tt>bv-f32-ref</tt>: two to check that the object is a bytevector, and two to check that the index is within range.  In this explanation we assume that the offset <i>idx</i> is already unboxed, but actually unboxing the index ends up being part of this work as well.</p><p>One of the goals of instruction explosion was that by breaking the operation into a number of smaller, more orthogonal parts, native code generation would be easier, because the compiler would only have to know about those small bits.  However without an optimizing compiler, it would be better to <a href="https://docs.google.com/document/d/15hmBrCrmMZzra8ekhl7meQZBodeahsauilNDImif5Xg/edit#heading=h.8zevmmfemvcv">reify a call out to a specialized <tt>bv-f32-ref</tt> runtime routine</a> instead of inlining all of this code -- probably whatever language you write your runtime routine in (C, rust, whatever) will do a better job optimizing than your compiler will.</p><p>But with an optimizing compiler, there is the possibility of removing possibly everything but the <tt>f32-ref</tt>.  Guile doesn't quite get there, but almost; here's the post-explosion optimized assembly of the inner loop of f32v-sum:</p><pre>L1:
  27    (handle-interrupts)
  28    (tag-fixnum 1 2)
  29    (s64&lt;? 2 4)             at (unknown file):4:8
  30    (jnl 15)               ;; -&gt; L5
  31    (uadd/immediate 0 2 4)  at (unknown file):5:12
  32    (u64&lt;? 2 7)             at (unknown file):6:19
  33    (jnl 5)                ;; -&gt; L2
  34    (f32-ref 2 5 2)
  35    (fadd 3 3 2)            at (unknown file):6:12
  36    (mov 2 0)               at (unknown file):5:8
  37    (j -10)                ;; -&gt; L1
</pre><p><b>good things</b></p><p>The first thing to note is that unlike the "before" code, there's no instruction in this loop that can throw an exception.  Neat.</p><p>Next, note that there's no type check on the bytevector; the <a href="https://wingolog.org/archives/2015/07/28/loop-optimizations-in-guile">peeled iteration</a> preceding the loop already proved that the bytevector is a bytevector.</p><p>And indeed there's no reference to the bytevector at all in the loop!  The value being dereferenced in <tt>(f32-ref 2 5 2)</tt> is a raw pointer.  (Read this instruction as, "sp[2] = *(float*)((byte*)sp[5] + (uptrdiff_t)sp[2])".)  The compiler does something interesting; the <tt>f32-ref</tt> CPS primcall actually takes three arguments: the garbage-collected object protecting the pointer, the pointer itself, and the offset.  The object itself doesn't appear in the residual code, but including it in the <tt>f32-ref</tt> primcall's inputs keeps it alive as long as the <tt>f32-ref</tt> itself is alive.</p><p><b>bad things</b></p><p>Then there are the limitations.  Firstly, instruction 28 tags the u64 loop index as a fixnum, but never uses the result.  Why is this here?  Sadly it's because the value is used in the bailout at L2.  Recall this pseudocode:</p><pre>(unless (and (&lt;= 4 len)
             (&lt;= idx (- len 4)))
  (error "out of range" idx))
</pre><p>Here the <a>error</a> ends up lowering to a <a href="https://lists.gnu.org/archive/html/guile-devel/2018-01/msg00003.html"><tt>throw</tt> CPS term</a> that the compiler recognizes as a bailout and renders out-of-line; cool.  But it uses <i>idx</i> as an argument, as a tagged SCM value.  The compiler untags the loop index, but has to keep a tagged version around for the error cases.</p><p>The right fix is probably some kind of allocation sinking pass that sinks the <tt>tag-fixnum</tt> to the bailouts.  Oh well.</p><p>Additionally, there are two tests in the loop.  Are both necessary?  Turns out, yes :( Imagine you have a bytevector of length 102<b>5</b>.  The loop continues until the last ref at offset 1024, which is within bounds of the bytevector but there's one one byte available at that point, so we need to throw an exception at this point.  The compiler did as good a job as we could expect it to do.</p><p><b>is is worth it?  where to now?</b></p><p>On the one hand, instruction explosion is a step sideways.  The code is more optimal, but it's more instructions.  Because Guile currently has a bytecode VM, that means more total interpreter overhead.  Testing on a 40-megabyte bytevector of 32-bit floats, the exploded <tt>f32v-sum</tt> completes in 115 milliseconds compared to around 97 for the earlier version.</p><p>On the other hand, it is very easy to imagine how to compile these instructions to native code, either ahead-of-time or via a simple template JIT.  You practically just have to look up the instructions in the corresponding ISA reference, is all.  The result should perform quite well.</p><p>I will probably take a whack at a simple template JIT first that does no register allocation, then ahead-of-time compilation with register allocation.  Getting the AOT-compiled artifacts to dynamically link with runtime routines is a sufficient pain in my mind that I will put it off a bit until later.  I also need to figure out a good strategy for truly polymorphic operations like general integer addition; probably involving inline caches.</p><p>So that's where we're at :)  Thanks for reading, and happy hacking in Guile in 2018!</p></div></div>

<p class="date">
<a href="http://wingolog.org/archives/2018/01/17/instruction-explosion-in-guile">by Andy Wingo at January 17, 2018 10:30 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">January 13, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=523" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net – slomo's blog">Sebastian Dröge</a> — <a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/">How to write GStreamer Elements in Rust Part 1: A Video Filter for converting RGB to grayscale</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dröge)" width="80" height="80">
<p>This is part one of a series of blog posts that I’ll write in the next weeks, as previously announced in <a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/" rel="noopener" target="_top">the GStreamer Rust bindings 0.10.0 release blog post</a>. Since the last series of blog posts about writing GStreamer plugins in Rust (<a href="https://coaxion.net/blog/2016/05/writing-gstreamer-plugins-and-elements-in-rust/" rel="noopener" target="_top">[1]</a> <a href="https://coaxion.net/blog/2016/09/writing-gstreamer-elements-in-rust-part-2-dont-panic-we-have-better-assertions-now-and-other-updates/" rel="noopener" target="_top">[2]</a> <a href="https://coaxion.net/blog/2016/11/writing-gstreamer-elements-in-rust-part-3-parsing-data-from-untrusted-sources-like-its-2016/" rel="noopener" target="_top">[3]</a> <a href="https://coaxion.net/blog/2017/03/writing-gstreamer-elements-in-rust-part-4-logging-cows-and-plugins/" rel="noopener" target="_top">[4]</a>) a lot has changed, and the content of those blog posts has only historical value now, as the journey of experimentation to what exists now.</p>
<p>In this first part we’re going to write a plugin that contains a video filter element. The video filter can convert from RGB to grayscale, either output as 8-bit per pixel grayscale or 32-bit per pixel RGB. In addition there’s a property to invert all grayscale values, or to shift them by up to 255 values. In the end this will allow you to watch <a href="https://peach.blender.org/" rel="noopener" target="_top">Big Bucky Bunny</a>, or anything else really that can somehow go into a GStreamer pipeline, in grayscale. Or encode the output to a new video file, send it over the network via <a href="https://gstconf.ubicast.tv/videos/gstreamer-webrtc/" rel="noopener" target="_top">WebRTC</a> or something else, or basically do anything you want with it.</p>
<img src="bbb-500x281.jpg" alt="" class="size-medium wp-image-540" width="500" height="281">Big Bucky Bunny – Grayscale
<p>This will show the basics of how to write a GStreamer plugin and element in Rust: the basic setup for registering a type and implementing it in Rust, and how to use the various GStreamer API and APIs from the Rust standard library to do the processing.</p>
<p>The final code for this plugin can be found <a href="https://github.com/sdroege/gst-plugin-rs/tree/0.1/gst-plugin-tutorial" rel="noopener" target="_top">here</a>,  and it is based on the 0.1 version of the <a href="https://crates.io/crates/gst-plugin" rel="noopener" target="_top">gst-plugin</a> crate and the 0.10 version of the <a href="https://crates.io/crates/gstreamer" rel="noopener" target="_top">gstreamer</a> crate. At least Rust 1.20 is required for all this. I’m also assuming that you have GStreamer (at least version 1.8) installed for your platform, see e.g. the GStreamer bindings <a href="https://github.com/sdroege/gstreamer-rs#installation" rel="noopener" target="_top">installation instructions</a>.</p>
<h3 id="toc">Table of Contents</h3>
<ol>
<li><a href="https://coaxion.net/blog/feed/#project-structure">Project Structure</a></li>
<li><a href="https://coaxion.net/blog/feed/#plugin-initialization">Plugin Initialization</a></li>
<li><a href="https://coaxion.net/blog/feed/#type-registration">Type Registration</a>
</li><li><a href="https://coaxion.net/blog/feed/#type-initialization">Type Class &amp; Instance Initialization</a></li>
<li><a href="https://coaxion.net/blog/feed/#caps-pad-templates">Caps &amp; Pad Templates</a></li>
<li><a href="https://coaxion.net/blog/feed/#caps-handling-1">Caps Handling Part 1</a></li>
<li><a href="https://coaxion.net/blog/feed/#caps-handling-2">Caps Handling Part 2</a></li>
<li><a href="https://coaxion.net/blog/feed/#conversion">Conversion of BGRx Video Frames to Grayscale</a></li>
<li><a href="https://coaxion.net/blog/feed/#testing">Testing the new element</a></li>
<li><a href="https://coaxion.net/blog/feed/#properties">Properties</a></li>
<li><a href="https://coaxion.net/blog/feed/#what-next">What next?</a></li>
</ol>
<h3 id="project-structure">Project Structure</h3>
<p>We’ll create a new <em>cargo</em> project with <em>cargo init –lib –name gst-plugin-tutorial</em>. This will create a basically empty <em>Cargo.toml</em> and a corresponding <em>src/lib.rs</em>. We will use this structure: <em>lib.rs</em> will contain all the plugin related code, separate modules will contain any GStreamer plugins that are added.</p>
<p>The empty <em>Cargo.toml</em> has to be updated to list all the dependencies that we need, and to define that the crate should result in a <em>cdylib</em>, i.e. a C library that does not contain any Rust-specific metadata. The final <em>Cargo.toml</em> looks as follows</p>
<p></p><pre class="crayon-plain-tag">[package]
name = "gst-plugin-tutorial"
version = "0.1.0"
authors = ["Sebastian Dröge &lt;sebastian@centricular.com&gt;"]
repository = "https://github.com/sdroege/gst-plugin-rs"
license = "MIT/Apache-2.0"

[dependencies]
glib = "0.4"
gstreamer = "0.10"
gstreamer-base = "0.10"
gstreamer-video = "0.10"
gst-plugin = "0.1"

[lib]
name = "gstrstutorial"
crate-type = ["cdylib"]
path = "src/lib.rs"</pre><p> </p>
<p>We’re depending on the <em>gst-plugin</em> crate, which provides all the basic infrastructure for implementing GStreamer plugins and elements. In addition we depend on the <em>gstreamer</em>, <em>gstreamer-base</em> and <em>gstreamer-video</em> crates for various GStreamer API that we’re going to use later, and the <em>glib</em> crate to be able to use some GLib API that we’ll need. GStreamer is building upon GLib, and this leaks through in various places.</p>
<p>With the basic project structure being set-up, we should be able to compile the project with <em>cargo build</em> now, which will download and build all dependencies and then creates a file called <em>target/debug/libgstrstutorial.so</em> (or .dll on Windows, .dylib on macOS). This is going to be our GStreamer plugin.</p>
<p>To allow GStreamer to find our new plugin and make it available in every GStreamer-based application, we could install it into the system- or user-wide GStreamer plugin path or simply point the <em>GST_PLUGIN_PATH</em> environment variable to the directory containing it:</p>
<p></p><pre class="crayon-plain-tag">export GST_PLUGIN_PATH=`pwd`/target/debug</pre><p> </p>
<p>If you now run the <em>gst-inspect-1.0</em> tool on the <em>libgstrstutorial.so</em>, it will not yet print all information it can extract from the plugin but for now just complains that this is not a valid GStreamer plugin. Which is true, we didn’t write any code for it yet.</p>
<h3 id="plugin-initialization">Plugin Initialization</h3>
<p>Let’s start editing <em>src/lib.rs</em> to make this an actual GStreamer plugin. First of all, we need to add various <em>extern crate</em> directives to be able to use our dependencies and also mark some of them <em>#[macro_use]</em> because we’re going to use macros defined in some of them. This looks like the following</p>
<p></p><pre class="crayon-plain-tag">extern crate glib;
#[macro_use]
extern crate gstreamer as gst;
extern crate gstreamer_base as gst_base;
extern crate gstreamer_video as gst_video;
#[macro_use]
extern crate gst_plugin;</pre><p> </p>
<p>Next we make use of the <em>plugin_define!</em> macro from the <em>gst-plugin</em> crate to set-up the static metadata of the plugin (and make the shared library recognizeable by GStreamer to be a valid plugin), and to define the name of our entry point function (<em>plugin_init</em>) where we will register all the elements that this plugin provides.</p>
<p></p><pre class="crayon-plain-tag">plugin_define!(
    b"rstutorial\0",
    b"Rust Tutorial Plugin\0",
    plugin_init,
    b"1.0\0",
    b"MIT/X11\0",
    b"rstutorial\0",
    b"rstutorial\0",
    b"https://github.com/sdroege/gst-plugin-rs\0",
    b"2017-12-30\0"
);</pre><p> </p>
<p>This is unfortunately not very beautiful yet due to a) GStreamer requiring this information to be statically available in the shared library, not returned by a function (starting with GStreamer 1.14 it can be a function), and b) Rust not allowing raw strings (<em>b”blabla</em>) to be concatenated with a macro like the <a href="https://doc.rust-lang.org/std/macro.concat.html" rel="noopener" target="_top"><em>std::concat</em></a> macro (so that the <em>b</em> and <em>\0</em> parts could be hidden away). Expect this to become better in the future.</p>
<p>The static plugin metadata that we provide here is</p>
<ol>
<li>name of the plugin</li>
<li>short description for the plugin</li>
<li>name of the plugin entry point function</li>
<li>version number of the plugin</li>
<li>license of the plugin (only a fixed set of licenses is allowed here, <a href="https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstPlugin.html#GstPluginDesc" rel="noopener" target="_top">see</a>)</li>
<li>source package name</li>
<li>binary package name (only really makes sense for e.g. Linux distributions)</li>
<li>origin of the plugin</li>
<li>release date of this version</li>
</ol>
<p>In addition we’re defining an empty plugin entry point function that just returns <em>true</em></p>
<p></p><pre class="crayon-plain-tag">fn plugin_init(plugin: &amp;gst::Plugin) -&gt; bool {
    true
}</pre><p></p>
<p>With all that given, <em>gst-inspect-1.0</em> should print exactly this information when running on the <em>libgstrstutorial.so</em> file (or .dll on Windows, or .dylib on macOS)</p>
<p></p><pre class="crayon-plain-tag">gst-inspect-1.0 target/debug/libgstrstutorial.so</pre><p> </p>
<h3 id="type-registration">Type Registration</h3>
<p>As a next step, we’re going to add another module <em>rgb2gray</em> to our project, and call a function called <em>register</em> from our <em>plugin_init</em> function.</p>
<p></p><pre class="crayon-plain-tag">mod rgb2gray;

fn plugin_init(plugin: &amp;gst::Plugin) -&gt; bool {
    rgb2gray::register(plugin);
    true
}</pre><p> </p>
<p>With that our <em>src/lib.rs</em> is complete, and all following code is only in <em>src/rgb2gray.rs</em>. At the top of the new file we first need to add various <em>use</em>-directives to import various types and functions we’re going to use into the current module’s scope</p>
<p></p><pre class="crayon-plain-tag">use glib;
use gst;
use gst::prelude::*;
use gst_video;

use gst_plugin::properties::*;
use gst_plugin::object::*;
use gst_plugin::element::*;
use gst_plugin::base_transform::*;

use std::i32;
use std::sync::Mutex;</pre><p> </p>
<p>GStreamer is based on the GLib object system (<a href="https://developer.gnome.org/gobject/stable/" rel="noopener" target="_top">GObject</a>). C (just like Rust) does not have built-in support for object orientated programming, inheritance, virtual methods and related concepts, and GObject makes these features available in C as a library. Without language support this is a quite verbose endeavour in C, and the <em>gst-plugin</em> crate tries to expose all this in a (as much as possible) Rust-style API while hiding all the details that do not really matter.</p>
<p>So, as a next step we need to register a new type for our RGB to Grayscale converter GStreamer element with the GObject type system, and then register that type with GStreamer to be able to create new instances of it. We do this with the following code</p>
<p></p><pre class="crayon-plain-tag">struct Rgb2GrayStatic;

impl ImplTypeStatic&lt;BaseTransform&gt; for Rgb2GrayStatic {
    fn get_name(&amp;self) -&gt; &amp;str {
        "Rgb2Gray"
    }

    fn new(&amp;self, element: &amp;BaseTransform) -&gt; Box&lt;BaseTransformImpl&lt;BaseTransform&gt;&gt; {
        Rgb2Gray::new(element)
    }

    fn class_init(&amp;self, klass: &amp;mut BaseTransformClass) {
        Rgb2Gray::class_init(klass);
    }
}

pub fn register(plugin: &amp;gst::Plugin) {
    let type_ = register_type(Rgb2GrayStatic);
    gst::Element::register(plugin, "rsrgb2gray", 0, type_);
}</pre><p> </p>
<p>This defines a zero-sized struct <em>Rgb2GrayStatic</em> that is used to implement the <em>ImplTypeStatic&lt;BaseTransform&gt;</em> trait on it for providing static information about the type to the type system. In our case this is a zero-sized struct, but in other cases this struct might contain actual data (for example if the same element code is used for multiple elements, e.g. when wrapping a generic codec API that provides support for multiple decoders and then wanting to register one element per decoder). By implementing <em>ImplTypeStatic&lt;BaseTransform&gt;</em> we also declare that our element is going to be based on the GStreamer <em>BaseTransform</em> base class, which provides a relatively simple API for 1:1 transformation elements like ours is going to be.</p>
<p><em>ImplTypeStatic</em> provides functions that return a name for the type, and functions for initializing/returning a new instance of our element (<em>new</em>) and for initializing the class metadata (<em>class_init</em>, more on that later). We simply let those functions proxy to associated functions on the <em>Rgb2Gray</em> struct that we’re going to define at a later time.</p>
<p>In addition, we also define a <em>register</em> function (the one that is already called from our <em>plugin_init</em> function) and in there first register the <em>Rgb2GrayStatic</em> type metadata with the GObject type system to retrieve a type ID, and then register this type ID to GStreamer to be able to create new instances of it with the name “rsrgb2gray” (e.g. when using <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.ElementFactory.html#method.make" rel="noopener" target="_top"><em>gst::ElementFactory::make</em></a>).</p>
<h3 id="type-initialization">Type Class &amp; Instance Initialization</h3>
<p>As a next step we declare the <em>Rgb2Gray</em> struct and implement the <em>new</em> and <em>class_init</em> functions on it. In the first version, this struct is almost empty but we will later use it to store all state of our element.</p>
<p></p><pre class="crayon-plain-tag">struct Rgb2Gray {
    cat: gst::DebugCategory,
}

impl Rgb2Gray {
    fn new(_transform: &amp;BaseTransform) -&gt; Box&lt;BaseTransformImpl&lt;BaseTransform&gt;&gt; {
        Box::new(Self {
            cat: gst::DebugCategory::new(
                "rsrgb2gray",
                gst::DebugColorFlags::empty(),
                "Rust RGB-GRAY converter",
            ),
        })
    }

    fn class_init(klass: &amp;mut BaseTransformClass) {
        klass.set_metadata(
            "RGB-GRAY Converter",
            "Filter/Effect/Converter/Video",
            "Converts RGB to GRAY or grayscale RGB",
            "Sebastian Dröge &lt;sebastian@centricular.com&gt;",
        );

        klass.configure(BaseTransformMode::NeverInPlace, false, false);
    }
}</pre><p> </p>
<p>In the <em>new</em> function we return a boxed (i.e. heap-allocated) version of our struct, containing a newly created GStreamer debug category of name “rsrgb2gray”. We’re going to use this debug category later for making use of GStreamer’s debug logging system for logging the state and changes of our element.</p>
<p>In the <em>class_init</em> function we, again, set up some metadata for our new element. In this case these are a description, a classification of our element, a longer description and the author. The metadata can later be retrieved and made use of via the <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.Registry.html" rel="noopener" target="_top">Registry</a> and <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.PluginFeature.html" rel="noopener" target="_top">PluginFeature</a>/<a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.ElementFactory.html" rel="noopener" target="_top">ElementFactory</a> API. We also configure the <em>BaseTransform</em> class and define that we will never operate in-place (producing our output in the input buffer), and that we don’t want to work in passthrough mode if the input/output formats are the same.</p>
<p>Additionally we need to implement various traits on the <em>Rgb2Gray</em> struct, which will later be used to override virtual methods of the various parent classes of our element. For now we can keep the trait implementations empty. There is one trait implementation required per parent class.</p>
<p></p><pre class="crayon-plain-tag">impl ObjectImpl&lt;BaseTransform&gt; for Rgb2Gray {}
impl ElementImpl&lt;BaseTransform&gt; for Rgb2Gray {}
impl BaseTransformImpl&lt;BaseTransform&gt; for Rgb2Gray {}</pre><p> </p>
<p>With all this defined, <em>gst-inspect-1.0</em> should be able to show some more information about our element already but will still complain that it’s not complete yet.</p>
<h3 id="caps-pad-templates">Caps &amp; Pad Templates</h3>
<p>Data flow of GStreamer elements is happening via pads, which are the input(s) and output(s) (or sinks and sources) of an element. Via the pads, buffers containing actual media data, events or queries are transferred. An element can have any number of sink and source pads, but our new element will only have one of each.</p>
<p>To be able to declare what kinds of pads an element can create (they are not necessarily all static but could be created at runtime by the element or the application), it is necessary to install so-called pad templates during the class initialization. These pad templates contain the name (or rather “name template”, it could be something like <em>src_%u</em> for e.g. pad templates that declare multiple possible pads), the direction of the pad (sink or source), the availability of the pad (is it <strong>always</strong> there, <strong>sometimes</strong> added/removed by the element or to be <strong>requested</strong> by the application) and all the possible media types (called caps) that the pad can consume (sink pads) or produce (src pads).</p>
<p>In our case we only have <strong>always</strong> pads, one sink pad called “sink”, on which we can only accept RGB (BGRx to be exact) data with any width/height/framerate and one source pad called “src”, on which we will produce either RGB (BGRx) data or GRAY8 (8-bit grayscale) data. We do this by adding the following code to the <em>class_init</em> function.</p>
<p></p><pre class="crayon-plain-tag">let caps = gst::Caps::new_simple(
            "video/x-raw",
            &amp;[
                (
                    "format",
                    &amp;gst::List::new(&amp;[
                        &amp;gst_video::VideoFormat::Bgrx.to_string(),
                        &amp;gst_video::VideoFormat::Gray8.to_string(),
                    ]),
                ),
                ("width", &amp;gst::IntRange::&lt;i32&gt;::new(0, i32::MAX)),
                ("height", &amp;gst::IntRange::&lt;i32&gt;::new(0, i32::MAX)),
                (
                    "framerate",
                    &amp;gst::FractionRange::new(
                        gst::Fraction::new(0, 1),
                        gst::Fraction::new(i32::MAX, 1),
                    ),
                ),
            ],
        );

        let src_pad_template = gst::PadTemplate::new(
            "src",
            gst::PadDirection::Src,
            gst::PadPresence::Always,
            &amp;caps,
        );
        klass.add_pad_template(src_pad_template);

        let caps = gst::Caps::new_simple(
            "video/x-raw",
            &amp;[
                ("format", &amp;gst_video::VideoFormat::Bgrx.to_string()),
                ("width", &amp;gst::IntRange::&lt;i32&gt;::new(0, i32::MAX)),
                ("height", &amp;gst::IntRange::&lt;i32&gt;::new(0, i32::MAX)),
                (
                    "framerate",
                    &amp;gst::FractionRange::new(
                        gst::Fraction::new(0, 1),
                        gst::Fraction::new(i32::MAX, 1),
                    ),
                ),
            ],
        );

        let sink_pad_template = gst::PadTemplate::new(
            "sink",
            gst::PadDirection::Sink,
            gst::PadPresence::Always,
            &amp;caps,
        );
        klass.add_pad_template(sink_pad_template);</pre><p> </p>
<p>The names “src” and “sink” are pre-defined by the <em>BaseTransform</em> class and this base-class will also create the actual pads with those names from the templates for us whenever a new element instance is created. Otherwise we would have to do that in our <em>new</em> function but here this is not needed.</p>
<p>If you now run <em>gst-inspect-1.0</em> on the <em>rsrgb2gray</em> element, these pad templates with their caps should also show up.</p>
<h3 id="caps-handling-1">Caps Handling Part 1</h3>
<p>As a next step we will add caps handling to our new element. This involves overriding 4 virtual methods from the <em>BaseTransformImpl</em> trait, and actually storing the configured input and output caps inside our element struct. Let’s start with the latter</p>
<p></p><pre class="crayon-plain-tag">struct State {
    in_info: gst_video::VideoInfo,
    out_info: gst_video::VideoInfo,
}

struct Rgb2Gray {
    cat: gst::DebugCategory,
    state: Mutex&lt;Option&lt;State&gt;&gt;,
}

impl Rgb2Gray {
    fn new(_transform: &amp;BaseTransform) -&gt; Box&lt;BaseTransformImpl&lt;BaseTransform&gt;&gt; {
        Box::new(Self {
            cat: gst::DebugCategory::new(
                "rsrgb2gray",
                gst::DebugColorFlags::empty(),
                "Rust RGB-GRAY converter",
            ),
            state: Mutex::new(None),
        })
    }
}</pre><p> </p>
<p>We define a new struct <em>State</em> that contains the input and output caps, stored in a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_video/struct.VideoInfo.html" rel="noopener" target="_top"><em>VideoInfo</em></a>. <em>VideoInfo</em> is a struct that contains various fields like width/height, framerate and the video format and allows to conveniently with the properties of (raw) video formats. We have to store it inside a <a href="https://doc.rust-lang.org/std/sync/struct.Mutex.html" rel="noopener" target="_top"><em>Mutex</em></a> in our <em>Rgb2Gray</em> struct as this can (in theory) be accessed from multiple threads at the same time.</p>
<p>Whenever input/output caps are configured on our element, the <em>set_caps</em> virtual method of <em>BaseTransform</em> is called with both caps (i.e. in the very beginning before the data flow and whenever it changes), and all following video frames that pass through our element should be according to those caps. Once the element is shut down, the <em>stop</em> virtual method is called and it would make sense to release the <em>State</em> as it only contains stream-specific information. We’re doing this by adding the following to the <em>BaseTransformImpl</em> trait implementation</p>
<p></p><pre class="crayon-plain-tag">impl BaseTransformImpl&lt;BaseTransform&gt; for Rgb2Gray {
    fn set_caps(&amp;self, element: &amp;BaseTransform, incaps: &amp;gst::Caps, outcaps: &amp;gst::Caps) -&gt; bool {
        let in_info = match gst_video::VideoInfo::from_caps(incaps) {
            None =&gt; return false,
            Some(info) =&gt; info,
        };
        let out_info = match gst_video::VideoInfo::from_caps(outcaps) {
            None =&gt; return false,
            Some(info) =&gt; info,
        };

        gst_debug!(
            self.cat,
            obj: element,
            "Configured for caps {} to {}",
            incaps,
            outcaps
        );

        *self.state.lock().unwrap() = Some(State {
            in_info: in_info,
            out_info: out_info,
        });

        true
    }

    fn stop(&amp;self, element: &amp;BaseTransform) -&gt; bool {
        // Drop state
        let _ = self.state.lock().unwrap().take();

        gst_info!(self.cat, obj: element, "Stopped");

        true
    }
}</pre><p> </p>
<p>This code should be relatively self-explanatory. In <em>set_caps</em> we’re parsing the two caps into a <em>VideoInfo</em> and then store this in our <em>State</em>, in <em>stop</em> we drop the <em>State</em> and replace it with <em>None</em>. In addition we make use of our debug category here and use the <em>gst_info!</em> and <em>gst_debug!</em> macros to output the current caps configuration to the GStreamer debug logging system. This information can later be useful for debugging any problems once the element is running.</p>
<p>Next we have to provide information to the <em>BaseTransform</em> base class about the size in bytes of a video frame with specific caps. This is needed so that the base class can allocate an appropriately sized output buffer for us, that we can then fill later. This is done with the <em>get_unit_size</em> virtual method, which is required to return the size of one processing unit in specific caps. In our case, one processing unit is one video frame. In the case of raw audio it would be the size of one sample multiplied by the number of channels.</p>
<p></p><pre class="crayon-plain-tag">impl BaseTransformImpl&lt;BaseTransform&gt; for Rgb2Gray {
    fn get_unit_size(&amp;self, _element: &amp;BaseTransform, caps: &amp;gst::Caps) -&gt; Option&lt;usize&gt; {
        gst_video::VideoInfo::from_caps(caps).map(|info| info.size())
    }
}</pre><p> </p>
<p>We simply make use of the <em>VideoInfo</em> API here again, which conveniently gives us the size of one video frame already.</p>
<p>Instead of <em>get_unit_size</em> it would also be possible to implement the <em>transform_size</em> virtual method, which is getting passed one size and the corresponding caps, another caps and is supposed to return the size converted to the second caps. Depending on how your element works, one or the other can be easier to implement.</p>
<h3 id="caps-handling-2">Caps Handling Part 2</h3>
<p>We’re not done yet with caps handling though. As a very last step it is required that we implement a function that is converting caps into the corresponding caps in the other direction. For example, if we receive BGRx caps with some width/height on the sinkpad, we are supposed to convert this into new caps with the same width/height but BGRx or GRAY8. That is, we can convert BGRx to BGRx or GRAY8. Similarly, if the element downstream of ours can accept GRAY8 with a specific width/height from our source pad, we have to convert this to BGRx with that very same width/height.</p>
<p>This has to be implemented in the <em>transform_caps</em> virtual method, and looks as following</p>
<p></p><pre class="crayon-plain-tag">impl BaseTransformImpl&lt;BaseTransform&gt; for Rgb2Gray {
    fn transform_caps(
        &amp;self,
        element: &amp;BaseTransform,
        direction: gst::PadDirection,
        caps: gst::Caps,
        filter: Option&lt;&amp;gst::Caps&gt;,
    ) -&gt; gst::Caps {
        let other_caps = if direction == gst::PadDirection::Src {
            let mut caps = caps.clone();

            for s in caps.make_mut().iter_mut() {
                s.set("format", &amp;gst_video::VideoFormat::Bgrx.to_string());
            }

            caps
        } else {
            let mut gray_caps = gst::Caps::new_empty();

            {
                let gray_caps = gray_caps.get_mut().unwrap();

                for s in caps.iter() {
                    let mut s_gray = s.to_owned();
                    s_gray.set("format", &amp;gst_video::VideoFormat::Gray8.to_string());
                    gray_caps.append_structure(s_gray);
                }
                gray_caps.append(caps.clone());
            }

            gray_caps
        };

        gst_debug!(
            self.cat,
            obj: element,
            "Transformed caps from {} to {} in direction {:?}",
            caps,
            other_caps,
            direction
        );

        if let Some(filter) = filter {
            filter.intersect_with_mode(&amp;other_caps, gst::CapsIntersectMode::First)
        } else {
            other_caps
        }
    }
}</pre><p> </p>
<p>This caps conversion happens in 3 steps. First we check if we got caps for the source pad. In that case, the caps on the other pad (the sink pad) are going to be exactly the same caps but no matter if the caps contained BGRx or GRAY8 they must become BGRx as that’s the only format that our sink pad can accept. We do this by creating a clone of the input caps, then making sure that those caps are actually writable (i.e. we’re having the only reference to them, or a copy is going to be created) and then iterate over all the structures inside the caps and then set the “format” field to BGRx. After this, all structures in the new caps will be with the format field set to BGRx.</p>
<p>Similarly, if we get caps for the sink pad and are supposed to convert it to caps for the source pad, we create new caps and in there append a copy of each structure of the input caps (which are BGRx) with the format field set to GRAY8. In the end we append the original caps, giving us first all caps as GRAY8 and then the same caps as BGRx. With this ordering we signal to GStreamer that we would prefer to output GRAY8 over BGRx.</p>
<p>In the end the caps we created for the other pad are filtered against optional filter caps to reduce the potential size of the caps. This is done by intersecting the caps with that filter, while keeping the order (and thus preferences) of the filter caps (<em>gst::CapsIntersectMode::First</em>).</p>
<h3 id="conversion">Conversion of BGRx Video Frames to Grayscale</h3>
<p>Now that all the caps handling is implemented, we can finally get to the implementation of the actual video frame conversion. For this we start with defining a helper function <em>bgrx_to_gray</em> that converts one BGRx pixel to a grayscale value. The BGRx pixel is passed as a <em>&amp;[u8]</em> slice with 4 elements and the function returns another <em>u8</em> for the grayscale value.</p>
<p></p><pre class="crayon-plain-tag">impl Rgb2Gray {
    #[inline]
    fn bgrx_to_gray(in_p: &amp;[u8]) -&gt; u8 {
        // See https://en.wikipedia.org/wiki/YUV#SDTV_with_BT.601
        const R_Y: u32 = 19595; // 0.299 * 65536
        const G_Y: u32 = 38470; // 0.587 * 65536
        const B_Y: u32 = 7471; // 0.114 * 65536

        assert_eq!(in_p.len(), 4);

        let b = u32::from(in_p[0]);
        let g = u32::from(in_p[1]);
        let r = u32::from(in_p[2]);

        let gray = ((r * R_Y) + (g * G_Y) + (b * B_Y)) / 65536;
        (gray as u8)
    }
}</pre><p> </p>
<p>This function works by extracting the blue, green and red components from each pixel (remember: we work on BGRx, so the first value will be blue, the second green, the third red and the fourth unused), extending it from 8 to 32 bits for a wider value-range and then converts it to the Y component of the YUV colorspace (basically what your grandparents’ black &amp; white TV would’ve displayed). The coefficients come from the Wikipedia page about YUV and are normalized to unsigned 16 bit integers so we can keep some accuracy, don’t have to work with floating point arithmetic and stay inside the range of 32 bit integers for all our calculations. As you can see, the green component is weighted more than the others, which comes from our eyes being more sensitive to green than to other colors.</p>
<p><strong>Note:</strong> This is only doing the actual conversion from linear RGB to grayscale (and in <a href="https://en.wikipedia.org/wiki/YUV#SDTV_with_BT.601" rel="noopener" target="_top">BT.601</a> colorspace). To do this conversion correctly you need to know your colorspaces and use the correct coefficients for conversion, and also do <a href="https://en.wikipedia.org/wiki/Gamma_correction" rel="noopener" target="_top">gamma correction</a>. See <a href="https://web.archive.org/web/20161024090830/http://www.4p8.com/eric.brasseur/gamma.html" rel="noopener" target="_top">this</a> about why it is important.</p>
<p>Afterwards we have to actually call this function on every pixel. For this the <em>transform</em> virtual method is implemented, which gets a input and output buffer passed and we’re supposed to read the input buffer and fill the output buffer. The implementation looks as follows, and is going to be our biggest function for this element</p>
<p></p><pre class="crayon-plain-tag">impl BaseTransformImpl&lt;BaseTransform&gt; for Rgb2Gray {
    fn transform(
        &amp;self,
        element: &amp;BaseTransform,
        inbuf: &amp;gst::Buffer,
        outbuf: &amp;mut gst::BufferRef,
    ) -&gt; gst::FlowReturn {
        let mut state_guard = self.state.lock().unwrap();
        let state = match *state_guard {
            None =&gt; {
                gst_element_error!(element, gst::CoreError::Negotiation, ["Have no state yet"]);
                return gst::FlowReturn::NotNegotiated;
            }
            Some(ref mut state) =&gt; state,
        };

        let in_frame = match gst_video::VideoFrameRef::from_buffer_ref_readable(
            inbuf.as_ref(),
            &amp;state.in_info,
        ) {
            None =&gt; {
                gst_element_error!(
                    element,
                    gst::CoreError::Failed,
                    ["Failed to map input buffer readable"]
                );
                return gst::FlowReturn::Error;
            }
            Some(in_frame) =&gt; in_frame,
        };

        let mut out_frame =
            match gst_video::VideoFrameRef::from_buffer_ref_writable(outbuf, &amp;state.out_info) {
                None =&gt; {
                    gst_element_error!(
                        element,
                        gst::CoreError::Failed,
                        ["Failed to map output buffer writable"]
                    );
                    return gst::FlowReturn::Error;
                }
                Some(out_frame) =&gt; out_frame,
            };

        let width = in_frame.width() as usize;
        let in_stride = in_frame.plane_stride()[0] as usize;
        let in_data = in_frame.plane_data(0).unwrap();
        let out_stride = out_frame.plane_stride()[0] as usize;
        let out_format = out_frame.format();
        let out_data = out_frame.plane_data_mut(0).unwrap();

        if out_format == gst_video::VideoFormat::Bgrx {
            assert_eq!(in_data.len() % 4, 0);
            assert_eq!(out_data.len() % 4, 0);
            assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

            let in_line_bytes = width * 4;
            let out_line_bytes = width * 4;

            assert!(in_line_bytes &lt;= in_stride);
            assert!(out_line_bytes &lt;= out_stride);

            for (in_line, out_line) in in_data
                .chunks(in_stride)
                .zip(out_data.chunks_mut(out_stride))
            {
                for (in_p, out_p) in in_line[..in_line_bytes]
                    .chunks(4)
                    .zip(out_line[..out_line_bytes].chunks_mut(4))
                {
                    assert_eq!(out_p.len(), 4);

                    let gray = Rgb2Gray::bgrx_to_gray(in_p);
                    out_p[0] = gray;
                    out_p[1] = gray;
                    out_p[2] = gray;
                }
            }
        } else if out_format == gst_video::VideoFormat::Gray8 {
            assert_eq!(in_data.len() % 4, 0);
            assert_eq!(out_data.len() / out_stride, in_data.len() / in_stride);

            let in_line_bytes = width * 4;
            let out_line_bytes = width;

            assert!(in_line_bytes &lt;= in_stride);
            assert!(out_line_bytes &lt;= out_stride);

            for (in_line, out_line) in in_data
                .chunks(in_stride)
                .zip(out_data.chunks_mut(out_stride))
            {
                for (in_p, out_p) in in_line[..in_line_bytes]
                    .chunks(4)
                    .zip(out_line[..out_line_bytes].iter_mut())
                {
                    let gray = Rgb2Gray::bgrx_to_gray(in_p);
                    *out_p = gray;
                }
            }
        } else {
            unimplemented!();
        }

        gst::FlowReturn::Ok
    }
}</pre><p> </p>
<p>What happens here is that we first of all lock our state (the input/output <em>VideoInfo</em>) and error out if we don’t have any yet (which can’t really happen unless other elements have a bug, but better safe than sorry). After that we map the input buffer readable and the output buffer writable with the <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_video/video_frame/struct.VideoFrameRef.html" rel="noopener" target="_top"><em>VideoFrameRef</em></a> API. By mapping the buffers we get access to the underlying bytes of them, and the mapping operation could for example make GPU memory available or just do nothing and give us access to a normally allocated memory area. We have access to the bytes of the buffer until the <em>VideoFrameRef</em> goes out of scope.</p>
<p>Instead of <em>VideoFrameRef</em> we could’ve also used the <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/buffer/struct.BufferRef.html#method.map_readable" rel="noopener" target="_top"><em>gst::Buffer::map_readable()</em></a> and <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/buffer/struct.BufferRef.html#method.map_writable" rel="noopener" target="_top"><em>gst::Buffer::map_writable()</em></a> API, but different to those the <em>VideoFrameRef</em> API also extracts various metadata from the raw video buffers and makes them available. For example we can directly access the different planes as slices without having to calculate the offsets ourselves, or we get directly access to the width and height of the video frame.</p>
<p>After mapping the buffers, we store various information we’re going to need later in local variables to save some typing later. This is the width (same for input and output as we never changed the width in <em>transform_caps</em>), the input and out (row-) stride (the number of bytes per row/line, which possibly includes some padding at the end of each line for alignment reasons), the output format (which can be BGRx or GRAY8 because of how we implemented <em>transform_caps</em>) and the pointers to the first plane of the input and output (which in this case also is the only plane, BGRx and GRAY8 both have only a single plane containing all the RGB/gray components).</p>
<p>Then based on whether the output is BGRx or GRAY8, we iterate over all pixels. The code is basically the same in both cases, so I’m only going to explain the case where BGRx is output.</p>
<p>We start by iterating over each line of the input and output, and do so by using the <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.chunks" rel="noopener" target="_top"><em>chunks</em></a> iterator to give us chunks of as many bytes as the (row-) stride of the video frame is, do the same for the other frame and then zip both iterators together. This means that on each iteration we get exactly one line as a slice from each of the frames and can then start accessing the actual pixels in each line.</p>
<p>To access the individual pixels in each line, we again use the <em>chunks</em> iterator the same way, but this time to always give us chunks of 4 bytes from each line. As BGRx uses 4 bytes for each pixel, this gives us exactly one pixel. Instead of iterating over the whole line, we only take the actual sub-slice that contains the pixels, not the whole line with stride number of bytes containing potential padding at the end. Now for each of these pixels we call our previously defined <em>bgrx_to_gray</em> function and then fill the B, G and R components of the output buffer with that value to get grayscale output. And that’s all.</p>
<p>Using Rust high-level abstractions like the <em>chunks</em> iterators and bounds-checking slice accesses might seem like it’s going to cause quite some performance penalty, but if you look at the generated assembly most of the bounds checks are completely optimized away and the resulting assembly code is close to what one would’ve written manually (especially when using the newly-added <a href="https://github.com/rust-lang/rust/pull/47126" rel="noopener" target="_top"><em>exact_chunks</em></a> iterators). Here you’re getting safe and high-level looking code with low-level performance!</p>
<p>You might’ve also noticed the various assertions in the processing function. These are there to give further hints to the compiler about properties of the code, and thus potentially being able to optimize the code better and moving e.g. bounds checks out of the inner loop and just having the assertion outside the loop check for the same. In Rust adding assertions can often improve performance by allowing further optimizations to be applied, but in the end always check the resulting assembly to see if what you did made any difference.</p>
<h3 id="testing">Testing the new element</h3>
<p>Now we implemented almost all functionality of our new element and could run it on actual video data. This can be done now with the <em>gst-launch-1.0</em> tool, or any application using GStreamer and allowing us to insert our new element somewhere in the video part of the pipeline. With <em>gst-launch-1.0</em> you could run for example the following pipelines</p>
<p></p><pre class="crayon-plain-tag"># Run on a test pattern
gst-launch-1.0 videotestsrc ! rsrgb2gray ! videoconvert ! autovideosink

# Run on some video file, also playing the audio
gst-launch-1.0 playbin uri=file:///path/to/some/file video-filter=rsrgb2gray</pre><p> </p>
<p>Note that you will likely want to compile with <em>cargo build –release</em> and add the <em>target/release</em> directory to <em>GST_PLUGIN_PATH</em> instead. The debug build might be too slow, and generally the release builds are multiple orders of magnitude (!) faster.</p>
<h3 id="properties">Properties</h3>
<p>The only feature missing now are the properties I mentioned in the opening paragraph: one boolean property to invert the grayscale value and one integer property to shift the value by up to 255. Implementing this on top of the previous code is not a lot of work. Let’s start with defining a struct for holding the property values and defining the property metadata.</p>
<p></p><pre class="crayon-plain-tag">const DEFAULT_INVERT: bool = false;
const DEFAULT_SHIFT: u32 = 0;

#[derive(Debug, Clone, Copy)]
struct Settings {
    invert: bool,
    shift: u32,
}

impl Default for Settings {
    fn default() -&gt; Self {
        Settings {
            invert: DEFAULT_INVERT,
            shift: DEFAULT_SHIFT,
        }
    }
}

static PROPERTIES: [Property; 2] = [
    Property::Boolean(
        "invert",
        "Invert",
        "Invert grayscale output",
        DEFAULT_INVERT,
        PropertyMutability::ReadWrite,
    ),
    Property::UInt(
        "shift",
        "Shift",
        "Shift grayscale output (wrapping around)",
        (0, 255),
        DEFAULT_SHIFT,
        PropertyMutability::ReadWrite,
    ),
];

struct Rgb2Gray {
    cat: gst::DebugCategory,
    settings: Mutex&lt;Settings&gt;,
    state: Mutex&lt;Option&lt;State&gt;&gt;,
}

impl Rgb2Gray {
    fn new(_transform: &amp;BaseTransform) -&gt; Box&lt;BaseTransformImpl&lt;BaseTransform&gt;&gt; {
        Box::new(Self {
            cat: gst::DebugCategory::new(
                "rsrgb2gray",
                gst::DebugColorFlags::empty(),
                "Rust RGB-GRAY converter",
            ),
            settings: Mutex::new(Default::default()),
            state: Mutex::new(None),
        })
    }
}</pre><p> </p>
<p>This should all be rather straightforward: we define a <em>Settings</em> struct that stores the two values, implement the <a href="https://doc.rust-lang.org/nightly/std/default/trait.Default.html" rel="noopener" target="_top"><em>Default</em></a> trait for it, then define a two-element array with property metadata (names, description, ranges, default value, writability), and then store the default value of our <em>Settings</em> struct inside another <em>Mutex</em> inside the element struct.</p>
<p>In the next step we have to make use of these: we need to tell the GObject type system about the properties, and we need to implement functions that are called whenever a property value is set or get.</p>
<p></p><pre class="crayon-plain-tag">impl Rgb2Gray {
    fn class_init(klass: &amp;mut BaseTransformClass) {
        [...]
        klass.install_properties(&amp;PROPERTIES);
        [...]
    }
}

impl ObjectImpl&lt;BaseTransform&gt; for Rgb2Gray {
    fn set_property(&amp;self, obj: &amp;glib::Object, id: u32, value: &amp;glib::Value) {
        let prop = &amp;PROPERTIES[id as usize];
        let element = obj.clone().downcast::&lt;BaseTransform&gt;().unwrap();

        match *prop {
            Property::Boolean("invert", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let invert = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing invert from {} to {}",
                    settings.invert,
                    invert
                );
                settings.invert = invert;
            }
            Property::UInt("shift", ..) =&gt; {
                let mut settings = self.settings.lock().unwrap();
                let shift = value.get().unwrap();
                gst_info!(
                    self.cat,
                    obj: &amp;element,
                    "Changing shift from {} to {}",
                    settings.shift,
                    shift
                );
                settings.shift = shift;
            }
            _ =&gt; unimplemented!(),
        }
    }

    fn get_property(&amp;self, _obj: &amp;glib::Object, id: u32) -&gt; Result&lt;glib::Value, ()&gt; {
        let prop = &amp;PROPERTIES[id as usize];

        match *prop {
            Property::Boolean("invert", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.invert.to_value())
            }
            Property::UInt("shift", ..) =&gt; {
                let settings = self.settings.lock().unwrap();
                Ok(settings.shift.to_value())
            }
            _ =&gt; unimplemented!(),
        }
    }
}</pre><p> </p>
<p>Property values can be changed from any thread at any time, that’s why the <em>Mutex</em> is needed here to protect our struct. And we’re using a new mutex to be able to have it locked only for the shorted possible amount of time: we don’t want to keep it locked for the whole time of the <em>transform</em> function, otherwise applications trying to set/get values would block for up to one frame.</p>
<p>In the property setter/getter functions we are working with a <em>glib::Value</em>. This is a dynamically typed value type that can contain values of any type, together with the type information of the contained value. Here we’re using it to handle an unsigned integer (u32) and a boolean for our two properties. To know which property is currently set/get, we get an identifier passed which is the index into our <em>PROPERTIES</em> array. We then simply match on the name of that to decide which property was meant</p>
<p>With this implemented, we can already compile everything, see the properties and their metadata in <em>gst-inspect-1.0</em> and can also set them on gst-launch-1.0 like this</p>
<p></p><pre class="crayon-plain-tag"># Set invert to true and shift to 128
gst-launch-1.0 videotestsrc ! rsrgb2gray invert=true shift=128 ! videoconvert ! autovideosink</pre><p> </p>
<p>If we set <em>GST_DEBUG=rsrgb2gray:6</em> in the environment before running that, we can also see the corresponding debug output when the values are changing. The only thing missing now is to actually make use of the property values for the processing. For this we add the following changes to <em>bgrx_to_gray</em> and the <em>transform</em> function</p>
<p></p><pre class="crayon-plain-tag">impl Rgb2Gray {
    #[inline]
    fn bgrx_to_gray(in_p: &amp;[u8], shift: u8, invert: bool) -&gt; u8 {
        [...]

        let gray = ((r * R_Y) + (g * G_Y) + (b * B_Y)) / 65536;
        let gray = (gray as u8).wrapping_add(shift);

        if invert {
            255 - gray
        } else {
            gray
        }
    }
}

impl BaseTransformImpl&lt;BaseTransform&gt; for Rgb2Gray {
    fn transform(
        &amp;self,
        element: &amp;BaseTransform,
        inbuf: &amp;gst::Buffer,
        outbuf: &amp;mut gst::BufferRef,
    ) -&gt; gst::FlowReturn {
        let settings = *self.settings.lock().unwrap();
        [...]
                    let gray = Rgb2Gray::bgrx_to_gray(in_p, settings.shift as u8, settings.invert);
        [...]
    }
}</pre><p> </p>
<p>And that’s all. If you run the element in <em>gst-launch-1.0</em> and change the values of the properties you should also see the corresponding changes in the video output.</p>
<p>Note that we always take a copy of the <em>Settings</em> struct at the beginning of the <em>transform</em> function. This ensures that we take the mutex only the shorted possible amount of time and then have a local snapshot of the settings for each frame.</p>
<p>Also keep in mind that the usage of the property values in the <em>bgrx_to_gray</em> function is far from optimal. It means the addition of another condition to the calculation of each pixel, thus potentially slowing it down a lot. Ideally this condition would be moved outside the inner loops and the <em>bgrx_to_gray</em> function would made generic over that. See for example <a href="https://bluejekyll.github.io/blog/rust/2018/01/10/branchless-rust.html" rel="noopener" target="_top">this blog post</a> about “branchless Rust” for ideas how to do that, the actual implementation is left as an exercise for the reader.</p>
<h3 id="what-next">What next?</h3>
<p>I hope the code walkthrough above was useful to understand how to implement GStreamer plugins and elements in Rust. If you have any questions, feel free to ask them here in the comments.</p>
<p>The same approach also works for audio filters or anything that can be handled in some way with the API of the <em>BaseTransform</em> base class. You can find another filter, an audio echo filter, using the same approach <a href="https://github.com/sdroege/gst-plugin-rs/blob/0.1/gst-plugin-audiofx/src/audioecho.rs" rel="noopener" target="_top">here</a>.</p>
<p>In the next blog post in this series I’ll show how to use another base class to implement another kind of element, but for the time being you can also check <a href="https://github.com/sdroege/gst-plugin-rs/tree/0.1" rel="noopener" target="_top">the GIT repository</a> for various other element implementations.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2018/01/how-to-write-gstreamer-elements-in-rust-part-1-a-video-filter-for-converting-rgb-to-grayscale/">by slomo at January 13, 2018 10:23 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">January 11, 2018</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://wingolog.org/2018/01/11/spectre-and-the-end-of-langsec">
<h3><a href="http://wingolog.org/" title="wingolog">Andy Wingo</a> — <a href="http://wingolog.org/archives/2018/01/11/spectre-and-the-end-of-langsec">spectre and the end of langsec</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="awingo.png" alt="(Andy Wingo)" width="74" height="100">
<div><p>I remember in 2008 seeing Gerald Sussman, creator of the Scheme language, resignedly describing a sea change in the MIT computer science curriculum.  In response to a question from the audience, <a href="https://wingolog.org/archives/2009/03/24/international-lisp-conference-day-two">he said</a>:</p><blockquote><p>The work of engineers used to be about taking small parts that they understood entirely and using simple techniques to compose them into larger things that do what they want.</p><p>But programming now isn't so much like that.  Nowadays you muck around with incomprehensible or nonexistent man pages for software you don't know who wrote.  <i>You have to do basic science on your libraries to see how they work</i>, trying out different inputs and seeing how the code reacts.  This is a fundamentally different job.  </p></blockquote><p>Like many I was profoundly saddened by this analysis.  I want to believe in constructive correctness, in math and in proofs.  And so with the rise of functional programming, I thought that this historical slide from reason towards observation was just that, historical, and that the "safe" languages had a compelling value that would be evident eventually: that "another world is possible".</p><p>In particular I found solace in "langsec", an approach to assessing and ensuring system security in terms of constructively correct programs.  One obvious application is parsing of untrusted input, and indeed the <a href="http://langsec.org/">langsec.org website</a> appears to emphasize this domain as one in which a programming languages approach can be fruitful.  It is, after all, <a href="https://www.gnu.org/software/guile/manual/html_node/Types-and-the-Web.html#Types-and-the-Web">a truth universally acknowledged, that a program with good use of data types, will be free from many common bugs.</a> So far so good, and so far so successful.</p><p>The basis of language security is starting from a programming language with a well-defined, easy-to-understand semantics.  From there you can prove (formally or informally) interesting security properties about particular programs.  For example, if a program has a secret <i>k</i>, but some untrusted subcomponent <i>C</i> of it should not have access to <i>k</i>, one can prove if <i>k</i> can or cannot leak to <i>C</i>.  This approach is taken, for example, by <a href="https://developers.google.com/caja/">Google's Caja compiler</a> to isolate components from each other, even when they run in the context of the same web page.</p><p>But the <a href="https://spectreattack.com/">Spectre</a> and <a href="https://meltdownattack.com/">Meltdown</a> attacks have seriously set back this endeavor.  One manifestation of the Spectre vulnerability is that code running in a process can now read the entirety of its address space, bypassing invariants of the language in which it is written, even if it is written in a "safe" language.  This is currently being used by JavaScript programs to exfiltrate passwords from a browser's password manager, or bitcoin wallets.</p><p>Mathematically, in terms of the semantics of e.g. JavaScript, these attacks should not be possible.  But practically, they work.  Spectre shows us that the building blocks provided to us by Intel, ARM, and all the rest are no longer "small parts understood entirely"; that instead now we have to do "basic science" on our CPUs and memory hierarchies to know what they do.</p><p>What's worse, we need to do basic science to come up with adequate mitigations to the Spectre vulnerabilities (side-channel exfiltration of results of speculative execution).  <a href="https://support.google.com/faqs/answer/7625886">Retpolines</a>, <a href="https://webkit.org/blog/8048/what-spectre-and-meltdown-mean-for-webkit/">poisons and masks</a>, et cetera: none of these are <i>proven</i> to work.  They are simply <i>observed</i> to be effective on current hardware.  Indeed mitigations are anathema to the correctness-by-construction: if you can prove that a problem doesn't exist, what is there to mitigate?</p><p>Spectre is not the first crack in the edifice of practical program correctness.  In particular, <a href="https://wingolog.org/archives/2014/12/02/there-are-no-good-constant-time-data-structures">timing side channels</a> are rarely captured in language semantics.  But I think it's fair to say that Spectre is the most devastating vulnerability in the langsec approach to security that has ever been uncovered.</p><p>Where do we go from here?  I see but two options.  One is to attempt to make the behavior of the machines targetted by secure language implementations behave rigorously as architecturally specified, and in no other way.  This is the approach taken by all of the deployed mitigations (retpolines, poisoned pointers, masked accesses): modify the compiler and runtime to prevent the CPU from speculating through vulnerable indirect branches (prevent speculative execution), or from using fetched values in further speculative fetches (prevent this particular side channel).  I think we are missing a model and a proof that these mitigations restore target architectural semantics, though.</p><p>However if we did have a model of what a CPU does, we have another opportunity, which is to incorporate that model in a semantics of the target language of a compiler (e.g. micro-x86 versus x86).  It could be that this model produces a co-evolution of the target architectures as well, whereby Intel decides to disclose and expose more of its microarchitecture to user code.  Cacheing and other microarchitectural side-effects would then become explicit rather than transparent.</p><p>Rich Hickey has this thing where he talks about "simple versus easy".  Both of them sound good but for him, only "simple" is good whereas "easy" is bad.  It's the sort of subjective distinction that can lead to an endless string of <a href="https://www.dreamsongs.com/Files/worse-is-worse.pdf">Worse Is Better Is Worse</a> Bourbaki papers, according to the perspective of the author.  Anyway transparent caching in the CPU has been marvelously easy for most application developers and fantastically beneficial from a performance perspective.  People needing constant-time operations have complained, of course, but that kind of person always complains.  Could it be, though, that actually there is some other, better-is-better kind of simplicity that should replace the all-pervasive, now-treacherous transparent cacheing?</p><p>I don't know.  All I will say is that an ad-hoc approach to determining which branches and loads are safe and which are not is not a plan that inspires confidence.  Godspeed to the langsec faithful in these dark times.</p></div></div>

<p class="date">
<a href="http://wingolog.org/archives/2018/01/11/spectre-and-the-end-of-langsec">by Andy Wingo at January 11, 2018 01:44 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 30, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="http://blog.mikeasoft.com/?p=752" lang="en-US">
<h3><a href="http://blog.mikeasoft.com/" title="Michael Sheldon's Stuff">Michael Sheldon</a> — <a href="http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/">Speech Recognition – Mozilla’s DeepSpeech, GStreamer and IBus</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="elleo.png" alt="(Michael Sheldon)" width="80" height="76">
<p>Recently Mozilla released <a href="https://github.com/mozilla/DeepSpeech">an open source implementation of Baidu’s DeepSpeech architecture</a>, along with a pre-trained model using data collected as part of their <a href="https://voice.mozilla.org/">Common Voice</a> project.</p>
<p>In an attempt to make it easier for application developers to start working with the DeepSpeech model I’ve developed a GStreamer plugin, an IBus plugin and created some PPAs. To demonstrate what’s possible here’s a video of the IBus plugin providing speech recognition to any application under Linux:</p>
<p></p><center><br>
<br>
<br>
<a href="https://youtu.be/Kjos5s0ZZDM">Video of DeepSpeech IBus Plugin</a><br>
</center><br>
<p></p>
<h3>GStreamer DeepSpeech Plugin</h3>
<p>I’ve created a GStreamer element which can be placed into an audio pipeline, it will then report any recognised speech via bus messages. It automatically segments audio based on configurable silence thresholds making it suitable for continuous dictation.</p>
<p>Here’s a couple of example pipelines using gst-launch.</p>
<p>To perform speech recognition on a file, printing all bus messages to the terminal:</p>
<p><code>gst-launch-1.0 -m filesrc location=/path/to/file.ogg ! decodebin ! audioconvert ! audiorate ! audioresample ! deepspeech ! fakesink</code></p>
<p>To perform speech recognition on audio recorded from the default system microphone, with changes to the silence thresholds:</p>
<p><code>gst-launch-1.0 -m pulsesrc ! audioconvert ! audiorate ! audioresample ! deepspeech silence-threshold=0.3 silence-length=20 ! fakesink</code></p>
<p>The source code is available here: <a href="https://github.com/Elleo/gst-deepspeech">https://github.com/Elleo/gst-deepspeech</a>.</p>
<h3>IBus Plugin</h3>
<p>I’ve also created a proof of concept IBus plugin which allows speech recognition to be used as an input method for virtually any application. It uses the above GStreamer plugin to perform speech recognition and then commits the text to the currently focused input field whenever a bus message is received from the deepspeech element.</p>
<p>It’ll need a lot more work before it’s really useful, especially in terms of adding in various voice editing commands, but hopefully it’ll provide a useful starting point for something more complete.</p>
<p>The source code is available here: <a href="https://github.com/Elleo/ibus-deepspeech">https://github.com/Elleo/ibus-deepspeech</a></p>
<h3>PPAs</h3>
<p>To make it extra easy to get started playing around with these projects I’ve also created a couple of PPAs for Ubuntu 17.10:</p>
<p><a href="https://launchpad.net/~michael-sheldon/+archive/ubuntu/deepspeech">DeepSpeech PPA</a> – This contains packages for libdeepspeech, libdeepspeech-dev, libtensorflow-cc and deepspeech-model (be warned, the model is around 1.3GB).</p>
<p><a href="https://launchpad.net/~michael-sheldon/+archive/ubuntu/gst-deepspeech">gst-deepspeech PPA</a> – This contains packages for my GStreamer and IBus plugins (gstreamer1.0-deepspeech and ibus-deepspeech). Please note that you’ll also need the DeepSpeech PPA enabled to fulfil the dependencies of these packages.</p>
<p>I’d love to hear about any projects that find these plugins useful <img src="1f642.png" alt="🙂" class="wp-smiley"></p></div>

<p class="date">
<a href="http://blog.mikeasoft.com/2017/12/30/speech-recognition-mozillas-deepspeech-gstreamer-and-ibus/">by Mike at December 30, 2017 09:13 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 22, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://coaxion.net/blog/?p=515" lang="en-US">
<h3><a href="https://coaxion.net/blog" title="coaxion.net – slomo's blog">Sebastian Dröge</a> — <a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/">GStreamer Rust bindings release 0.10.0 &amp; gst-plugin release 0.1.0</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="slomo.png" alt="(Sebastian Dröge)" width="80" height="80">
<p>Today I’ve released version 0.10.0 of the <a href="https://rust-lang.org/" rel="noopener" target="_top">Rust</a> <a href="https://gstreamer.freedesktop.org/" rel="noopener" target="_top">GStreamer</a> <a href="https://crates.io/crates/gstreamer" rel="noopener" target="_top">bindings</a>, and <a href="https://coaxion.net/blog/2016/05/writing-gstreamer-plugins-and-elements-in-rust/" rel="noopener" target="_top">after a journey of more than 1½ years</a> the first release of the GStreamer plugin writing infrastructure crate <a href="https://crates.io/crates/gst-plugin" rel="noopener" target="_top">“gst-plugin”</a>.</p>
<p>Check the repositories<a href="https://github.com/sdroege/gstreamer-rs" rel="noopener" target="_top">¹</a><a href="https://github.com/sdroege/gst-plugin-rs" rel="noopener" target="_top">²</a> of both for more details, the code and various examples.</p>
<h4>GStreamer Bindings</h4>
<p>Some of the changes since the 0.9.0 release were already outlined in the previous blog post, and most of the other changes were also things I found while writing GStreamer plugins. For the full changelog, take a look at the <a href="https://github.com/sdroege/gstreamer-rs/blob/master/gstreamer/CHANGELOG.md#0100---2017-12-22" rel="noopener" target="_top">CHANGELOG.md</a> in the repository.</p>
<p>Other changes include</p>
<ul>
<li>I went over the whole API in the last days, added any missing things I found, simplified API as it made sense, changed functions to take <i>Option&lt;_&gt;</i> if allowed, etc.</li>
<li>Bindings for <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.SliceTypeFind.html#method.type_find" rel="noopener" target="_top">using</a> and <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.TypeFind.html#method.register" rel="noopener" target="_top">writing</a> typefinders. Typefinders are the part of GStreamer that try to guess what kind of media is to be handled based on looking at the bytes. Especially writing those in Rust seems worthwhile, considering that basically all of the <a href="https://cgit.freedesktop.org/gstreamer/gst-plugins-base/log/gst/typefind/gsttypefindfunctions.c" rel="noopener" target="_top">GIT log</a> of the existing typefinders consists of fixes for various kinds of memory-safety problems.</li>
<li>Bindings for the <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer/struct.Registry.html" rel="noopener" target="_top">Registry</a> and PluginFeature were added, as well as fixing the relevant API that works with paths/filenames to actually work on <a href="https://doc.rust-lang.org/std/path/struct.Path.html" rel="noopener" target="_top">Paths</a></li>
<li>Bindings for the GStreamer Net library were added, allowing to build applications that synchronize their media of the network by using <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.PtpClock.html" rel="noopener" target="_top">PTP</a>, <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.NtpClock.html" rel="noopener" target="_top">NTP</a> or a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.NetClientClock.html" rel="noopener" target="_top">custom</a> GStreamer protocol (for which there also exists a <a href="https://sdroege.github.io/rustdoc/gstreamer/gstreamer_net/struct.NetTimeProvider.html" rel="noopener" target="_top">server</a>). This could be used for building video-walls, systems recording the same scene from multiple cameras, etc. and provides (depending on network conditions) up to &lt; 1ms synchronization between devices.</li>
</ul>
<p>Generally, this is something like a “1.0” release for me now (due to depending on too many pre-1.0 crates this is not going to be 1.0 anytime soon). The basic API is all there and nicely usable now and hopefully without any bugs, the known-missing APIs are not too important for now and can easily be added at a later time when needed. At this point I don’t expect many API changes anymore.</p>
<h4>GStreamer Plugins</h4>
<p>The other important part of this announcement is the first release of the <a href="https://crates.io/crates/gst-plugin" rel="noopener" target="_top">“gst-plugin”</a> crate. This provides the basic infrastructure for writing GStreamer plugins and elements in Rust, without having to write any unsafe code.</p>
<p>I started experimenting with using Rust for this more than 1½ years ago, and while a lot of things have changed in that time, this release is a nice milestone. In the beginning there were no GStreamer bindings and I was writing everything manually, and there were also still quite a few pieces of code written in C. Nowadays everything is in Rust and using the automatically generated GStreamer bindings.</p>
<p>Unfortunately there is no real documentation for any of this yet, there’s only the autogenerated rustdoc documentation available from <a href="https://sdroege.github.io/rustdoc/gst-plugin/gst_plugin/" rel="noopener" target="_top">here</a>, and various example GStreamer plugins inside the <a href="https://github.com/sdroege/gst-plugin-rs" rel="noopener" target="_top">GIT repository </a>that can be used as a starting point. And various people already wrote their GStreamer plugins in Rust based on this.</p>
<p>The basic idea of the API is however that everything is as Rust-y as possible. Which might not be too much due to having to map subtyping, virtual methods and the like to something reasonable in Rust, but I believe it’s nice to use now. You basically only have to implement one or more traits on your structs, and that’s it. There’s still quite some boilerplate required, but it’s far less than what would be required in C. The best example at this point might be the <a href="https://github.com/sdroege/gst-plugin-rs/blob/master/gst-plugin-audiofx/src/audioecho.rs" rel="noopener" target="_top">audioecho</a> element.</p>
<p>Over the next days (or weeks?) I’m not going to write any documentation yet, but instead will write a couple of very simple, minimal elements that do basically nothing and can be used as starting points to learn how all this works together. And will write another blog post or two about the different parts of writing a GStreamer plugin and element in Rust, so that all of you can get started with that.</p>
<p>Let’s hope that the number of new GStreamer plugins written in C is going to decrease in the future, and maybe even new people who would’ve never done that in C, with all the footguns everywhere, can get started with writing GStreamer plugins in Rust now.</p></div>

<p class="date">
<a href="https://coaxion.net/blog/2017/12/gstreamer-rust-bindings-release-0-10-0-gst-plugin-release-0-1-0/">by slomo at December 22, 2017 04:52 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">


<div class="entrygroup" id="https://k-d-w.org/103 at https://k-d-w.org" lang="en">
<h3><a href="https://k-d-w.org/rss.xml" title="Sebastian Pölsterl's blog - Where he blogs about his latest hacking adventures.">Sebastian Pölsterl</a> — <a href="https://k-d-w.org/node/103">Denoising Autoencoder as TensorFlow estimator</a></h3>
<div class="entry">
<div class="content">
<span class="field field-name-title field-formatter-string field-type-string field-label-hidden">Denoising Autoencoder as TensorFlow estimator</span>
<div class="clearfix text-formatted field field-node--body field-formatter-text-default field-name-body field-type-text-with-summary field-label-hidden has-single"><div class="field__items"><div class="field__item"><div class="tex2jax_process"><p>I recently started to use Google's deep learning framework TensorFlow. Since version 1.3, TensorFlow includes a <a href="https://www.tensorflow.org/get_started/estimator">high-level interface</a> inspired by scikit-learn. Unfortunately, as of version 1.4, only 3 different classification and 3 different regression models implementing the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface are included. To better understand the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface, <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> API, and components in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">tf-slim</a>, I started to implement a simple Autoencoder and applied it to the well-known MNIST dataset of handwritten digits. This post is about my journey and is split in the following sections:
</p><ol><li><a href="https://k-d-w.org/rss.xml#estimators">Custom Estimators</a></li>
<li><a href="https://k-d-w.org/rss.xml#autoencoder-net">Autoencoder network architecture</a></li>
<li><a href="https://k-d-w.org/rss.xml#autoencoder-model-fn">Autoencoder as TensorFlow Estimator</a></li>
<li><a href="https://k-d-w.org/rss.xml#dataset-api">Using the Dataset API</a></li>
<li><a href="https://k-d-w.org/rss.xml#denoising-autoencoder">Denoising Autocendoer</a></li>
</ol><p>I will assume that you are familiar with TensorFlow basics. The full code is available at <a href="https://github.com/sebp/tf_autoencoder">https://github.com/sebp/tf_autoencoder</a>.</p>
<p>A second part on <a href="https://k-d-w.org/node/107">Convolutional Autoencoders</a> is available too.</p>
<!--break--><h2>Estimators</h2>
<p><a name="estimators" id="estimators"></a>The <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">tf.estimator.Estimator</a> is at the heart TenorFlow's high-level interface and is similar to <a href="https://keras.io/models/model/">Kera's Model API</a>. It hides most of the boilerplate required to train a model: managing <span class="geshifilter"><code class="text geshifilter-text">Sessions</code></span>, writing summary statistics for TensorBoard, or saving and loading checkpoints. An <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> has three main methods: <span class="geshifilter"><code class="text geshifilter-text">train</code></span>, <span class="geshifilter"><code class="text geshifilter-text">evaluate</code></span>, and <span class="geshifilter"><code class="text geshifilter-text">predict</code></span>. Each of these methods requires a callable input function as first argument that feeds the data to the estimator (more on that later).</p>
<p><img src="estimators1.jpeg" width="350"></p>
<h2>Custom estimators</h2>
<p>You can write your own custom model implementing the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface by passing a function returning an instance of <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.EstimatorSpec</code></span> as first argument to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator</code></span>.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> model_fn<span class="br0">(</span>features<span class="sy0">,</span> labels<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; …
&nbsp; &nbsp; <span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; train_op<span class="sy0">=</span>train_op<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; eval_metric_ops<span class="sy0">=</span>eval_metric_ops<span class="br0">)</span></pre></div></div>
<p>The first argument – <span class="geshifilter"><code class="text geshifilter-text">mode</code></span> – is one of <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.TRAIN</code></span>, <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.EVAL</code></span> or <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.PREDICT</code></span> and determines which of the remaining values must be provided.</p>
<p>In <span class="geshifilter"><code class="text geshifilter-text">TRAIN</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">loss</code></span>: A Tensor containing a scalar loss value.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">train_op</code></span>: An Op that runs one step of training. We can use the return value of <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss">tf.contrib.layers.optimize_loss</a> here.</li>
</ul><p>In <span class="geshifilter"><code class="text geshifilter-text">EVAL</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">loss</code></span>: A scalar Tensor containing the loss on the validation data.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">eval_metric_ops</code></span>: A dictionary that maps metric names to Tensors of metrics to calculate, typically, one of the <a href="https://www.tensorflow.org/api_docs/python/tf/metrics">tf.metrics</a> functions.</li>
</ul><p>In <span class="geshifilter"><code class="text geshifilter-text">PREDICT</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">predictions</code></span>: A dictionary that maps key names of your choice to Tensors containing the predictions from the model.</li>
</ul><p>An important difference to the Estimators included with TensorFlow is that we need to call relevant <span class="geshifilter"><code class="text geshifilter-text">tf.summary</code></span> functions in <span class="geshifilter"><code class="text geshifilter-text">model_fn</code></span> ourselves. However, the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> will take care of writing summaries to disk so we can inspect them in TenorBoard.</p>
<h2>Autoencoder model</h2>
<p><a name="autoencoder-net" id="autoencoder-net"></a><img src="autoencoder.svg" alt="Autoencoder architecture" width="400"></p>
<p>The Autoencoder model is straightforward, it consists of two major parts: an encoder and an decoder. The encoder has an input layer (28*28 = 784 dimensions in the case of MNIST) and one or more hidden layers, decreasing in size. In the decoder, we reverse the operations of the encoder by blowing the output of the smallest hidden layer up to the size of the input (optionally, with hidden layers of increasing size in-between). The loss function computes the difference between the original image and the reconstructed image (the output of the decoder). Common loss functions are mean squared error and <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">cross-entropy</a>.</p>
<p>To construct the encoder network, we specify a list containing the number of hidden units for each layer and (optionally) add dropout layers in-between:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> encoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">for</span> num_hidden_units <span class="kw1">in</span> hidden_units:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_hidden_units<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> dropout <span class="kw1">is</span> <span class="kw1">not</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">dropout</span><span class="br0">(</span>net<span class="sy0">,</span> is_training<span class="sy0">=</span>is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; add_hidden_layer_summary<span class="br0">(</span>net<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div><br>
where <span class="geshifilter"><code class="text geshifilter-text">add_hidden_layer_summary</code></span> adds a histogram of the activations and the fraction of non-zero activations to be displayed in TensorBoard. The latter is particularly useful when debugging networks with <a href="https://cs231n.github.io/neural-networks-1/">rectified linear units (ReLU)</a>. If too many hidden units return 0 values early during optimization, the model won't be able to learn anymore, in which case one would typically try to lower the learning rate or choose a different activation function.
<p>The network of the decoder is almost identical, we just explicitly use a linear activation function (<span class="geshifilter"><code class="text geshifilter-text">activation_fn=None</code></span>) and no dropout in the last layer:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> decoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">for</span> num_hidden_units <span class="kw1">in</span> hidden_units<span class="br0">[</span>:-<span class="nu0">1</span><span class="br0">]</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_hidden_units<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> dropout <span class="kw1">is</span> <span class="kw1">not</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">dropout</span><span class="br0">(</span>net<span class="sy0">,</span> is_training<span class="sy0">=</span>is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; add_hidden_layer_summary<span class="br0">(</span>net<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>net<span class="sy0">,</span> hidden_units<span class="br0">[</span>-<span class="nu0">1</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>
&nbsp; &nbsp; tf.<span class="me1">summary</span>.<span class="me1">histogram</span><span class="br0">(</span><span class="st0">'activation'</span><span class="sy0">,</span> net<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div>
<p>You may have noticed that we did no specify any activation function so far. Thanks to TenorFlow's <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/framework/arg_scope">arg_scope context manager</a>, we can easily set the activation function for <em>all</em> fully connected layers. At the same time we set an appropriate weight initializer and  (optionally) use weight decay:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> autoencoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> activation_fn<span class="sy0">,</span> dropout<span class="sy0">,</span> weight_decay<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; is_training <span class="sy0">=</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>
&nbsp;
&nbsp; &nbsp; weights_init <span class="sy0">=</span> slim.<span class="me1">initializers</span>.<span class="me1">variance_scaling_initializer</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">if</span> weight_decay <span class="kw1">is</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp; &nbsp; <span class="kw1">else</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_reg <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">l2_regularizer</span><span class="br0">(</span>weight_decay<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">with</span> slim.<span class="me1">arg_scope</span><span class="br0">(</span><span class="br0">[</span>tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_initializer<span class="sy0">=</span>weights_init<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer<span class="sy0">=</span>weights_reg<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span>activation_fn<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> encoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; n_features <span class="sy0">=</span> inputs.<span class="me1">shape</span><span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>.<span class="me1">value</span>
&nbsp; &nbsp; &nbsp; &nbsp; decoder_units <span class="sy0">=</span> hidden_units<span class="br0">[</span>:-<span class="nu0">1</span><span class="br0">]</span><span class="br0">[</span>::-<span class="nu0">1</span><span class="br0">]</span> + <span class="br0">[</span>n_features<span class="br0">]</span>
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> decoder<span class="br0">(</span>net<span class="sy0">,</span> decoder_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div><br>
where <span class="geshifilter"><code class="text geshifilter-text">slim.initializers.variance_scaling_initializer</code></span> corresponds to the <a href="https://arxiv.org/abs/1502.01852">initialization of He et al.</a>, which is the <a href="https://cs231n.github.io/neural-networks-2/#init">current recommendation</a> for networks with ReLU activations.
<p>This concludes the architecture of the autoencoder. Next, we need to implement the <span class="geshifilter"><code class="text geshifilter-text">model_fn</code></span> function passed to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator</code></span> as outlined above.</p>
<h2>Autoencoder model_fn</h2>
<p><a name="autoencoder-model-fn" id="autoencoder-model-fn"></a>First, we construct the network's architecture using the <span class="geshifilter"><code class="text geshifilter-text">autoencoder</code></span> function described above:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">logits <span class="sy0">=</span> autoencoder<span class="br0">(</span>inputs<span class="sy0">=</span>features<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_units<span class="sy0">=</span>hidden_units<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;activation_fn<span class="sy0">=</span>activation_fn<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dropout<span class="sy0">=</span>dropout<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;weight_decay<span class="sy0">=</span>weight_decay<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mode<span class="sy0">=</span>mode<span class="br0">)</span></pre></div></div>
<p>Subsequent steps depend on the value of <span class="geshifilter"><code class="text geshifilter-text">mode</code></span>. In prediction mode, we merely have to return the reconstructed image, therefore we make sure all values are within the interval [0; 1] by applying the sigmoid function:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">probs <span class="sy0">=</span> tf.<span class="me1">nn</span>.<span class="me1">sigmoid</span><span class="br0">(</span>logits<span class="br0">)</span>
predictions <span class="sy0">=</span> <span class="br0">{</span><span class="st0">"prediction"</span>: probs<span class="br0">}</span>
<span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">PREDICT</span>:
&nbsp; &nbsp; <span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="br0">)</span></pre></div></div>
<p>In training and evaluation mode, we need to compute the loss, which is cross-entropy in this example:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">tf.<span class="me1">losses</span>.<span class="me1">sigmoid_cross_entropy</span><span class="br0">(</span>labels<span class="sy0">,</span> logits<span class="br0">)</span>
total_loss <span class="sy0">=</span> tf.<span class="me1">losses</span>.<span class="me1">get_total_loss</span><span class="br0">(</span>add_regularization_losses<span class="sy0">=</span>is_training<span class="br0">)</span></pre></div></div><br>
The second line is needed to add the $\ell_2$-losses used in weight decay.
<p>Most importantly, training relies on choosing an optimizer, here we use <a href="http://arxiv.org/abs/1412.6980">Adam</a> and an exponential learning rate decay. The latter dynamically updates the learning rate during training according to the formula<br>
$$<br>
\text{decayed learning rate} = \text{base learning rate} \cdot 0.96^{\lfloor i / 1000 \rfloor} ,<br>
$$ where $i$ is the current iteration. It would probably work as well without learning rate decay, but I included it for the sake of completeness.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>:
&nbsp; &nbsp; train_op <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">optimize_loss</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; optimizer<span class="sy0">=</span><span class="st0">"Adam"</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; learning_rate<span class="sy0">=</span>learning_rate<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; learning_rate_decay_fn<span class="sy0">=</span><span class="kw1">lambda</span> lr<span class="sy0">,</span> gs: tf.<span class="me1">train</span>.<span class="me1">exponential_decay</span><span class="br0">(</span>lr<span class="sy0">,</span> gs<span class="sy0">,</span> <span class="nu0">1000</span><span class="sy0">,</span> <span class="nu0">0.96</span><span class="sy0">,</span> staircase<span class="sy0">=</span><span class="kw2">True</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; global_step<span class="sy0">=</span>tf.<span class="me1">train</span>.<span class="me1">get_global_step</span><span class="br0">(</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; summaries<span class="sy0">=</span><span class="br0">[</span><span class="st0">"learning_rate"</span><span class="sy0">,</span> <span class="st0">"global_gradient_norm"</span><span class="br0">]</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="co1"># Add histograms for trainable variables</span>
&nbsp; &nbsp; <span class="kw1">for</span> var <span class="kw1">in</span> tf.<span class="me1">trainable_variables</span><span class="br0">(</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">summary</span>.<span class="me1">histogram</span><span class="br0">(</span>var.<span class="me1">op</span>.<span class="me1">name</span><span class="sy0">,</span> var<span class="br0">)</span></pre></div></div><br>
Note that we add a histogram of all trainable variables for TensorBoard in the last part.
<p>Finally, we compute the root mean squared error when in evaluation mode:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">EVAL</span>:
&nbsp; &nbsp; eval_metric_ops <span class="sy0">=</span> <span class="br0">{</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="st0">"rmse"</span>: tf.<span class="me1">metrics</span>.<span class="me1">root_mean_squared_error</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">cast</span><span class="br0">(</span>labels<span class="sy0">,</span> tf.<span class="me1">float64</span><span class="br0">)</span><span class="sy0">,</span> tf.<span class="me1">cast</span><span class="br0">(</span>probs<span class="sy0">,</span> tf.<span class="me1">float64</span><span class="br0">)</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="br0">}</span></pre></div></div><br>
and return the specification of our autoencoder estimator:<br><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="sy0">,</span>
&nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; train_op<span class="sy0">=</span>train_op<span class="sy0">,</span>
&nbsp; &nbsp; eval_metric_ops<span class="sy0">=</span>eval_metric_ops<span class="br0">)</span></pre></div></div><br>
&nbsp;
<h2>Feeding data to an Estimator via the Dataset API</h2>
<p><a name="dataset-api" id="dataset-api"></a>Once we constructed our estimator, e.g. via<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">estimator <span class="sy0">=</span> AutoEncoder<span class="br0">(</span>hidden_units<span class="sy0">=</span><span class="br0">[</span><span class="nu0">128</span><span class="sy0">,</span> <span class="nu0">64</span><span class="sy0">,</span> <span class="nu0">32</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropout<span class="sy0">=</span><span class="kw2">None</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weight_decay<span class="sy0">=</span><span class="nu0">1e-5</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; learning_rate<span class="sy0">=</span><span class="nu0">0.001</span><span class="br0">)</span></pre></div></div><br>
we would like to train it by calling <span class="geshifilter"><code class="text geshifilter-text">train</code></span>, which expects a callable that returns two tensors, one representing the input data and one the groundtruth data. The easiest way would be to use <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn">tf.estimator.inputs.numpy_input_fn</a>, but instead I want to introduce TensorFlow's Dataset API, which is more generic.
<p>The Dataset API comprises two elements:</p>
<ol><li><span class="geshifilter"><code class="text geshifilter-text">tf.data.Dataset</code></span> represents a dataset and any transformations applied to it.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">tf.data.Iterator</code></span> is used to extract elements from a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span>. In particular, <span class="geshifilter"><code class="text geshifilter-text">Iterator.get_next()</code></span> returns the next element of a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> and typically is what is fed to an estimator.</li>
</ol><p>Here, I'm using what is called an <em>initializable</em> Iterator, inspired by <a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0">this post</a>. We define one placeholder for the input image and one for the groundtruth image and initialize the placeholders before training starts using a hook. First, let's create a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> from the placeholders:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">placeholders <span class="sy0">=</span> <span class="br0">[</span>
&nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span>data.<span class="me1">dtype</span><span class="sy0">,</span> data.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'input_image'</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span>data.<span class="me1">dtype</span><span class="sy0">,</span> data.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'groundtruth_image'</span><span class="br0">)</span>
<span class="br0">]</span>
dataset <span class="sy0">=</span> tf.<span class="me1">data</span>.<span class="me1">Dataset</span>.<span class="me1">from_tensor_slices</span><span class="br0">(</span>placeholders<span class="br0">)</span></pre></div></div>
<p>Next, we shuffle the dataset and allow retrieving data from it until the specified number of epochs has been reached:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="me1">shuffle</span><span class="br0">(</span>buffer_size<span class="sy0">=</span><span class="nu0">10000</span><span class="br0">)</span>
dataset <span class="sy0">=</span> dataset.<span class="me1">repeat</span><span class="br0">(</span>num_epochs<span class="br0">)</span></pre></div></div><br>
When creating input for evaluation or prediction, we are going to skip these two steps.
<p>Finally, we combine multiple elements into a batch and create an iterator from the dataset:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="me1">batch</span><span class="br0">(</span>batch_size<span class="br0">)</span>
&nbsp;
iterator <span class="sy0">=</span> dataset.<span class="me1">make_initializable_iterator</span><span class="br0">(</span><span class="br0">)</span>
next_example<span class="sy0">,</span> next_label <span class="sy0">=</span> iterator.<span class="me1">get_next</span><span class="br0">(</span><span class="br0">)</span></pre></div></div>
<p>To initialize the placeholders, we need to call <span class="geshifilter"><code class="text geshifilter-text">tf.Sesssion.run</code></span> with <span class="geshifilter"><code class="python geshifilter-python">feed_dict <span class="sy0">=</span> <span class="br0">{</span>placeholders<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span>: input_data<span class="sy0">,</span> placeholders<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>: groundtruth_data<span class="br0">}</span></code></span>. Since the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> will create a <span class="geshifilter"><code class="text geshifilter-text">Session</code></span> for us, we need a way to call our initialization code after the session has been created and before training begins. The Estimator's train, evaluate and predict methods accept a list of <span class="geshifilter"><code class="text geshifilter-text">SessionRunHook</code></span> subclasses as the hooks argument, which we can use to inject our code in the right place. Therefore, we first create a generic hook that runs after the session has been created:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">class</span> IteratorInitializerHook<span class="br0">(</span>tf.<span class="me1">train</span>.<span class="me1">SessionRunHook</span><span class="br0">)</span>:
&nbsp; &nbsp; <span class="st0">"""Hook to initialise data iterator after Session is created."""</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">def</span> <span class="kw4">__init__</span><span class="br0">(</span><span class="kw2">self</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span> <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">def</span> after_create_session<span class="br0">(</span><span class="kw2">self</span><span class="sy0">,</span> session<span class="sy0">,</span> coord<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="st0">"""Initialise the iterator after the session has been created."""</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">assert</span> <span class="kw2">callable</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span><span class="br0">(</span>session<span class="br0">)</span></pre></div></div>
<p>To make things a little bit nicer, we create an <span class="geshifilter"><code class="text geshifilter-text">InputFunction</code></span> class which implements the <span class="geshifilter"><code class="text geshifilter-text">__call__</code></span> method. Thus, it will behave like a function and we can pass it directly to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator.train</code></span> and related methods.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">class</span> InputFunction:
&nbsp; &nbsp; <span class="kw1">def</span> <span class="kw4">__init__</span><span class="br0">(</span><span class="kw2">self</span><span class="sy0">,</span> data<span class="sy0">,</span> batch_size<span class="sy0">,</span> num_epochs<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">data</span> <span class="sy0">=</span> data
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">batch_size</span> <span class="sy0">=</span> batch_size
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">mode</span> <span class="sy0">=</span> mode
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">num_epochs</span> <span class="sy0">=</span> num_epochs
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">init_hook</span> <span class="sy0">=</span> IteratorInitializerHook<span class="br0">(</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp;<span class="kw1">def</span> <span class="kw4">__call__</span><span class="br0">(</span><span class="kw2">self</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># Define placeholders</span>
&nbsp; &nbsp; &nbsp; &nbsp; placeholders <span class="sy0">=</span> <span class="br0">[</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">dtype</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'input_image'</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">dtype</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'reconstruct_image'</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="br0">]</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># Build dataset pipeline</span>
&nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> tf.<span class="me1">data</span>.<span class="me1">Dataset</span>.<span class="me1">from_tensor_slices</span><span class="br0">(</span>placeholders<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> <span class="kw2">self</span>.<span class="me1">mode</span> <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">shuffle</span><span class="br0">(</span>buffer_size<span class="sy0">=</span><span class="nu0">10000</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">repeat</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">num_epochs</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">batch</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">batch_size</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># create iterator from dataset</span>
&nbsp; &nbsp; &nbsp; &nbsp; iterator <span class="sy0">=</span> dataset.<span class="me1">make_initializable_iterator</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; next_example<span class="sy0">,</span> next_label <span class="sy0">=</span> iterator.<span class="me1">get_next</span><span class="br0">(</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># create initialization hook</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">def</span> _init<span class="br0">(</span>sess<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; feed_dict <span class="sy0">=</span> <span class="kw2">dict</span><span class="br0">(</span><span class="kw2">zip</span><span class="br0">(</span>placeholders<span class="sy0">,</span> <span class="br0">[</span><span class="kw2">self</span>.<span class="me1">data</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span><span class="br0">]</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sess.<span class="me1">run</span><span class="br0">(</span>iterator.<span class="me1">initializer</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;feed_dict<span class="sy0">=</span>feed_dict<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">init_hook</span>.<span class="me1">iterator_initializer_func</span> <span class="sy0">=</span> _init
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">return</span> next_example<span class="sy0">,</span> next_label</pre></div></div>
<p>Finally, we can use the <span class="geshifilter"><code class="text geshifilter-text">InputFunction</code></span> class to train our autoencoder for 30 epochs:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">from</span> tensorflow.<span class="me1">examples</span>.<span class="me1">tutorials</span>.<span class="me1">mnist</span> <span class="kw1">import</span> input_data <span class="kw1">as</span> mnist_data
&nbsp;
mnist <span class="sy0">=</span> mnist_data.<span class="me1">read_data_sets</span><span class="br0">(</span><span class="st0">'mnist_data'</span><span class="sy0">,</span> one_hot<span class="sy0">=</span><span class="kw2">False</span><span class="br0">)</span>
train_input_fn <span class="sy0">=</span> InputFunction<span class="br0">(</span>
&nbsp; &nbsp; data<span class="sy0">=</span>mnist.<span class="me1">train</span>.<span class="me1">images</span><span class="sy0">,</span>
&nbsp; &nbsp; batch_size<span class="sy0">=</span><span class="nu0">256</span><span class="sy0">,</span>
&nbsp; &nbsp; num_epochs<span class="sy0">=</span><span class="nu0">30</span><span class="sy0">,</span>
&nbsp; &nbsp; mode<span class="sy0">=</span>tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span><span class="br0">)</span>
autoencoder.<span class="me1">train</span><span class="br0">(</span>train_input_fn<span class="sy0">,</span> hooks<span class="sy0">=</span><span class="br0">[</span>train_input_fn.<span class="me1">init_hook</span><span class="br0">]</span><span class="br0">)</span></pre></div></div>
<p>The video below shows ten reconstructed images from the test data and their corresponding groundtruth after each epoch of training:<br>Your browser does not support the video tag.</p>
<h2>Denoising Autoencoder</h2>
<p><a name="denoising-autoencoder" id="denoising-autoencoder"></a>A <em>denoising autoencoder</em> is slight variation on the autoencoder described above. The only difference is that input images are randomly corrupted before they are fed to the autoencoder (we still use the original, uncorrupted image to compute the loss). This acts as a form of regularization to avoid overfitting.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">noise_factor <span class="sy0">=</span> <span class="nu0">0.5</span> &nbsp;<span class="co1"># a float in [0; 1)</span>
&nbsp;
<span class="kw1">def</span> add_noise<span class="br0">(</span>input_img<span class="sy0">,</span> groundtruth_img<span class="br0">)</span>:
&nbsp; &nbsp; noise <span class="sy0">=</span> noise_factor * tf.<span class="me1">random_normal</span><span class="br0">(</span>input_img.<span class="me1">shape</span>.<span class="me1">as_list</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
&nbsp; &nbsp; input_corrupted <span class="sy0">=</span> tf.<span class="me1">clip_by_value</span><span class="br0">(</span>tf.<span class="me1">add</span><span class="br0">(</span>input_img<span class="sy0">,</span> noise<span class="br0">)</span><span class="sy0">,</span> <span class="nu0">0</span>.<span class="sy0">,</span> <span class="nu0">1</span>.<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> input_corrupted<span class="sy0">,</span> groundtruth</pre></div></div>
<p>The function above takes two Tensors representing the input and groundtruth image, respectively, and corrupts the input image by the specified amount of noise. We can use this function to transform all of the images using Dataset's <span class="geshifilter"><code class="text geshifilter-text">map</code></span> function:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="kw2">map</span><span class="br0">(</span>add_noise<span class="sy0">,</span> num_parallel_calls<span class="sy0">=</span><span class="nu0">4</span><span class="br0">)</span>
dataset <span class="sy0">=</span> dataset.<span class="me1">prefetch</span><span class="br0">(</span><span class="nu0">512</span><span class="br0">)</span></pre></div></div><br>
The function passed to map will be part of the compute graph, thus you have to use TensorFlow operations to modify your input or use <a href="https://www.tensorflow.org/api_docs/python/tf/py_func">tf.py_func</a>. The <span class="geshifilter"><code class="text geshifilter-text">num_parallel_calls</code></span> arguments speeds up preprocessing significantly, because multiple images are transformed in parallel. The second line ensures a certain amount of corrupted images are precomputed, otherwise the transformation would only be applied when executing <span class="geshifilter"><code class="text geshifilter-text">iterator.get_next()</code></span>, which would result in a delay for each batch and bad GPU utilization. The video below shows the groundtruth, input and output of the denoising autoencoder for up to 60 epochs:<br>Your browser does not support the video tag.<p>I hope this tutorial gave you some insight on how to implement a custom TensorFlow estimator and use the Dataset API.</p>
<p><strong>Update:</strong> Have a look at the second part on <a href="https://k-d-w.org/node/107">Convolutional Autoencoders</a>.</p>
<h2>References</h2>
<ul><li><a href="https://www.tensorflow.org/extend/estimators">Creating Estimators in tf.estimator</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/datasets">Importing data</a></li>
<li><a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html"> Introduction to TensorFlow Datasets and Estimators </a></li>
<li><a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0">Higher-Level APIs in TensorFlow</a></li>
</ul></div></div></div>
</div>
<span class="field field-name-uid field-formatter-author field-type-entity-reference field-label-hidden"><span>sebp</span></span>
<span class="field field-name-created field-formatter-timestamp field-type-created field-label-hidden">Fri, 12/22/2017 - 12:39</span>
<a name="comments" id="comments"></a>
  <h1 class="begin-comments">Comments</h1><a id="comment-8589"></a>
<div class="comment__container"><h3 class="comment__title">
        <a href="https://k-d-w.org/comment/8589#comment-8589" class="permalink" rel="bookmark" hreflang="en">Getting Latent Represetations</a>        </h3>
      <div class="comment__meta">
      
      <div class="comment__submitted">
        <span class="comment__author"><span>Michael (not verified)</span></span>
        <span class="comment__pubdate">Wed, 02/07/2018 - 21:07</span>
      </div>
    </div>

    <div class="comment__content">
      <div class="clearfix text-formatted field field-comment--comment-body field-formatter-text-default field-name-comment-body field-type-text-long field-label-hidden has-single"><div class="field__items"><div class="field__item"><p>Thanks for the tutorial, more or less what I was looking for. I am left wondering how to access the latent representations in the Estimator framework though - any tips?</p>
</div></div>
</div>
</div>

  </div></div>

<p class="date">
<a href="https://k-d-w.org/node/103">by sebp at December 22, 2017 11:39 AM</a>
</p>
</div>
</div>




<div class="entrygroup" id="http://k-d-w.org/103 at http://k-d-w.org" lang="en">
<h3><a href="https://k-d-w.org/rss.xml" title="Sebastian Pölsterl's blog - Where he blogs about his latest hacking adventures.">Sebastian Pölsterl</a> — <a href="http://k-d-w.org/node/103">Denoising Autoencoder as TensorFlow estimator</a></h3>
<div class="entry">
<div class="content">
<span class="field field-name-title field-formatter-string field-type-string field-label-hidden">Denoising Autoencoder as TensorFlow estimator</span>
<div class="clearfix text-formatted field field-node--body field-formatter-text-default field-name-body field-type-text-with-summary field-label-hidden has-single"><div class="field__items"><div class="field__item"><div class="tex2jax_process"><p>I recently started to use Google's deep learning framework TensorFlow. Since version 1.3, TensorFlow includes a <a href="https://www.tensorflow.org/get_started/estimator">high-level interface</a> inspired by scikit-learn. Unfortunately, as of version 1.4, only 3 different classification and 3 different regression models implementing the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface are included. To better understand the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface, <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> API, and components in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">tf-slim</a>, I started to implement a simple Autoencoder and applied it to the well-known MNIST dataset of handwritten digits. This post is about my journey and is split in the following sections:
</p><ol><li><a href="http://k-d-w.org/rss.xml#estimators">Custom Estimators</a></li>
<li><a href="http://k-d-w.org/rss.xml#autoencoder-net">Autoencoder network architecture</a></li>
<li><a href="http://k-d-w.org/rss.xml#autoencoder-model-fn">Autoencoder as TensorFlow Estimator</a></li>
<li><a href="http://k-d-w.org/rss.xml#dataset-api">Using the Dataset API</a></li>
<li><a href="http://k-d-w.org/rss.xml#denoising-autoencoder">Denoising Autocendoer</a></li>
</ol><p>I will assume that you are familiar with TensorFlow basics. The full code is available at <a href="https://github.com/sebp/tf_autoencoder">https://github.com/sebp/tf_autoencoder</a>.</p>
<!--break--><h2>Estimators</h2>
<p><a name="estimators" id="estimators"></a>The <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">tf.estimator.Estimator</a> is at the heart TenorFlow's high-level interface and is similar to <a href="https://keras.io/models/model/">Kera's Model API</a>. It hides most of the boilerplate required to train a model: managing <span class="geshifilter"><code class="text geshifilter-text">Sessions</code></span>, writing summary statistics for TensorBoard, or saving and loading checkpoints. An <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> has three main methods: <span class="geshifilter"><code class="text geshifilter-text">train</code></span>, <span class="geshifilter"><code class="text geshifilter-text">evaluate</code></span>, and <span class="geshifilter"><code class="text geshifilter-text">predict</code></span>. Each of these methods requires a callable input function as first argument that feeds the data to the estimator (more on that later).</p>
<p><img src="estimators1.jpeg" width="350"></p>
<h2>Custom estimators</h2>
<p>You can write your own custom model implementing the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> interface by passing a function returning an instance of <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.EstimatorSpec</code></span> as first argument to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator</code></span>.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> model_fn<span class="br0">(</span>features<span class="sy0">,</span> labels<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; …
&nbsp; &nbsp; <span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; train_op<span class="sy0">=</span>train_op<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; eval_metric_ops<span class="sy0">=</span>eval_metric_ops<span class="br0">)</span></pre></div></div>
<p>The first argument – <span class="geshifilter"><code class="text geshifilter-text">mode</code></span> – is one of <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.TRAIN</code></span>, <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.EVAL</code></span> or <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.ModeKeys.PREDICT</code></span> and determines which of the remaining values must be provided.</p>
<p>In <span class="geshifilter"><code class="text geshifilter-text">TRAIN</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">loss</code></span>: A Tensor containing a scalar loss value.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">train_op</code></span>: An Op that runs one step of training. We can use the return value of <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss">tf.contrib.layers.optimize_loss</a> here.</li>
</ul><p>In <span class="geshifilter"><code class="text geshifilter-text">EVAL</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">loss</code></span>: A scalar Tensor containing the loss on the validation data.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">eval_metric_ops</code></span>: A dictionary that maps metric names to Tensors of metrics to calculate, typically, one of the <a href="https://www.tensorflow.org/api_docs/python/tf/metrics">tf.metrics</a> functions.</li>
</ul><p>In <span class="geshifilter"><code class="text geshifilter-text">PREDICT</code></span> mode:</p>
<ul><li><span class="geshifilter"><code class="text geshifilter-text">predictions</code></span>: A dictionary that maps key names of your choice to Tensors containing the predictions from the model.</li>
</ul><p>An important difference to the Estimators included with TensorFlow is that we need to call relevant <span class="geshifilter"><code class="text geshifilter-text">tf.summary</code></span> functions in <span class="geshifilter"><code class="text geshifilter-text">model_fn</code></span> ourselves. However, the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> will take care of writing summaries to disk so we can inspect them in TenorBoard.</p>
<h2>Autoencoder model</h2>
<p><a name="autoencoder-net" id="autoencoder-net"></a><img src="autoencoder_001.svg" alt="Autoencoder architecture" width="400"></p>
<p>The Autoencoder model is straightforward, it consists of two major parts: an encoder and an decoder. The encoder has an input layer (28*28 = 784 dimensions in the case of MNIST) and one or more hidden layers, decreasing in size. In the decoder, we reverse the operations of the encoder by blowing the output of the smallest hidden layer up to the size of the input (optionally, with hidden layers of increasing size in-between). The loss function computes the difference between the original image and the reconstructed image (the output of the decoder). Common loss functions are mean squared error and <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">cross-entropy</a>.</p>
<p>To construct the encoder network, we specify a list containing the number of hidden units for each layer and (optionally) add dropout layers in-between:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> encoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">for</span> num_hidden_units <span class="kw1">in</span> hidden_units:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_hidden_units<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> dropout <span class="kw1">is</span> <span class="kw1">not</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">dropout</span><span class="br0">(</span>net<span class="sy0">,</span> is_training<span class="sy0">=</span>is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; add_hidden_layer_summary<span class="br0">(</span>net<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div><br>
where <span class="geshifilter"><code class="text geshifilter-text">add_hidden_layer_summary</code></span> adds a histogram of the activations and the fraction of non-zero activations to be displayed in TensorBoard. The latter is particularly useful when debugging networks with <a href="https://cs231n.github.io/neural-networks-1/">rectified linear units (ReLU)</a>. If too many hidden units return 0 values early during optimization, the model won't be able to learn anymore, in which case one would typically try to lower the learning rate or choose a different activation function.
<p>The network of the decoder is almost identical, we just explicitly use a linear activation function (<span class="geshifilter"><code class="text geshifilter-text">activation_fn=None</code></span>) and no dropout in the last layer:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> decoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>:
&nbsp; &nbsp; net <span class="sy0">=</span> inputs
&nbsp; &nbsp; <span class="kw1">for</span> num_hidden_units <span class="kw1">in</span> hidden_units<span class="br0">[</span>:-<span class="nu0">1</span><span class="br0">]</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net<span class="sy0">,</span> num_outputs<span class="sy0">=</span>num_hidden_units<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> dropout <span class="kw1">is</span> <span class="kw1">not</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> slim.<span class="me1">dropout</span><span class="br0">(</span>net<span class="sy0">,</span> is_training<span class="sy0">=</span>is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; add_hidden_layer_summary<span class="br0">(</span>net<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; net <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">(</span>net<span class="sy0">,</span> hidden_units<span class="br0">[</span>-<span class="nu0">1</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span><span class="kw2">None</span><span class="br0">)</span>
&nbsp; &nbsp; tf.<span class="me1">summary</span>.<span class="me1">histogram</span><span class="br0">(</span><span class="st0">'activation'</span><span class="sy0">,</span> net<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div>
<p>You may have noticed that we did no specify any activation function so far. Thanks to TenorFlow's <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/framework/arg_scope">arg_scope context manager</a>, we can easily set the activation function for <em>all</em> fully connected layers. At the same time we set an appropriate weight initializer and  (optionally) use weight decay:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">def</span> autoencoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> activation_fn<span class="sy0">,</span> dropout<span class="sy0">,</span> weight_decay<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; is_training <span class="sy0">=</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>
&nbsp;
&nbsp; &nbsp; weights_init <span class="sy0">=</span> slim.<span class="me1">initializers</span>.<span class="me1">variance_scaling_initializer</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">if</span> weight_decay <span class="kw1">is</span> <span class="kw2">None</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp; &nbsp; <span class="kw1">else</span>:
&nbsp; &nbsp; &nbsp; &nbsp; weights_reg <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">l2_regularizer</span><span class="br0">(</span>weight_decay<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">with</span> slim.<span class="me1">arg_scope</span><span class="br0">(</span><span class="br0">[</span>tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">fully_connected</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_initializer<span class="sy0">=</span>weights_init<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weights_regularizer<span class="sy0">=</span>weights_reg<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activation_fn<span class="sy0">=</span>activation_fn<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> encoder<span class="br0">(</span>inputs<span class="sy0">,</span> hidden_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; n_features <span class="sy0">=</span> inputs.<span class="me1">shape</span><span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>.<span class="me1">value</span>
&nbsp; &nbsp; &nbsp; &nbsp; decoder_units <span class="sy0">=</span> hidden_units<span class="br0">[</span>:-<span class="nu0">1</span><span class="br0">]</span><span class="br0">[</span>::-<span class="nu0">1</span><span class="br0">]</span> + <span class="br0">[</span>n_features<span class="br0">]</span>
&nbsp; &nbsp; &nbsp; &nbsp; net <span class="sy0">=</span> decoder<span class="br0">(</span>net<span class="sy0">,</span> decoder_units<span class="sy0">,</span> dropout<span class="sy0">,</span> is_training<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> net</pre></div></div><br>
where <span class="geshifilter"><code class="text geshifilter-text">slim.initializers.variance_scaling_initializer</code></span> corresponds to the <a href="https://arxiv.org/abs/1502.01852">initialization of He et al.</a>, which is the <a href="https://cs231n.github.io/neural-networks-2/#init">current recommendation</a> for networks with ReLU activations.
<p>This concludes the architecture of the autoencoder. Next, we need to implement the <span class="geshifilter"><code class="text geshifilter-text">model_fn</code></span> function passed to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator</code></span> as outlined above.</p>
<h2>Autoencoder model_fn</h2>
<p><a name="autoencoder-model-fn" id="autoencoder-model-fn"></a>First, we construct the network's architecture using the <span class="geshifilter"><code class="text geshifilter-text">autoencoder</code></span> function described above:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">logits <span class="sy0">=</span> autoencoder<span class="br0">(</span>inputs<span class="sy0">=</span>features<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_units<span class="sy0">=</span>hidden_units<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;activation_fn<span class="sy0">=</span>activation_fn<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dropout<span class="sy0">=</span>dropout<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;weight_decay<span class="sy0">=</span>weight_decay<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mode<span class="sy0">=</span>mode<span class="br0">)</span></pre></div></div>
<p>Subsequent steps depend on the value of <span class="geshifilter"><code class="text geshifilter-text">mode</code></span>. In prediction mode, we merely have to return the reconstructed image, therefore we make sure all values are within the interval [0; 1] by applying the sigmoid function:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">probs <span class="sy0">=</span> tf.<span class="me1">nn</span>.<span class="me1">sigmoid</span><span class="br0">(</span>logits<span class="br0">)</span>
predictions <span class="sy0">=</span> <span class="br0">{</span><span class="st0">"prediction"</span>: probs<span class="br0">}</span>
<span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">PREDICT</span>:
&nbsp; &nbsp; <span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="br0">)</span></pre></div></div>
<p>In training and evaluation mode, we need to compute the loss, which is cross-entropy in this example:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">tf.<span class="me1">losses</span>.<span class="me1">sigmoid_cross_entropy</span><span class="br0">(</span>labels<span class="sy0">,</span> logits<span class="br0">)</span>
total_loss <span class="sy0">=</span> tf.<span class="me1">losses</span>.<span class="me1">get_total_loss</span><span class="br0">(</span>add_regularization_losses<span class="sy0">=</span>is_training<span class="br0">)</span></pre></div></div><br>
The second line is needed to add the $\ell_2$-losses used in weight decay.
<p>Most importantly, training relies on choosing an optimizer, here we use <a href="http://arxiv.org/abs/1412.6980">Adam</a> and an exponential learning rate decay. The latter dynamically updates the learning rate during training according to the formula<br>
$$<br>
\text{decayed learning rate} = \text{base learning rate} \cdot 0.96^{\lfloor i / 1000 \rfloor} ,<br>
$$ where $i$ is the current iteration. It would probably work as well without learning rate decay, but I included it for the sake of completeness.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>:
&nbsp; &nbsp; train_op <span class="sy0">=</span> tf.<span class="me1">contrib</span>.<span class="me1">layers</span>.<span class="me1">optimize_loss</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; optimizer<span class="sy0">=</span><span class="st0">"Adam"</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; learning_rate<span class="sy0">=</span>learning_rate<span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; learning_rate_decay_fn<span class="sy0">=</span><span class="kw1">lambda</span> lr<span class="sy0">,</span> gs: tf.<span class="me1">train</span>.<span class="me1">exponential_decay</span><span class="br0">(</span>lr<span class="sy0">,</span> gs<span class="sy0">,</span> <span class="nu0">1000</span><span class="sy0">,</span> <span class="nu0">0.96</span><span class="sy0">,</span> staircase<span class="sy0">=</span><span class="kw2">True</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; global_step<span class="sy0">=</span>tf.<span class="me1">train</span>.<span class="me1">get_global_step</span><span class="br0">(</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; summaries<span class="sy0">=</span><span class="br0">[</span><span class="st0">"learning_rate"</span><span class="sy0">,</span> <span class="st0">"global_gradient_norm"</span><span class="br0">]</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; <span class="co1"># Add histograms for trainable variables</span>
&nbsp; &nbsp; <span class="kw1">for</span> var <span class="kw1">in</span> tf.<span class="me1">trainable_variables</span><span class="br0">(</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">summary</span>.<span class="me1">histogram</span><span class="br0">(</span>var.<span class="me1">op</span>.<span class="me1">name</span><span class="sy0">,</span> var<span class="br0">)</span></pre></div></div><br>
Note that we add a histogram of all trainable variables for TensorBoard in the last part.
<p>Finally, we compute the root mean squared error when in evaluation mode:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">if</span> mode <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">EVAL</span>:
&nbsp; &nbsp; eval_metric_ops <span class="sy0">=</span> <span class="br0">{</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="st0">"rmse"</span>: tf.<span class="me1">metrics</span>.<span class="me1">root_mean_squared_error</span><span class="br0">(</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">cast</span><span class="br0">(</span>labels<span class="sy0">,</span> tf.<span class="me1">float64</span><span class="br0">)</span><span class="sy0">,</span> tf.<span class="me1">cast</span><span class="br0">(</span>probs<span class="sy0">,</span> tf.<span class="me1">float64</span><span class="br0">)</span><span class="br0">)</span>
&nbsp; &nbsp; <span class="br0">}</span></pre></div></div><br>
and return the specification of our autoencoder estimator:<br><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">return</span> tf.<span class="me1">estimator</span>.<span class="me1">EstimatorSpec</span><span class="br0">(</span>
&nbsp; &nbsp; mode<span class="sy0">=</span>mode<span class="sy0">,</span>
&nbsp; &nbsp; predictions<span class="sy0">=</span>predictions<span class="sy0">,</span>
&nbsp; &nbsp; loss<span class="sy0">=</span>total_loss<span class="sy0">,</span>
&nbsp; &nbsp; train_op<span class="sy0">=</span>train_op<span class="sy0">,</span>
&nbsp; &nbsp; eval_metric_ops<span class="sy0">=</span>eval_metric_ops<span class="br0">)</span></pre></div></div><br>
&nbsp;
<h2>Feeding data to an Estimator via the Dataset API</h2>
<p><a name="dataset-api" id="dataset-api"></a>Once we constructed our estimator, e.g. via<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">estimator <span class="sy0">=</span> AutoEncoder<span class="br0">(</span>hidden_units<span class="sy0">=</span><span class="br0">[</span><span class="nu0">128</span><span class="sy0">,</span> <span class="nu0">64</span><span class="sy0">,</span> <span class="nu0">32</span><span class="br0">]</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropout<span class="sy0">=</span><span class="kw2">None</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weight_decay<span class="sy0">=</span><span class="nu0">1e-5</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; learning_rate<span class="sy0">=</span><span class="nu0">0.001</span><span class="br0">)</span></pre></div></div><br>
we would like to train it by calling <span class="geshifilter"><code class="text geshifilter-text">train</code></span>, which expects a callable that returns two tensors, one representing the input data and one the groundtruth data. The easiest way would be to use <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn">tf.estimator.inputs.numpy_input_fn</a>, but instead I want to introduce TensorFlow's Dataset API, which is more generic.
<p>The Dataset API comprises two elements:</p>
<ol><li><span class="geshifilter"><code class="text geshifilter-text">tf.data.Dataset</code></span> represents a dataset and any transformations applied to it.</li>
<li><span class="geshifilter"><code class="text geshifilter-text">tf.data.Iterator</code></span> is used to extract elements from a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span>. In particular, <span class="geshifilter"><code class="text geshifilter-text">Iterator.get_next()</code></span> returns the next element of a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> and typically is what is fed to an estimator.</li>
</ol><p>Here, I'm using what is called an <em>initializable</em> Iterator, inspired by <a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0">this post</a>. We define one placeholder for the input image and one for the groundtruth image and initialize the placeholders before training starts using a hook. First, let's create a <span class="geshifilter"><code class="text geshifilter-text">Dataset</code></span> from the placeholders:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">placeholders <span class="sy0">=</span> <span class="br0">[</span>
&nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span>data.<span class="me1">dtype</span><span class="sy0">,</span> data.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'input_image'</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span>data.<span class="me1">dtype</span><span class="sy0">,</span> data.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'groundtruth_image'</span><span class="br0">)</span>
<span class="br0">]</span>
dataset <span class="sy0">=</span> tf.<span class="me1">data</span>.<span class="me1">Dataset</span>.<span class="me1">from_tensor_slices</span><span class="br0">(</span>placeholders<span class="br0">)</span></pre></div></div>
<p>Next, we shuffle the dataset and allow retrieving data from it until the specified number of epochs has been reached:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="me1">shuffle</span><span class="br0">(</span>buffer_size<span class="sy0">=</span><span class="nu0">10000</span><span class="br0">)</span>
dataset <span class="sy0">=</span> dataset.<span class="me1">repeat</span><span class="br0">(</span>num_epochs<span class="br0">)</span></pre></div></div><br>
When creating input for evaluation or prediction, we are going to skip these two steps.
<p>Finally, we combine multiple elements into a batch and create an iterator from the dataset:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="me1">batch</span><span class="br0">(</span>batch_size<span class="br0">)</span>
&nbsp;
iterator <span class="sy0">=</span> dataset.<span class="me1">make_initializable_iterator</span><span class="br0">(</span><span class="br0">)</span>
next_example<span class="sy0">,</span> next_label <span class="sy0">=</span> iterator.<span class="me1">get_next</span><span class="br0">(</span><span class="br0">)</span></pre></div></div>
<p>To initialize the placeholders, we need to call <span class="geshifilter"><code class="text geshifilter-text">tf.Sesssion.run</code></span> with <span class="geshifilter"><code class="python geshifilter-python">feed_dict <span class="sy0">=</span> <span class="br0">{</span>placeholders<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span>: input_data<span class="sy0">,</span> placeholders<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>: groundtruth_data<span class="br0">}</span></code></span>. Since the <span class="geshifilter"><code class="text geshifilter-text">Estimator</code></span> will create a <span class="geshifilter"><code class="text geshifilter-text">Session</code></span> for us, we need a way to call our initialization code after the session has been created and before training begins. The Estimator's train, evaluate and predict methods accept a list of <span class="geshifilter"><code class="text geshifilter-text">SessionRunHook</code></span> subclasses as the hooks argument, which we can use to inject our code in the right place. Therefore, we first create a generic hook that runs after the session has been created:</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">class</span> IteratorInitializerHook<span class="br0">(</span>tf.<span class="me1">train</span>.<span class="me1">SessionRunHook</span><span class="br0">)</span>:
&nbsp; &nbsp; <span class="st0">"""Hook to initialise data iterator after Session is created."""</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">def</span> <span class="kw4">__init__</span><span class="br0">(</span><span class="kw2">self</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span> <span class="sy0">=</span> <span class="kw2">None</span>
&nbsp;
&nbsp; &nbsp; <span class="kw1">def</span> after_create_session<span class="br0">(</span><span class="kw2">self</span><span class="sy0">,</span> session<span class="sy0">,</span> coord<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="st0">"""Initialise the iterator after the session has been created."""</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">assert</span> <span class="kw2">callable</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">iterator_initializer_func</span><span class="br0">(</span>session<span class="br0">)</span></pre></div></div>
<p>To make things a little bit nicer, we create an <span class="geshifilter"><code class="text geshifilter-text">InputFunction</code></span> class which implements the <span class="geshifilter"><code class="text geshifilter-text">__call__</code></span> method. Thus, it will behave like a function and we can pass it directly to <span class="geshifilter"><code class="text geshifilter-text">tf.estimator.Estimator.train</code></span> and related methods.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">class</span> InputFunction:
&nbsp; &nbsp; <span class="kw1">def</span> <span class="kw4">__init__</span><span class="br0">(</span><span class="kw2">self</span><span class="sy0">,</span> data<span class="sy0">,</span> batch_size<span class="sy0">,</span> num_epochs<span class="sy0">,</span> mode<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">data</span> <span class="sy0">=</span> data
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">batch_size</span> <span class="sy0">=</span> batch_size
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">mode</span> <span class="sy0">=</span> mode
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">num_epochs</span> <span class="sy0">=</span> num_epochs
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">init_hook</span> <span class="sy0">=</span> IteratorInitializerHook<span class="br0">(</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp;<span class="kw1">def</span> <span class="kw4">__call__</span><span class="br0">(</span><span class="kw2">self</span><span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># Define placeholders</span>
&nbsp; &nbsp; &nbsp; &nbsp; placeholders <span class="sy0">=</span> <span class="br0">[</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">dtype</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'input_image'</span><span class="br0">)</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tf.<span class="me1">placeholder</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">dtype</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span>.<span class="me1">shape</span><span class="sy0">,</span> name<span class="sy0">=</span><span class="st0">'reconstruct_image'</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="br0">]</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># Build dataset pipeline</span>
&nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> tf.<span class="me1">data</span>.<span class="me1">Dataset</span>.<span class="me1">from_tensor_slices</span><span class="br0">(</span>placeholders<span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">if</span> <span class="kw2">self</span>.<span class="me1">mode</span> <span class="sy0">==</span> tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">shuffle</span><span class="br0">(</span>buffer_size<span class="sy0">=</span><span class="nu0">10000</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">repeat</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">num_epochs</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; dataset <span class="sy0">=</span> dataset.<span class="me1">batch</span><span class="br0">(</span><span class="kw2">self</span>.<span class="me1">batch_size</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># create iterator from dataset</span>
&nbsp; &nbsp; &nbsp; &nbsp; iterator <span class="sy0">=</span> dataset.<span class="me1">make_initializable_iterator</span><span class="br0">(</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; next_example<span class="sy0">,</span> next_label <span class="sy0">=</span> iterator.<span class="me1">get_next</span><span class="br0">(</span><span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="co1"># create initialization hook</span>
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">def</span> _init<span class="br0">(</span>sess<span class="br0">)</span>:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; feed_dict <span class="sy0">=</span> <span class="kw2">dict</span><span class="br0">(</span><span class="kw2">zip</span><span class="br0">(</span>placeholders<span class="sy0">,</span> <span class="br0">[</span><span class="kw2">self</span>.<span class="me1">data</span><span class="sy0">,</span> <span class="kw2">self</span>.<span class="me1">data</span><span class="br0">]</span><span class="br0">)</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sess.<span class="me1">run</span><span class="br0">(</span>iterator.<span class="me1">initializer</span><span class="sy0">,</span>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;feed_dict<span class="sy0">=</span>feed_dict<span class="br0">)</span>
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw2">self</span>.<span class="me1">init_hook</span>.<span class="me1">iterator_initializer_func</span> <span class="sy0">=</span> _init
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="kw1">return</span> next_example<span class="sy0">,</span> next_label</pre></div></div>
<p>Finally, we can use the <span class="geshifilter"><code class="text geshifilter-text">InputFunction</code></span> class to train our autoencoder for 30 epochs:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1"><span class="kw1">from</span> tensorflow.<span class="me1">examples</span>.<span class="me1">tutorials</span>.<span class="me1">mnist</span> <span class="kw1">import</span> input_data <span class="kw1">as</span> mnist_data
&nbsp;
mnist <span class="sy0">=</span> mnist_data.<span class="me1">read_data_sets</span><span class="br0">(</span><span class="st0">'mnist_data'</span><span class="sy0">,</span> one_hot<span class="sy0">=</span><span class="kw2">False</span><span class="br0">)</span>
train_input_fn <span class="sy0">=</span> InputFunction<span class="br0">(</span>
&nbsp; &nbsp; data<span class="sy0">=</span>mnist.<span class="me1">train</span>.<span class="me1">images</span><span class="sy0">,</span>
&nbsp; &nbsp; batch_size<span class="sy0">=</span><span class="nu0">256</span><span class="sy0">,</span>
&nbsp; &nbsp; num_epochs<span class="sy0">=</span><span class="nu0">30</span><span class="sy0">,</span>
&nbsp; &nbsp; mode<span class="sy0">=</span>tf.<span class="me1">estimator</span>.<span class="me1">ModeKeys</span>.<span class="me1">TRAIN</span><span class="br0">)</span>
autoencoder.<span class="me1">train</span><span class="br0">(</span>train_input_fn<span class="sy0">,</span> hooks<span class="sy0">=</span><span class="br0">[</span>train_input_fn.<span class="me1">init_hook</span><span class="br0">]</span><span class="br0">)</span></pre></div></div>
<p>The video below shows ten reconstructed images from the test data and their corresponding groundtruth after each epoch of training:<br>Your browser does not support the video tag.</p>
<h2>Denoising Autoencoder</h2>
<p><a name="denoising-autoencoder" id="denoising-autoencoder"></a>A <em>denoising autoencoder</em> is slight variation on the autoencoder described above. The only difference is that input images are randomly corrupted before they are fed to the autoencoder (we still use the original, uncorrupted image to compute the loss). This acts as a form of regularization to avoid overfitting.</p>
<p></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">noise_factor <span class="sy0">=</span> <span class="nu0">0.5</span> &nbsp;<span class="co1"># a float in [0; 1)</span>
&nbsp;
<span class="kw1">def</span> add_noise<span class="br0">(</span>input_img<span class="sy0">,</span> groundtruth_img<span class="br0">)</span>:
&nbsp; &nbsp; noise <span class="sy0">=</span> noise_factor * tf.<span class="me1">random_normal</span><span class="br0">(</span>input_img.<span class="me1">shape</span>.<span class="me1">as_list</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
&nbsp; &nbsp; input_corrupted <span class="sy0">=</span> tf.<span class="me1">clip_by_value</span><span class="br0">(</span>tf.<span class="me1">add</span><span class="br0">(</span>input_img<span class="sy0">,</span> noise<span class="br0">)</span><span class="sy0">,</span> <span class="nu0">0</span>.<span class="sy0">,</span> <span class="nu0">1</span>.<span class="br0">)</span>
&nbsp; &nbsp; <span class="kw1">return</span> input_corrupted<span class="sy0">,</span> groundtruth</pre></div></div>
<p>The function above takes two Tensors representing the input and groundtruth image, respectively, and corrupts the input image by the specified amount of noise. We can use this function to transform all of the images using Dataset's <span class="geshifilter"><code class="text geshifilter-text">map</code></span> function:<br></p><div class="geshifilter"><div class="python geshifilter-python"><pre class="de1">dataset <span class="sy0">=</span> dataset.<span class="kw2">map</span><span class="br0">(</span>add_noise<span class="sy0">,</span> num_parallel_calls<span class="sy0">=</span><span class="nu0">4</span><span class="br0">)</span>
dataset <span class="sy0">=</span> dataset.<span class="me1">prefetch</span><span class="br0">(</span><span class="nu0">512</span><span class="br0">)</span></pre></div></div><br>
The function passed to map will be part of the compute graph, thus you have to use TensorFlow operations to modify your input or use <a href="https://www.tensorflow.org/api_docs/python/tf/py_func">tf.py_func</a>. The <span class="geshifilter"><code class="text geshifilter-text">num_parallel_calls</code></span> arguments speeds up preprocessing significantly, because multiple images are transformed in parallel. The second line ensures a certain amount of corrupted images are precomputed, otherwise the transformation would only be applied when executing <span class="geshifilter"><code class="text geshifilter-text">iterator.get_next()</code></span>, which would result in a delay for each batch and bad GPU utilization. The video below shows the groundtruth, input and output of the denoising autoencoder for up to 60 epochs:<br>Your browser does not support the video tag.<p>I hope this tutorial gave you some insight on how to implement a custom TensorFlow estimator and use the Dataset API.</p>
<h2>References</h2>
<ul><li><a href="https://www.tensorflow.org/extend/estimators">Creating Estimators in tf.estimator</a></li>
<li><a href="https://www.tensorflow.org/programmers_guide/datasets">Importing data</a></li>
<li><a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html"> Introduction to TensorFlow Datasets and Estimators </a></li>
<li><a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0">Higher-Level APIs in TensorFlow</a></li>
</ul></div></div></div>
</div>
<span class="field field-name-uid field-formatter-author field-type-entity-reference field-label-hidden"><span>sebp</span></span>
<span class="field field-name-created field-formatter-timestamp field-type-created field-label-hidden">Fri, 12/22/2017 - 12:39</span>
<a name="comments" id="comments"></a>
  <h1 class="begin-comments">Comments</h1></div>

<p class="date">
<a href="http://k-d-w.org/node/103">by sebp at December 22, 2017 11:39 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 19, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2780" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> — <a href="https://blogs.gnome.org/uraeus/2017/12/19/why-hasnt-the-year-of-the-linux-desktop-happened-yet/">Why hasn’t The Year of the Linux Desktop happened yet?</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>Having spent 20 years of my life on Desktop Linux I thought I should write up my thinking about why we so far hasn’t had the Linux on the Desktop breakthrough and maybe more importantly talk about the avenues I see for that breakthrough still happening. There has been a lot written of this over the years, with different people coming up with their explanations. My thesis is that there really isn’t one reason, but rather a range of issues that all have contributed to holding the Linux Desktop back from reaching a bigger market. Also to put this into context, success here in my mind would be having something like 10% market share of desktop systems, that to me means we reached critical mass. So let me start by listing some of the main reasons I see for why we are not at that 10% mark today before going onto talking about how I think that goal might possible to reach going forward.</p>
<p><b>Things that have held us back</b></p>
<ul>
<li>Fragmented market</li>
<p>One of the most common explanations for why the Linux Desktop never caught on more is the fragmented state of the Linux Desktop space. We got a large host of desktop projects like GNOME, KDE, Enlightenment, Cinnamon etc. and a even larger host of distributions shipping these desktops. I used to think this state should get a lot of the blame, and I still believe it owns some of the blame, but I have also come to conclude in recent years that it is probably more of a symptom than a cause. If someone had come up with a model strong enough to let Desktop Linux break out of its current technical user niche then I am now convinced that model would easily have also been strong enough to leave the Linux desktop fragmentation behind for all practical purposes. Because at that point the alternative desktops for Linux would be as important as the alternative MS Windows shells are. So in summary, the fragmentation hasn’t helped for sure and is still not helpful, but it is probably a problem that has been overstated.</p>
<li>Lack of special applications</li>
<p>Another common item that has been pointed to is the lack of applications. We know that for sure in the early days of Desktop Linux the challenge you always had when trying to convince anyone of moving to Desktop Linux was that they almost invariably had one or more application they relied on that was only available on Windows. I remember in one of my first jobs after University when I worked as a sysadmin we had a long list of these applications that various parts of the organization relied on, be that special tools to interface with a supplier, with the bank, dealing with nutritional values of food in the company cafeteria etc. This is a problem that has been in rapid decline for the last 5-10 years due to the move to web applications, but I am sure that in a given major organization you can still probably find a few of them. But between the move to the web and Wine I don’t think this is a major issue anymore. So in summary this was a major roadblock in the early years, but is a lot less of an impediment these days.
</p>
<li>Lack of big name applications</li>
<p>
Adopting a new platform is always easier if you can take the applications you are familiar with you. So the lack of things like MS Office and Adobe Photoshop would always contribute to making a switch less likely. Just because in addition to switching OS you would also have to learn to use new tools. And of course along those lines there where always the challenge of file format compatibility, in the early days in a hard sense that you simply couldn’t reliably load documents coming from some of these applications, to more recently softer problems like lack of metrically identical fonts. The font for example issue has been mostly resolved due to Google releasing fonts metrically compatible with MS default fonts a few years ago, but it was definitely a hindrance for adoption for many years. The move to web for a lot of these things has greatly reduced this problem too, with organizations adopting things like Google Docs at rapid pace these days. So in summary, once again something that used to be a big problem, but which is at least a lot less of a problem these days, but of course there are still apps not available for Linux that does stop people from adopting desktop linux.
</p>
<li>Lack of API and ABI stability</li>
<p>This is another item that many people have brought up over the years. I think I have personally vacillated over the importance of this one multiple times over the years. Changing APIs are definitely not a fun thing for developers to deal with, it adds extra work often without bringing direct benefit to their application. Linux packaging philosophy probably magnified this problem for developers with anything that could be split out and packaged separately was, meaning that every application was always living on top of a lot of moving parts. That said the reason I am sceptical to putting to much blame onto this is that you could always find stable subsets to rely on. So for instance if you targeted GTK2 or Qt back in the day and kept away from some of the more fast moving stuff offered by GNOME and KDE you would not be hit with this that often. And of course if the Linux Desktop market share had been higher then people would have been prepared to deal with these challenges regardless, just like they are on other platforms that keep changing and evolving quickly like the mobile operating systems.</p>
<li>Apple resurgence</li>
<p>This might of course be the result of subjective memory, but one of the times where it felt like there could have been a Linux desktop breakthrough was at the same time as Linux on the server started making serious inroads. The old Unix workstation market was coming apart and moving to Linux already, the worry of a Microsoft monopoly was at its peak and Apple was in what seemed like mortal decline. There was a lot of media buzz around the Linux desktop and VC funded companies was set up to try to build a business around it. Reaching some kind of critical mass seemed like it could be within striking distance. Of course what happened here was that Steve Jobs returned to Apple and we suddenly had MacOSX come onto the scene taking at least some air out of the Linux Desktop space. The importance of this one I do find exceptionally hard to quantify though, part of me feels it had a lot of impact, but on the other hand it isn’t 100% clear to me that the market and the players at the time would have been able to capitalize even if Apple had gone belly-up.</p>
<li>Microsoft aggressive response</li>
<p>In the first 10 years of Desktop linux there was no doubt that Microsoft was working hard to try to nip any sign of Desktop Linux gaining any kind of foothold or momentum. I do remember for instance that Novell for quite some time was trying to establish a serious Desktop Linux business after having bought Miguel de Icaza’s company Helix Code. However it seemed like a pattern quickly emerged that every time Novell or anyone else tried to announce a major Linux desktop deal, Microsoft came running in offering next to free Windows licensing to get people to stay put. Looking at Linux migrations even seemed like it became a goto policy for negotiating better prices from Microsoft. So anyone wanting to attack the desktop market with Linux would have to contend with not only market inertia, but a general depression of the price of a desktop operating systems, and knowing that Microsoft would respond to any attempt to build momentum around Linux desktop deals with very aggressive sales efforts. So in summary, this probably played an important part as it meant that the pay per copy/subscription business model that for instance Red Hat built their server business around became really though to make work in the desktop space. Because the price point ended up so low it required gigantic volumes to become profitable, which of course is a hard thing to quickly achieve when fighting against an entrenched market leader. So in summary Microsoft in some sense successfully fended of Linux breaking through as a competitor although it could be said they did so at the cost of fatally wounding the per copy fee business model they built their company around and ensured that the next wave of competitors Microsoft had to deal with like iOS and Android based themselves on business models where the cost of the OS was assumed to be zero, thus contributing to the Windows Phone efforts being doomed.
</p>
<li>Piracy</li>
<p>One of the big aspirations of the Linux community from the early days was the idea that a open source operating system would enable more people to be able to afford running a computer and thus take part in the economic opportunities that the digital era would provide. For the desktop space there was always this idea that while Microsoft was entrenched in North America and Europe there was this ocean of people in the rest of the world that had never used a computer before and thus would be more open to adopting a desktop linux system. I think this so far panned out only in a limited degree, where running a Linux distribution has surely opened job and financial opportunities for a lot of people, yet when you look at things from a volume perspective most of these potential Linux users found that a pirated Windows copy suited their needs just as much or more. As an anecdote here, there was recently a bit of noise and writing around the sudden influx of people on Steam playing Player Unknown: Battlegrounds, as it caused the relatively Linux marketshare to decline. So most of these people turned out to be running Windows in Mandarin language. Studies have found that about 70% of all software in China is unlicensed so I don’t think I am going to far out on a limb here assuming that most of these gamers are not providing Microsoft with Windows licensing revenue, but it does illustrate the challenge of getting these people onto Linux as they already are getting an operating system for free. So in summary, in addition to facing cut throat pricing from Microsoft in the business sector one had to overcome the basically free price of pirated software in the consumer sector.</p>
<li>Red Hat mostly stayed away</li>
<p>So few people probably don’t remember or know this, but Red Hat was actually founded as a desktop Linux company. The first major investment in software development that Red Hat ever did was setting up the Red Hat Advanced Development Labs, hiring a bunch of core GNOME developers to move that effort forward. But when Red Hat pivoted to the server with the introduction of Red Hat Enterprise Linux the desktop quickly started playing second fiddle. And before I proceed, all these events where many years before I joined the company, so just as with my other points here, read this as an analysis of someone without first hand knowledge. So while Red Hat has always offered a desktop product and have always been a major contributor to keeping the Linux desktop ecosystem viable, Red Hat was focused on the server side solutions and the desktop offering was always aimed more narrowly things like technical workstation customers and people developing towards the RHEL server. It is hard to say how big an impact Red Hats decision to not go after this market has had, on one side it would probably have been beneficial to have the Linux company with the deepest pockets and the strongest brand be a more active participant, but on the other hand staying mostly out of the fight gave other companies a bigger room to give it a go.</p>
<li>Canonical business model not working out</li>
<p>This bullet point is probably going to be somewhat controversial considering I work for Red Hat (although this is my private blog my with own personal opinions), but on the other hand I feel one can not talk about the trajectory of the Linux Desktop over the last decade without mentioning Canonical and Ubuntu. So I have to assume that when Mark Shuttleworth was mulling over doing Ubuntu he probably saw a lot of the challenges that I mention above, especially the revenue generation challenges that the competition from Microsoft provided. So in the end he decided on the standard internet business model of the time, which was to try to quickly build up a huge userbase and then dealing with how to monetize it later on. So Ubuntu was launched with an effective price point of zero, in fact you could even get install media sent to you for free. The effort worked in the sense that Ubuntu quickly became the biggest player in the Linux desktop space and it certainly helped the Linux desktop marketshare grow in the early years. Unfortunately I think it still basically failed, and the reason I am saying that is that it didn’t manage to grow big enough to provide Ubuntu with enough revenue through their appstore or their partner agreements to allow them to seriously re-invest in the Linux Desktop and invest in the kind of marketing effort needed to take Linux to a less super technical audience. So once it plateaued what they had was enough revenue to keep what is a relatively barebones engineering effort going, but not the kind of income that would allow them to steadily build the Linux Desktop market further. Mark then tried to capitalize on the mindshare and market share he had managed to build, by branching out into efforts like their TV and Phone efforts, but all those efforts eventually failed.<br>
It would probably be an article in itself to deeply discuss why the grow userbase strategy failed here vs why for instance Android succeeded with this model, but I think the short version goes back to the fact that you had an entrenched market leader and the Linux Desktop isn’t different enough from a Mac or Windows desktops to drive the type of market change the transition from feature phones to smartphones was.<br>
And to be clear I am not criticizing Mark here for the strategy he choose, if I where in his shoes back when he started Ubuntu I am not sure I would have been able to come up a different strategy that would have been plausible to succeed from his starting point. That said it did contribute to even further push the expected price of desktop Linux down and thus making it even harder for people to generate significant revenue from desktop linux. On the other hand one can argue that this would likely have happened anyway due to competitive pressure and Windows piracy. Canonicals recent focus pivot away from the desktop towards trying to build a business in the server and IoT space is in some sense a natural consequence of hitting the desktop growth plateau and not having enough revenue to invest in further growth.<br>
So in summary, what was once seen as the most likely contender to take the Linux Desktop to critical mass turned out to have taken off with to little rocket fuel and eventually gravity caught up with them. And what we can never know for sure is if they during this run sucked so much air out of the market that it kept someone who could have taken us further with a different business model from jumping in.</p>
<li>Original device manufacturer support</li>
<p>THis one is a bit of a chicken and egg issue. Yes, lack of (perfect) hardware support has for sure kept Linux back on the Desktop, but lack of marketshare has also kept hardware support back. As with any system this is a question of reaching critical mass despite your challenges and thus eventually being so big that nobody can afford ignoring you. This is an area where we even today are still not fully there yet, but which I do feel we are getting closer all the time. When I installed Linux for the very first time, which I think was Red Hat Linux 3.1 (pre RHEL days) I spent about a weekend fiddling just to get my sound card working. I think I had to grab a experimental driver from somewhere and compile it myself. These days I mostly expect everything to work out of the box except more unique hardware like ambient light sensors or fingerprint readers, but even such devices are starting to land, and thanks to efforts from vendors such as Dell things are looking pretty good here. But the memory of these issues is long so a lot of people, especially those not using Linux themselves, but have heard about Linux, still assume hardware support is a very much hit or miss issue still.
</p>
</ul>
<h1>What does the future hold?</h1>
<p>So any who has read my blog posts probably know I am an optimist by nature. This isn’t just some kind of genetic disposition towards optimism, but also a philosophical belief that optimism breeds opportunity while pessimism breeds failure. So just because we haven’t gotten the Linux Desktop to 10% marketshare so far doesn’t mean it will not happen going forward. It just means we haven’t achieved it so far. One of the key identifies of open source is that it is incredibly hard to kill, because unlike proprietary software, just because a company goes out of business or decides to shut down a part of its business, the software doesn’t go away or stop getting developed. As long as there is a strong community interested in pushing it forward it remains and evolves and thus when opportunity comes knocking again it is ready to try again. And that is definitely true of Desktop Linux which from a technical perspective is better than it has ever been, the level of polish is higher than ever before, the level of hardware support is better than ever before and the range of software available is better than ever before.</p>
<p>
And the important thing to remember here is that we don’t exist in a vacuum, the world around us constantly change too, which means that the things that blocked us in the past or the companies that blocked us in the past might no be around or able to block us tomorrow. Apple and Microsoft are very different companies today than they where 10 or 20 years ago and their focus and who they compete with are very different. The dynamics of the desktop software market is changing with new technologies and paradigms all the time. Like how online media consumption has moved from things like your laptop to phones and tablets for instance. 5 years ago I would have considered iTunes a big competitive problem, today the move to streaming services like Spotify, Hulu, Amazon or Netflix has made iTunes feel archaic and a symbol of bygone times.
</p>
<p>
And many of the problems we faced before, like weird Windows applications without a Linux counterpart has been washed away by the switch to browser based applications. And while Valve’s SteamOS effort didn’t taken off, it has provided Linux users with access to a huge catalog of games, removing a reason that I know caused a few of my friends to mostly abandon using Linux on their computers. And you can actually as a consumer buy linux from a range of vendors now, who try to properly support Linux on their hardware. And this includes a major player like Dell and smaller outfits like System76 and Purism.</p>
<p>
And since I do work for Red Hat managing our Desktop Engineering team I should address the question of if Red Hat will be a major driver in taking Desktop linux to that 10%? Well Red Hat will continue to support end evolve our current RHEL Workstation product, and we are seeing a steady growth of new customers for it. So if you are looking for a solid developer workstation for your company you should absolutely talk to Red Hat sales about RHEL Workstation, but Red Hat is not looking at aggressively targeting general consumer computers anytime soon. Caveat here, I am not a C-level executive at Red Hat, so I guess there is always a chance Jim Whitehurst or someone else in the top brass is mulling over a gigantic new desktop effort and I simply don’t know about it, but I don’t think it is likely and thus would not advice anyone to hold their breath waiting for such a thing to be announced :). That said Red Hat like any company out there do react to market opportunities as they arise, so who knows what will happen down the road. And we will definitely keep pushing Fedora Workstation forward as the place to experience the leading edge of the Desktop Linux experience and a great portal into the world of Linux on servers and in the cloud.</p>
<p>So to summarize; there are a lot of things happening in the market that could provide the right set of people the opportunity they need to finally take Linux to critical mass. Whether there is anyone who has the timing and skills to pull it off is of course always an open question and it is a question which will only be answered the day someone does it. The only thing I am sure of is that Linux community are providing a stronger technical foundation for someone to succeed with than ever before, so the question is just if someone can come up with the business model and the market skills to take it to the next level. There is also the chance that it will come in a shape we don’t appreciate today, for instance maybe <a href="https://en.wikipedia.org/wiki/Chrome_OS">ChromeOS</a> evolves into a more full fledged operating system as it grows in popularity and thus ends up being the Linux on the Desktop end game? Or maybe Valve decides to relaunch their SteamOS effort and it provides the foundation for a major general desktop growth? Or maybe market opportunities arise that will cause us at <a href="http://www.redhat.com/">Red Hat</a> to decide to go after the desktop market in a wider sense than we do today? Or maybe <a href="https://endlessos.com/home/">Endless</a> succeeds with their vision for a Linux desktop operating system?  Or maybe the idea of a desktop operating system gets supplanted to the degree that we in the end just sit there saying ‘Alexa, please open the IDE and take dictation of this new graphics driver I am writing’ (ok, probably not that last one ;)
</p>
<p>
And to be fair there are a lot of people saying that Linux already made it on the desktop in the form of things like Android tablets. Which is technically correct as Android does run on the Linux kernel, but I think for many of us it feels a bit more like a distant cousin as opposed to a close family member both in terms of use cases it targets and in terms of technological pedigree.</p>
<p>
As a sidenote, I am heading of on Yuletide vacation tomorrow evening, taking my wife and kids to Norway to spend time with our family there. So don’t expect a lot new blog posts from me until I am back from <a href="https://devconf.cz/">DevConf</a> in early February. I hope to see many of you at DevConf though, it is a great conference and Brno is a great town even in freezing winter. As we say in Norway, there is no such thing as bad weather, it is only bad clothing.</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/12/19/why-hasnt-the-year-of-the-linux-desktop-happened-yet/">by uraeus at December 19, 2017 04:00 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2 class="date">December 15, 2017</h2>

<div class="channelgroup">


<div class="entrygroup" id="https://blogs.gnome.org/uraeus/?p=2773" lang="en-US">
<h3><a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> — <a href="https://blogs.gnome.org/uraeus/2017/12/15/some-predictions-for-2018/">Some predictions for 2018</a></h3>
<div class="entry">
<div class="content">
<img class="face" src="uraeus.png" alt="(Christian Schaller)" width="70" height="73">
<p>So I spent a few hours polishing my crystal ball today, so here are some predictions for Linux on the Desktop in 2018. The advantage of course for me to publish these now is that I can then later selectively quote the ones I got right to prove my brilliance and the internet can selectively quote the ones I got wrong to prove my stupidity :) </p>
<p><b>Prediction 1: Meson becomes the defacto build system of the Linux community</b></p>
<p>Meson has been going from strength to strength this year and a lot of projects<br>
which passed on earlier attempts to replace autotools has adopted it. I predict this<br>
trend will continue in 2018 and that by the end of the year everyone agrees that Meson<br>
has replaced autotools as the Linux community build system of choice. That said I am not<br>
convinced the Linux kernel itself will adopt Meson in 2018.</p>
<p><b>Prediction 2: Rust puts itself on a clear trajectory to replace C and C++ for low level programming</b></p>
<p>Another rising start of 2017 is the programming language Rust. And while its pace of adoption<br>
will be slower than Meson I do believe that by the time 2018 comes to a close the general opinion is<br>
that Rust is the future of low level programming, replacing old favorites like C and C++. Major projects<br>
like GNOME and GStreamer are already adopting Rust at a rapid pace and I believe even more projects will<br>
join them in 2018.</p>
<p><b>Prediction 3: Apples decline as a PC vendor becomes obvious</b></p>
<p>
Ever since Steve Jobs died it has become quite clear in my opinion that the emphasis<br>
on the traditional desktop is fading from Apple. The pace of hardware refreshes seems<br>
to be slowing and MacOS X seems to be going more and more stale. Some pundits have already<br>
started pointing this out and I predict that in 2018 Apple will be no longer consider the<br>
cool kid on the block for people looking for laptops, especially among the tech savvy crowd.<br>
Hopefully a good opportunity for Linux on the desktop to assert itself more.</p>
<p><b>Prediction 4: Traditional distro packaging for desktop applications<br>
will start fading away in favour of Flatpak</b></p>
<p>From where I am standing I think 2018 will be the breakout year for Flatpak as a replacement<br>
for gettings your desktop applications as RPMS or debs. I predict that by the end of 2018 more or<br>
less every Linux Desktop user will be at least running 1 flatpak on their system.</p>
<p><b>Prediction 5: Linux Graphics competitive across the board</b></p>
<p>I think 2018 will be a breakout year for Linux graphics support. I think our GPU drivers and API will be competitive with any other platform both in completeness and performance. So by the end of 2018 I predict that you will see Linux game ports by major porting houses<br>
like Aspyr and Feral that perform just as well as their Windows counterparts. What is more I also predict that by the end of 2018 discreet graphics will be considered a solved problem on Linux.</p>
<p><b>Prediction 6: H265 will be considered a failure</b></p>
<p>I predict that by the end of 2018 H265 will be considered a failed codec effort and the era of royalty bearing media codecs will effectively start coming to and end. H264 will be considered the last successful royalty bearing codec and all new codecs coming out will<br>
all be open source and royalty free.</p></div>

<p class="date">
<a href="https://blogs.gnome.org/uraeus/2017/12/15/some-predictions-for-2018/">by uraeus at December 15, 2017 08:53 PM</a>
</p>
</div>
</div>


</div>

</div>
</div>

<div id="sidebar">

<div id="about">
<p>
Planet GStreamer is powered by <a href="http://www.planetplanet.org/">Planet</a> and hosted by <a href="http://www.freedesktop.org/">freedesktop.org</a>
</p>
<p>
<a href="http://gstreamer.net/">GStreamer Home</a>
</p>
<p>
<a href="http://www.catb.org/hacker-emblem/"><img class="button" src="hacker.png" alt="[Hacker]" width="80" height="15"></a>
<a href="http://www.planetplanet.org/"><img class="button" src="planet.png" alt="[Planet]" width="80" height="15"></a>
</p>
</div>

<div id="freshness">
<p>
<em>Last updated: March 26, 2018 02:18 AM. All times are UTC.</em>
</p>
</div>

<div id="feeds">
<h2>Feeds:</h2>
<p>
Planet GStreamer has aggregated feeds available as
<a href="https://gstreamer.freedesktop.org/planet/atom.xml">Atom 1.0</a>,
<a href="https://gstreamer.freedesktop.org/planet/rss10.xml">RSS 1.0</a>, and
<a href="https://gstreamer.freedesktop.org/planet/rss20.xml">RSS 2.0</a>
and subscription lists in
<a href="https://gstreamer.freedesktop.org/planet/foafroll.xml">FOAF</a> and
<a href="https://gstreamer.freedesktop.org/planet/opml.xml">OPML</a>.
</p>
</div>

<div id="subscriptions">
<h2>Subscriptions:</h2>
<ul>
<li>
<a href="http://abock.org/" class="message" title="no activity in 90 days">Aaron Bockover</a> <a href="https://abock.org/feed" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://genuinepulse.blogspot.com/" class="message" title="http status 401">Andre Dieb Martins</a> <a href="http://genuinepulse.blogspot.com/feeds/posts/default/-/gstreamer" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://wingolog.org/" title="wingolog">Andy Wingo</a> <a href="http://wingolog.org/feed/atom/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://arunraghavan.net/" title="Arun Raghavan">Arun Raghavan</a> <a href="https://arunraghavan.net/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://hadess.net/" class="message" title="no activity in 90 days">Bastien Nocera</a> <a href="http://www.hadess.net/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/company/" class="message" title="no activity in 90 days">Benjamin Otte</a> <a href="http://www.advogato.org/person/company/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://dotsony.blogspot.com/" class="message" title="no activity in 90 days">Brandon Lewis</a> <a href="http://dotsony.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.oracle.com/yippi/feed/entries/rss" class="message" title="404: not found">Brian Cameron</a> <a href="http://blogs.oracle.com/yippi/feed/entries/rss" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.gnome.org/uraeus" title="Christian F.K. Schaller">Christian Schaller</a> <a href="https://blogs.gnome.org/uraeus/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://schleef.org/" class="message" title="internal server error">David Schleef</a> <a href="http://schleef.org/blog/feeds/atom/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.gnome.org/portal/edwardrv" class="message" title="no activity in 90 days">Edward Hervey</a> <a href="http://blogs.gnome.org/edwardrv/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://eocanha.org/blog" class="message" title="no activity in 90 days">Enrique Ocaña González</a> <a href="http://eocanha.org/blog/category/gstreamer/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/omega/" class="message" title="no activity in 90 days">Erik Walthinsen</a> <a href="http://www.advogato.org/person/omega/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://felipec.wordpress.com/" class="message" title="410: gone">Felipe Contreras</a> <a href="http://felipec.wordpress.com/category/planet/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://fcarvalho.blogspot.com/" class="message" title="no activity in 90 days">Flavio Oliveira</a> <a href="http://fcarvalho.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.flumotion.net/news/" class="message" title="no activity in 90 days">Flumotion</a> <a href="http://www.flumotion.net/news/rss-1.0.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://gstreamer.freedesktop.org/news/" title="GStreamer News">GStreamer</a> <a href="https://gstreamer.freedesktop.org/news/rss-1.0.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://gstreamer.freedesktop.org/" class="message" title="no activity in 90 days">GStreamer Newsletter</a> <a href="https://gstreamer.freedesktop.org/news/status-rss-1.0.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://gkiagia.wordpress.com/" class="message" title="no activity in 90 days">George Kiagiadakis</a> <a href="https://gkiagia.wordpress.com/category/gstreamer-2/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.desmottes.be/?feed/en/rss2" class="message" title="no activity in 90 days">Guillaume Desmottes</a> <a href="https://blog.desmottes.be/?feed/en/rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://guij.emont.org/blog/category/geekeries/" class="message" title="no activity in 90 days">Guillaume Emont</a> <a href="http://guij.emont.org/blog/category/geekeries/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://andrescolubri.net/feed.xml" class="message" title="no activity in 90 days">Gustavo Orrillo</a> <a href="http://andrescolubri.net/feed.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://blogs.igalia.com/zzoon/" class="message" title="404: not found">Hyunjun Ko</a> <a href="https://blogs.igalia.com/zzoon/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/peaceandlove/" class="message" title="no activity in 90 days">Iain Holmes</a> <a href="http://www.advogato.org/person/peaceandlove/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://masher.homeip.net/~jan/diary/" class="message" title="no activity in 90 days">Jan Schmidt</a> <a href="http://noraisin.net/diary/?feed=rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://fortintam.com/blog" title="J.F. Fortin Tam">Jean-François Fortin Tam</a> <a href="http://fortintam.com/blog/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/jdahlin/" class="message" title="no activity in 90 days">Johan Dahlin</a> <a href="http://www.advogato.org/person/jdahlin/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.jokosher.org/" class="message" title="internal server error">Jokosher News</a> <a href="http://www.jokosher.org/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://laszlopandy.com/" class="message" title="no activity in 90 days">Laszlo Pandy</a> <a href="http://laszlopandy.com/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/lmjohns3/" class="message" title="no activity in 90 days">Leif Johnson</a> <a href="http://www.advogato.org/person/lmjohns3/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://oxcsnicho-stamp.blogspot.com/" class="message" title="no activity in 90 days">Lin YANG</a> <a href="http://oxcsnicho-stamp.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://mathieuduponchelle.blogspot.com/" class="message" title="no activity in 90 days">Mathieu Duponchelle</a> <a href="http://mathieuduponchelle.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.mikeasoft.com/" title="Michael Sheldon's Stuff">Michael Sheldon</a> <a href="http://blog.mikeasoft.com/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://mikesmith.wordpress.com/" class="message" title="no activity in 90 days">Michael Smith</a> <a href="https://mikesmith.wordpress.com/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://macslow.thepimp.net/" class="message" title="internal server error">Mirco Müller</a> <a href="http://macslow.thepimp.net/?feed=rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://ndufresne.ca/" class="message" title="no activity in 90 days">Nicolas Dufresne</a> <a href="http://ndufresne.ca/category/planet/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.nirbheek.in/" title="Nirbheek’s Rantings">Nirbheek Chauhan</a> <a href="http://blog.nirbheek.in/feeds/posts/default/-/gstreamer" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.tester.ca/" class="message" title="no activity in 90 days">Olivier Crete</a> <a href="http://ocrete.ca/category/english/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://base-art.net/" title="Base-Art - Philippe Normand">Phil Normand</a> <a href="http://base-art.net/Articles/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/phkhal/" class="message" title="no activity in 90 days">Philippe Khalaf</a> <a href="http://www.advogato.org/person/phkhal/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/rillian/" class="message" title="no activity in 90 days">Ralph Giles</a> <a href="http://www.advogato.org/person/rillian/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://heisenbugs.blogspot.com/" class="message" title="no activity in 90 days">Reynaldo Verdejo</a> <a href="http://heisenbugs.blogspot.com/feeds/posts/default/-/FreeDesktop" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://richard-spiers.blogspot.com/" class="message" title="no activity in 90 days">Richard Spiers</a> <a href="http://richard-spiers.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://ramcq.net/" class="message" title="no activity in 90 days">Robert McQueen</a> <a href="http://robot101.net/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://gstmediaservices.blogspot.com/" class="message" title="no activity in 90 days">Roberto Fagá</a> <a href="http://gstmediaservices.blogspot.com/atom.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://relekandcode.blogspot.com/" class="message" title="no activity in 90 days">Roland Elek</a> <a href="http://relekandcode.blogspot.com/feeds/posts/default" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://coaxion.net/blog" title="coaxion.net – slomo's blog">Sebastian Dröge</a> <a href="https://coaxion.net/blog/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://k-d-w.org/rss.xml" title="Sebastian Pölsterl's blog - Where he blogs about his latest hacking adventures.">Sebastian Pölsterl</a> <a href="https://k-d-w.org/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://sjoerd.luon.net/tags/gnome/" class="message" title="no activity in 90 days">Sjoerd Simons</a> <a href="http://sjoerd.luon.net/tags/gnome/index.atom" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/ensonic/" class="message" title="403: forbidden">Stefan Kost</a> <a href="http://www.advogato.org/person/ensonic/rss.xml" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blog.thiagoss.com/" class="message" title="http status 502">Thiago Santos</a> <a href="http://blog.thiagoss.com/tag/gstreamer/rss/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://blogs.gnome.org/tsaunier" class="message" title="no activity in 90 days">Thibault Saunier</a> <a href="http://blogs.gnome.org/tsaunier/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://thomas.apestaart.org/log" class="message" title="no activity in 90 days">Thomas Vander Stichele</a> <a href="http://thomas.apestaart.org/log/?feed=rss2" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://blogs.igalia.com/vjaquez" class="message" title="no activity in 90 days">Víctor Jáquez</a> <a href="https://blogs.igalia.com/vjaquez/tag/gstreamer/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="https://blogs.igalia.com/xrcalvar" class="message" title="no activity in 90 days">Xabier Rodríguez Calvar</a> <a href="https://blogs.igalia.com/xrcalvar/category/planets/planet-gstreamer/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://zaheer.merali.org/" class="message" title="no activity in 90 days">Zaheer Abbas Merali</a> <a href="http://zaheer.merali.org/feed/" title="subscribe">(feed)</a>
</li>
<li>
<a href="http://www.advogato.org/person/zeenix/" class="message" title="no activity in 90 days">Zeeshan Ali</a> <a href="http://www.advogato.org/person/zeenix/rss.xml" title="subscribe">(feed)</a>
</li>
</ul>
</div>

<div id="planetarium">
<h2>Planetarium:</h2>
<ul>
<li><a href="http://www.planetapache.org/">Planet Apache</a></li>
<li><a href="http://classpath.wildebeest.org/planet/">Planet Classpath</a></li>
<li><a href="http://planet.debian.net/">Planet Debian</a></li>
<li><a href="http://planet.debian.org.hk/">Planet Debian HK</a></li>
<li><a href="http://planet.freedesktop.org/">Planet freedesktop.org</a></li>
<li><a href="http://planet.gnome.org/">Planet GNOME</a></li>
<li><a href="http://gnome.or.kr/pgk/">Planet GNOME Korea</a></li>
<li><a href="http://planetjava.org/">Planet Java.org</a></li>
<li><a href="http://planet.perl.org/">Planet Perl</a></li>
<li><a href="http://www.planetsuse.org/">Planet SuSE</a></li>
<li><a href="http://planet.twistedmatrix.com/">Planet Twisted</a></li>
<li><a href="http://planet.arslinux.com/">Planet Ars Linux</a></li>
<li><a href="http://www.planetkde.org/">Planet KDE</a></li>
<li><a href="http://fossplanet.osdir.com/">FOSS Planet</a></li>
<li><a href="http://live.linuxchix.org/">LinuxChix Live</a></li>
<li><a href="http://kerneltrap.org/hackers/linux">Linux @ KernelTrap</a></li>
<li><a href="http://www.go-mono.com/monologue/">Mono</a></li>
<li><a href="http://www.planet-php.net/">PHP</a></li>
<li><a href="http://planetrdf.com/">RDF</a></li>
<li><a href="http://xfce.org/blog/">XFCE</a></li>
<li><a href="http://advogato.org/recentlog.html?thresh=4">Advogato</a></li>
</ul>
</div>

</div>




</body>
</html>
